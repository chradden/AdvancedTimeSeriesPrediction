{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Deep Learning: LSTM & GRU\n",
                "\n",
                "**Ziel:** Recurrent Neural Networks f√ºr Zeitreihenvorhersage\n",
                "\n",
                "**Modelle:**\n",
                "1. Simple RNN (Baseline)\n",
                "2. LSTM (Long Short-Term Memory)\n",
                "3. GRU (Gated Recurrent Unit)\n",
                "4. Bidirectional LSTM\n",
                "5. Stacked LSTM\n",
                "\n",
                "**Basierend auf:** Week05_RNNs_LSTM_GRU"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.append('../src')\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from pathlib import Path\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Deep Learning\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "from evaluation.metrics import calculate_metrics, print_metrics, compare_models\n",
                "from visualization.plots import plot_forecast, plot_multiple_forecasts\n",
                "\n",
                "plt.style.use('seaborn-v0_8-darkgrid')\n",
                "%matplotlib inline\n",
                "\n",
                "# Check GPU\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"‚úÖ Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Daten laden und vorbereiten"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "DATA_TYPE = 'solar'\n",
                "data_dir = Path('../data/processed')\n",
                "\n",
                "# 1. Lade skalierte Daten f√ºr das Training\n",
                "train_df = pd.read_csv(data_dir / f'{DATA_TYPE}_train_scaled.csv', parse_dates=['timestamp'])\n",
                "val_df = pd.read_csv(data_dir / f'{DATA_TYPE}_val_scaled.csv', parse_dates=['timestamp'])\n",
                "test_df = pd.read_csv(data_dir / f'{DATA_TYPE}_test_scaled.csv', parse_dates=['timestamp'])\n",
                "\n",
                "# 2. Lade Original-Daten um den Scaler zu rekonstruieren (WICHTIG f√ºr Vergleich!)\n",
                "train_df_original = pd.read_csv(data_dir / f'{DATA_TYPE}_train.csv', parse_dates=['timestamp'])\n",
                "\n",
                "# Scaler neu fitten\n",
                "scaler = StandardScaler()\n",
                "scaler.fit(train_df_original[['value']])\n",
                "\n",
                "print(f\"Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}\")\n",
                "print(f\"Scaler Mean: {scaler.mean_[0]:.2f}, Scale: {scaler.scale_[0]:.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Sequenzen erstellen\n",
                "\n",
                "RNNs ben√∂tigen Sequenzen als Input: (sequence_length, features)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_sequences(data, seq_length, target_col='value'):\n",
                "    \"\"\"\n",
                "    Create sequences for RNN training\n",
                "    \n",
                "    Args:\n",
                "        data: DataFrame with features\n",
                "        seq_length: Length of input sequence\n",
                "        target_col: Name of target column\n",
                "    \n",
                "    Returns:\n",
                "        X, y arrays\n",
                "    \"\"\"\n",
                "    X, y = [], []\n",
                "    \n",
                "    # Nur value f√ºr einfache Modelle\n",
                "    values = data[target_col].values\n",
                "    \n",
                "    for i in range(len(values) - seq_length):\n",
                "        X.append(values[i:i+seq_length])\n",
                "        y.append(values[i+seq_length])\n",
                "    \n",
                "    return np.array(X), np.array(y)\n",
                "\n",
                "# Sequenzl√§nge (z.B. 24 Stunden f√ºr st√ºndliche Daten)\n",
                "SEQ_LENGTH = 24\n",
                "\n",
                "X_train, y_train = create_sequences(train_df, SEQ_LENGTH)\n",
                "X_val, y_val = create_sequences(val_df, SEQ_LENGTH)\n",
                "X_test, y_test = create_sequences(test_df, SEQ_LENGTH)\n",
                "\n",
                "# Reshape f√ºr PyTorch: (samples, seq_length, features)\n",
                "X_train = X_train.reshape(-1, SEQ_LENGTH, 1)\n",
                "X_val = X_val.reshape(-1, SEQ_LENGTH, 1)\n",
                "X_test = X_test.reshape(-1, SEQ_LENGTH, 1)\n",
                "\n",
                "print(f\"X_train shape: {X_train.shape}\")\n",
                "print(f\"y_train shape: {y_train.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. PyTorch Dataset und DataLoader"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TimeSeriesDataset(Dataset):\n",
                "    def __init__(self, X, y):\n",
                "        self.X = torch.FloatTensor(X)\n",
                "        self.y = torch.FloatTensor(y)\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.X)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        return self.X[idx], self.y[idx]\n",
                "\n",
                "# Create datasets\n",
                "train_dataset = TimeSeriesDataset(X_train, y_train)\n",
                "val_dataset = TimeSeriesDataset(X_val, y_val)\n",
                "test_dataset = TimeSeriesDataset(X_test, y_test)\n",
                "\n",
                "# Create dataloaders\n",
                "BATCH_SIZE = 64\n",
                "\n",
                "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
                "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
                "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
                "\n",
                "print(f\"‚úÖ DataLoaders created\")\n",
                "print(f\"Train batches: {len(train_loader)}\")\n",
                "print(f\"Val batches: {len(val_loader)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Model Definitions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LSTMModel(nn.Module):\n",
                "    def __init__(self, input_size=1, hidden_size=64, num_layers=2, dropout=0.2):\n",
                "        super(LSTMModel, self).__init__()\n",
                "        \n",
                "        self.hidden_size = hidden_size\n",
                "        self.num_layers = num_layers\n",
                "        \n",
                "        self.lstm = nn.LSTM(\n",
                "            input_size=input_size,\n",
                "            hidden_size=hidden_size,\n",
                "            num_layers=num_layers,\n",
                "            dropout=dropout if num_layers > 1 else 0,\n",
                "            batch_first=True\n",
                "        )\n",
                "        \n",
                "        self.fc = nn.Linear(hidden_size, 1)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        # x shape: (batch, seq, features)\n",
                "        lstm_out, _ = self.lstm(x)\n",
                "        # Take last output\n",
                "        last_output = lstm_out[:, -1, :]\n",
                "        predictions = self.fc(last_output)\n",
                "        return predictions.squeeze()\n",
                "\n",
                "\n",
                "class GRUModel(nn.Module):\n",
                "    def __init__(self, input_size=1, hidden_size=64, num_layers=2, dropout=0.2):\n",
                "        super(GRUModel, self).__init__()\n",
                "        \n",
                "        self.gru = nn.GRU(\n",
                "            input_size=input_size,\n",
                "            hidden_size=hidden_size,\n",
                "            num_layers=num_layers,\n",
                "            dropout=dropout if num_layers > 1 else 0,\n",
                "            batch_first=True\n",
                "        )\n",
                "        \n",
                "        self.fc = nn.Linear(hidden_size, 1)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        gru_out, _ = self.gru(x)\n",
                "        last_output = gru_out[:, -1, :]\n",
                "        predictions = self.fc(last_output)\n",
                "        return predictions.squeeze()\n",
                "\n",
                "\n",
                "class BiLSTMModel(nn.Module):\n",
                "    def __init__(self, input_size=1, hidden_size=64, num_layers=2, dropout=0.2):\n",
                "        super(BiLSTMModel, self).__init__()\n",
                "        \n",
                "        self.lstm = nn.LSTM(\n",
                "            input_size=input_size,\n",
                "            hidden_size=hidden_size,\n",
                "            num_layers=num_layers,\n",
                "            dropout=dropout if num_layers > 1 else 0,\n",
                "            batch_first=True,\n",
                "            bidirectional=True\n",
                "        )\n",
                "        \n",
                "        # *2 because bidirectional\n",
                "        self.fc = nn.Linear(hidden_size * 2, 1)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        lstm_out, _ = self.lstm(x)\n",
                "        last_output = lstm_out[:, -1, :]\n",
                "        predictions = self.fc(last_output)\n",
                "        return predictions.squeeze()\n",
                "\n",
                "print(\"‚úÖ Models defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Training Function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_model(model, train_loader, val_loader, epochs=50, lr=0.001):\n",
                "    \"\"\"\n",
                "    Train a PyTorch model\n",
                "    \"\"\"\n",
                "    model = model.to(device)\n",
                "    criterion = nn.MSELoss()\n",
                "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
                "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
                "    \n",
                "    train_losses = []\n",
                "    val_losses = []\n",
                "    best_val_loss = float('inf')\n",
                "    patience_counter = 0\n",
                "    \n",
                "    for epoch in range(epochs):\n",
                "        # Training\n",
                "        model.train()\n",
                "        train_loss = 0\n",
                "        for X_batch, y_batch in train_loader:\n",
                "            X_batch = X_batch.to(device)\n",
                "            y_batch = y_batch.to(device)\n",
                "            \n",
                "            optimizer.zero_grad()\n",
                "            predictions = model(X_batch)\n",
                "            loss = criterion(predictions, y_batch)\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "            \n",
                "            train_loss += loss.item()\n",
                "        \n",
                "        train_loss /= len(train_loader)\n",
                "        train_losses.append(train_loss)\n",
                "        \n",
                "        # Validation\n",
                "        model.eval()\n",
                "        val_loss = 0\n",
                "        with torch.no_grad():\n",
                "            for X_batch, y_batch in val_loader:\n",
                "                X_batch = X_batch.to(device)\n",
                "                y_batch = y_batch.to(device)\n",
                "                predictions = model(X_batch)\n",
                "                loss = criterion(predictions, y_batch)\n",
                "                val_loss += loss.item()\n",
                "        \n",
                "        val_loss /= len(val_loader)\n",
                "        val_losses.append(val_loss)\n",
                "        \n",
                "        # Learning rate scheduling\n",
                "        scheduler.step(val_loss)\n",
                "        \n",
                "        # Early stopping\n",
                "        if val_loss < best_val_loss:\n",
                "            best_val_loss = val_loss\n",
                "            patience_counter = 0\n",
                "        else:\n",
                "            patience_counter += 1\n",
                "        \n",
                "        if (epoch + 1) % 10 == 0:\n",
                "            print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.6f} - Val Loss: {val_loss:.6f}\")\n",
                "        \n",
                "        if patience_counter >= 10:\n",
                "            print(f\"Early stopping at epoch {epoch+1}\")\n",
                "            break\n",
                "    \n",
                "    return model, train_losses, val_losses\n",
                "\n",
                "\n",
                "def predict(model, data_loader):\n",
                "    \"\"\"\n",
                "    Make predictions with a trained model\n",
                "    \"\"\"\n",
                "    model.eval()\n",
                "    predictions = []\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for X_batch, _ in data_loader:\n",
                "            X_batch = X_batch.to(device)\n",
                "            pred = model(X_batch)\n",
                "            predictions.extend(pred.cpu().numpy())\n",
                "    \n",
                "    return np.array(predictions)\n",
                "\n",
                "print(\"‚úÖ Training functions defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Train LSTM"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Training LSTM...\\n\")\n",
                "\n",
                "lstm_model = LSTMModel(\n",
                "    input_size=1,\n",
                "    hidden_size=64,\n",
                "    num_layers=2,\n",
                "    dropout=0.2\n",
                ")\n",
                "\n",
                "lstm_model, lstm_train_loss, lstm_val_loss = train_model(\n",
                "    lstm_model,\n",
                "    train_loader,\n",
                "    val_loader,\n",
                "    epochs=50,\n",
                "    lr=0.001\n",
                ")\n",
                "\n",
                "# Predict\n",
                "lstm_pred_scaled = predict(lstm_model, test_loader)\n",
                "\n",
                "# Inverse Transform (Zur√ºckrechnen auf MW)\n",
                "lstm_pred = scaler.inverse_transform(lstm_pred_scaled.reshape(-1, 1)).flatten()\n",
                "y_test_original = scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
                "\n",
                "# Evaluate on ORIGINAL scale\n",
                "lstm_metrics = calculate_metrics(y_test_original, lstm_pred, prefix='test_')\n",
                "print(\"\\n\")\n",
                "print_metrics(lstm_metrics, \"LSTM (Original Scale)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Train GRU"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Training GRU...\\n\")\n",
                "\n",
                "gru_model = GRUModel(\n",
                "    input_size=1,\n",
                "    hidden_size=64,\n",
                "    num_layers=2,\n",
                "    dropout=0.2\n",
                ")\n",
                "\n",
                "gru_model, gru_train_loss, gru_val_loss = train_model(\n",
                "    gru_model,\n",
                "    train_loader,\n",
                "    val_loader,\n",
                "    epochs=50,\n",
                "    lr=0.001\n",
                ")\n",
                "\n",
                "gru_pred_scaled = predict(gru_model, test_loader)\n",
                "\n",
                "# Inverse Transform\n",
                "gru_pred = scaler.inverse_transform(gru_pred_scaled.reshape(-1, 1)).flatten()\n",
                "\n",
                "# Evaluate\n",
                "gru_metrics = calculate_metrics(y_test_original, gru_pred, prefix='test_')\n",
                "print(\"\\n\")\n",
                "print_metrics(gru_metrics, \"GRU (Original Scale)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Train Bidirectional LSTM"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Training Bidirectional LSTM...\\n\")\n",
                "\n",
                "bilstm_model = BiLSTMModel(\n",
                "    input_size=1,\n",
                "    hidden_size=64,\n",
                "    num_layers=2,\n",
                "    dropout=0.2\n",
                ")\n",
                "\n",
                "bilstm_model, bilstm_train_loss, bilstm_val_loss = train_model(\n",
                "    bilstm_model,\n",
                "    train_loader,\n",
                "    val_loader,\n",
                "    epochs=50,\n",
                "    lr=0.001\n",
                ")\n",
                "\n",
                "bilstm_pred_scaled = predict(bilstm_model, test_loader)\n",
                "\n",
                "# Inverse Transform\n",
                "bilstm_pred = scaler.inverse_transform(bilstm_pred_scaled.reshape(-1, 1)).flatten()\n",
                "\n",
                "# Evaluate\n",
                "bilstm_metrics = calculate_metrics(y_test_original, bilstm_pred, prefix='test_')\n",
                "print(\"\\n\")\n",
                "print_metrics(bilstm_metrics, \"Bidirectional LSTM (Original Scale)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Model Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "all_results = {\n",
                "    'LSTM': lstm_metrics,\n",
                "    'GRU': gru_metrics,\n",
                "    'BiLSTM': bilstm_metrics\n",
                "}\n",
                "\n",
                "comparison_df = compare_models(all_results, sort_by='test_rmse')\n",
                "\n",
                "print(\"\\n\" + \"=\" * 80)\n",
                "print(\"DEEP LEARNING MODELS - COMPARISON (ORIGINAL SCALE)\")\n",
                "print(\"=\" * 80)\n",
                "display(comparison_df.round(2))\n",
                "\n",
                "best_model_name = comparison_df['test_rmse'].idxmin()\n",
                "print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
                "\n",
                "# Save results\n",
                "comparison_df.to_csv('../results/metrics/solar_deep_learning_results.csv')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Learning Curves"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
                "\n",
                "# LSTM\n",
                "axes[0].plot(lstm_train_loss, label='Train Loss', alpha=0.7)\n",
                "axes[0].plot(lstm_val_loss, label='Val Loss', alpha=0.7)\n",
                "axes[0].set_title('LSTM Learning Curves', fontsize=14, fontweight='bold')\n",
                "axes[0].set_xlabel('Epoch')\n",
                "axes[0].set_ylabel('Loss (MSE)')\n",
                "axes[0].legend()\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "# GRU\n",
                "axes[1].plot(gru_train_loss, label='Train Loss', alpha=0.7)\n",
                "axes[1].plot(gru_val_loss, label='Val Loss', alpha=0.7)\n",
                "axes[1].set_title('GRU Learning Curves', fontsize=14, fontweight='bold')\n",
                "axes[1].set_xlabel('Epoch')\n",
                "axes[1].set_ylabel('Loss (MSE)')\n",
                "axes[1].legend()\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "# BiLSTM\n",
                "axes[2].plot(bilstm_train_loss, label='Train Loss', alpha=0.7)\n",
                "axes[2].plot(bilstm_val_loss, label='Val Loss', alpha=0.7)\n",
                "axes[2].set_title('BiLSTM Learning Curves', fontsize=14, fontweight='bold')\n",
                "axes[2].set_xlabel('Epoch')\n",
                "axes[2].set_ylabel('Loss (MSE)')\n",
                "axes[2].legend()\n",
                "axes[2].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Visualisierung"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Zeige nur die ersten 300 Stunden f√ºr bessere Sichtbarkeit\n",
                "plot_len = 300\n",
                "\n",
                "plt.figure(figsize=(15, 6))\n",
                "plt.plot(y_test_original[:plot_len], label='Tats√§chlich', color='black', alpha=0.6)\n",
                "plt.plot(lstm_pred[:plot_len], label='LSTM', alpha=0.7)\n",
                "plt.plot(gru_pred[:plot_len], label='GRU', alpha=0.7)\n",
                "plt.plot(bilstm_pred[:plot_len], label='BiLSTM', alpha=0.7)\n",
                "\n",
                "plt.title('Deep Learning Modelle - Vorhersage (Erste 300 Stunden)', fontsize=16)\n",
                "plt.xlabel('Stunden')\n",
                "plt.ylabel('Solar Generation (MW)')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
