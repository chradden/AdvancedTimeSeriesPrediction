{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06e38ee4",
   "metadata": {},
   "source": [
    "# XGBoost Hyperparameter Tuning\n",
    "\n",
    "**Ziel:** Optimierung des besten ML-Modells aus Phase 1\n",
    "\n",
    "**Strategie:**\n",
    "1. Random Search f√ºr schnelle Exploration\n",
    "2. Grid Search f√ºr feine Abstimmung\n",
    "3. Time-Series Cross-Validation\n",
    "\n",
    "**Parameter zu optimieren:**\n",
    "- `n_estimators`: Anzahl der B√§ume (100-1000)\n",
    "- `max_depth`: Baumtiefe (3-10)\n",
    "- `learning_rate`: Lernrate (0.01-0.3)\n",
    "- `subsample`: Datenstichprobe pro Baum (0.7-1.0)\n",
    "- `colsample_bytree`: Feature-Stichprobe pro Baum (0.7-1.0)\n",
    "- `min_child_weight`: Minimum samples in leaf (1-10)\n",
    "\n",
    "**Baseline:** MAE ~246 MW, R¬≤ ~0.983"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5437be70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML Libraries\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Custom modules\n",
    "from evaluation.metrics import calculate_metrics, print_metrics\n",
    "from visualization.plots import plot_forecast\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úÖ Libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f175453a",
   "metadata": {},
   "source": [
    "## 1. Daten laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afaa37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TYPE = 'solar'\n",
    "data_dir = Path('../data/processed')\n",
    "\n",
    "# Load non-scaled data (XGBoost works better with original scale)\n",
    "train_df = pd.read_csv(data_dir / f'{DATA_TYPE}_train.csv', parse_dates=['timestamp'])\n",
    "val_df = pd.read_csv(data_dir / f'{DATA_TYPE}_val.csv', parse_dates=['timestamp'])\n",
    "test_df = pd.read_csv(data_dir / f'{DATA_TYPE}_test.csv', parse_dates=['timestamp'])\n",
    "\n",
    "print(f\"Train: {len(train_df)} samples\")\n",
    "print(f\"Val:   {len(val_df)} samples\")\n",
    "print(f\"Test:  {len(test_df)} samples\")\n",
    "print(f\"\\nValue range: [{train_df['value'].min():.0f}, {train_df['value'].max():.0f}] MW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23022840",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering\n",
    "\n",
    "Erstelle die gleichen Features wie in `05_ml_tree_models.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1613472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    \"\"\"Create time-based features for tree models\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Time features\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    df['day'] = df['timestamp'].dt.day\n",
    "    df['month'] = df['timestamp'].dt.month\n",
    "    df['dayofweek'] = df['timestamp'].dt.dayofweek\n",
    "    df['dayofyear'] = df['timestamp'].dt.dayofyear\n",
    "    df['week'] = df['timestamp'].dt.isocalendar().week\n",
    "    df['quarter'] = df['timestamp'].dt.quarter\n",
    "    \n",
    "    # Cyclical encoding for hour and month\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    \n",
    "    # Lag features (previous hours)\n",
    "    for lag in [1, 2, 3, 24, 168]:  # 1h, 2h, 3h, 1day, 1week\n",
    "        df[f'lag_{lag}'] = df['value'].shift(lag)\n",
    "    \n",
    "    # Rolling statistics\n",
    "    for window in [24, 168]:  # 1 day, 1 week\n",
    "        df[f'rolling_mean_{window}'] = df['value'].shift(1).rolling(window=window).mean()\n",
    "        df[f'rolling_std_{window}'] = df['value'].shift(1).rolling(window=window).std()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply feature engineering\n",
    "train_df = create_features(train_df)\n",
    "val_df = create_features(val_df)\n",
    "test_df = create_features(test_df)\n",
    "\n",
    "# Drop NaN from lag/rolling features\n",
    "train_df = train_df.dropna()\n",
    "val_df = val_df.dropna()\n",
    "test_df = test_df.dropna()\n",
    "\n",
    "print(f\"‚úÖ Features created: {train_df.shape[1] - 2} features\")  # -2 for timestamp and value\n",
    "print(f\"\\nAfter dropna:\")\n",
    "print(f\"Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2732002d",
   "metadata": {},
   "source": [
    "## 3. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0deac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features\n",
    "feature_cols = [col for col in train_df.columns if col not in ['timestamp', 'value']]\n",
    "\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df['value']\n",
    "\n",
    "X_val = val_df[feature_cols]\n",
    "y_val = val_df['value']\n",
    "\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df['value']\n",
    "\n",
    "print(f\"Feature matrix shape: {X_train.shape}\")\n",
    "print(f\"\\nFeatures: {', '.join(feature_cols[:5])}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bae631",
   "metadata": {},
   "source": [
    "## 4. Baseline Model\n",
    "\n",
    "Reproduziere die Baseline aus `05_ml_tree_models.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041fbe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Baseline XGBoost...\\n\")\n",
    "\n",
    "baseline_model = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=7,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "baseline_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Predict\n",
    "y_pred_baseline = baseline_model.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "baseline_metrics = calculate_metrics(y_test, y_pred_baseline, prefix='test_')\n",
    "print_metrics(baseline_metrics, \"Baseline XGBoost\")\n",
    "\n",
    "print(f\"\\nüìä Baseline Performance:\")\n",
    "print(f\"   MAE:  {baseline_metrics['test_mae']:.2f} MW\")\n",
    "print(f\"   RMSE: {baseline_metrics['test_rmse']:.2f} MW\")\n",
    "print(f\"   R¬≤:   {baseline_metrics['test_r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8451c106",
   "metadata": {},
   "source": [
    "## 5. Random Search\n",
    "\n",
    "**Strategie:** Breite Exploration des Hyperparameter-Raums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917bf897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Parameter distributions for random search\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(100, 1000),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'learning_rate': uniform(0.01, 0.29),  # 0.01 to 0.3\n",
    "    'subsample': uniform(0.7, 0.3),        # 0.7 to 1.0\n",
    "    'colsample_bytree': uniform(0.7, 0.3), # 0.7 to 1.0\n",
    "    'min_child_weight': randint(1, 10),\n",
    "    'gamma': uniform(0, 0.5)\n",
    "}\n",
    "\n",
    "# Time Series Cross-Validation\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "# Base model\n",
    "base_model = xgb.XGBRegressor(\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    tree_method='hist'  # Faster training\n",
    ")\n",
    "\n",
    "print(\"Starting Random Search...\")\n",
    "print(f\"Parameter space: {len(param_distributions)} parameters\")\n",
    "print(f\"Iterations: 50\")\n",
    "print(f\"CV Folds: 3\\n\")\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=base_model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50,  # Number of random combinations\n",
    "    cv=tscv,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit on train + val\n",
    "X_train_val = pd.concat([X_train, X_val])\n",
    "y_train_val = pd.concat([y_train, y_val])\n",
    "\n",
    "random_search.fit(X_train_val, y_train_val)\n",
    "\n",
    "print(\"\\n‚úÖ Random Search completed\")\n",
    "print(f\"\\nBest parameters:\")\n",
    "for param, value in random_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\nBest CV MAE: {-random_search.best_score_:.2f} MW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec03e5e5",
   "metadata": {},
   "source": [
    "## 6. Evaluate Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2360bf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model from random search\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_tuned = best_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "tuned_metrics = calculate_metrics(y_test, y_pred_tuned, prefix='test_')\n",
    "print_metrics(tuned_metrics, \"Tuned XGBoost\")\n",
    "\n",
    "# Compare with baseline\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BASELINE vs TUNED COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Baseline': baseline_metrics,\n",
    "    'Tuned': tuned_metrics\n",
    "}).T\n",
    "\n",
    "display(comparison[['test_mae', 'test_rmse', 'test_mape', 'test_r2']].round(4))\n",
    "\n",
    "# Calculate improvement\n",
    "mae_improvement = (baseline_metrics['test_mae'] - tuned_metrics['test_mae']) / baseline_metrics['test_mae'] * 100\n",
    "rmse_improvement = (baseline_metrics['test_rmse'] - tuned_metrics['test_rmse']) / baseline_metrics['test_rmse'] * 100\n",
    "\n",
    "print(f\"\\nüìà Improvement:\")\n",
    "print(f\"   MAE:  {mae_improvement:+.2f}%\")\n",
    "print(f\"   RMSE: {rmse_improvement:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e055c8",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcd135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': best_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot top 15 features\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(importance_df['feature'].head(15)[::-1], \n",
    "         importance_df['importance'].head(15)[::-1])\n",
    "plt.xlabel('Feature Importance', fontsize=12)\n",
    "plt.title('Top 15 Most Important Features (Tuned XGBoost)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 Features:\")\n",
    "for i, row in importance_df.head(10).iterrows():\n",
    "    print(f\"  {row['feature']:20s}: {row['importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60119997",
   "metadata": {},
   "source": [
    "## 8. Prediction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd35e1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot first 7 days (168 hours)\n",
    "plot_len = 168\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Baseline\n",
    "axes[0].plot(y_test.values[:plot_len], label='Actual', color='black', alpha=0.7, linewidth=2)\n",
    "axes[0].plot(y_pred_baseline[:plot_len], label='Baseline', color='blue', alpha=0.7)\n",
    "axes[0].set_title(f'Baseline XGBoost - First 7 Days (MAE: {baseline_metrics[\"test_mae\"]:.2f} MW)', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Solar Generation (MW)')\n",
    "axes[0].legend(loc='upper right')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Tuned\n",
    "axes[1].plot(y_test.values[:plot_len], label='Actual', color='black', alpha=0.7, linewidth=2)\n",
    "axes[1].plot(y_pred_tuned[:plot_len], label='Tuned', color='green', alpha=0.7)\n",
    "axes[1].set_title(f'Tuned XGBoost - First 7 Days (MAE: {tuned_metrics[\"test_mae\"]:.2f} MW)', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Hours')\n",
    "axes[1].set_ylabel('Solar Generation (MW)')\n",
    "axes[1].legend(loc='upper right')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a7c446",
   "metadata": {},
   "source": [
    "## 9. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592e9fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals\n",
    "baseline_errors = y_test.values - y_pred_baseline\n",
    "tuned_errors = y_test.values - y_pred_tuned\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Residuals distribution\n",
    "axes[0].hist(baseline_errors, bins=50, alpha=0.5, label='Baseline', color='blue')\n",
    "axes[0].hist(tuned_errors, bins=50, alpha=0.5, label='Tuned', color='green')\n",
    "axes[0].axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0].set_xlabel('Prediction Error (MW)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Error Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Error by hour of day\n",
    "test_df_with_pred = test_df.copy()\n",
    "test_df_with_pred['baseline_error'] = np.abs(baseline_errors)\n",
    "test_df_with_pred['tuned_error'] = np.abs(tuned_errors)\n",
    "\n",
    "hourly_errors_baseline = test_df_with_pred.groupby('hour')['baseline_error'].mean()\n",
    "hourly_errors_tuned = test_df_with_pred.groupby('hour')['tuned_error'].mean()\n",
    "\n",
    "axes[1].plot(hourly_errors_baseline.index, hourly_errors_baseline.values, \n",
    "             marker='o', label='Baseline', color='blue')\n",
    "axes[1].plot(hourly_errors_tuned.index, hourly_errors_tuned.values, \n",
    "             marker='o', label='Tuned', color='green')\n",
    "axes[1].set_xlabel('Hour of Day')\n",
    "axes[1].set_ylabel('Mean Absolute Error (MW)')\n",
    "axes[1].set_title('Error by Hour of Day', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xticks(range(0, 24, 2))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Error Statistics:\")\n",
    "print(f\"\\nBaseline:\")\n",
    "print(f\"  Mean Error:   {baseline_errors.mean():.2f} MW\")\n",
    "print(f\"  Std Error:    {baseline_errors.std():.2f} MW\")\n",
    "print(f\"\\nTuned:\")\n",
    "print(f\"  Mean Error:   {tuned_errors.mean():.2f} MW\")\n",
    "print(f\"  Std Error:    {tuned_errors.std():.2f} MW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e5519a",
   "metadata": {},
   "source": [
    "## 10. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b430ba2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tuned model results\n",
    "results_df = pd.DataFrame({\n",
    "    'model': ['XGBoost_baseline', 'XGBoost_tuned'],\n",
    "    **{k: [baseline_metrics[k], tuned_metrics[k]] for k in baseline_metrics.keys()}\n",
    "})\n",
    "\n",
    "results_df.to_csv('../results/metrics/solar_xgboost_tuning_results.csv', index=False)\n",
    "print(\"‚úÖ Results saved to: results/metrics/solar_xgboost_tuning_results.csv\")\n",
    "\n",
    "# Save best parameters\n",
    "best_params_df = pd.DataFrame([random_search.best_params_])\n",
    "best_params_df.to_csv('../results/metrics/xgboost_best_params.csv', index=False)\n",
    "print(\"‚úÖ Best parameters saved to: results/metrics/xgboost_best_params.csv\")\n",
    "\n",
    "# Save feature importance\n",
    "importance_df.to_csv('../results/metrics/xgboost_feature_importance_tuned.csv', index=False)\n",
    "print(\"‚úÖ Feature importance saved to: results/metrics/xgboost_feature_importance_tuned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff1ae0f",
   "metadata": {},
   "source": [
    "## 11. Summary & Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44ab4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"XGBOOST HYPERPARAMETER TUNING - SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nüìä Baseline Model:\")\n",
    "print(f\"   MAE:  {baseline_metrics['test_mae']:.2f} MW\")\n",
    "print(f\"   RMSE: {baseline_metrics['test_rmse']:.2f} MW\")\n",
    "print(f\"   R¬≤:   {baseline_metrics['test_r2']:.6f}\")\n",
    "\n",
    "print(f\"\\nüéØ Tuned Model:\")\n",
    "print(f\"   MAE:  {tuned_metrics['test_mae']:.2f} MW\")\n",
    "print(f\"   RMSE: {tuned_metrics['test_rmse']:.2f} MW\")\n",
    "print(f\"   R¬≤:   {tuned_metrics['test_r2']:.6f}\")\n",
    "\n",
    "print(f\"\\nüìà Improvement:\")\n",
    "print(f\"   MAE:  {mae_improvement:+.2f}%\")\n",
    "print(f\"   RMSE: {rmse_improvement:+.2f}%\")\n",
    "\n",
    "print(f\"\\nüîß Best Parameters:\")\n",
    "for param, value in random_search.best_params_.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"   {param:20s}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"   {param:20s}: {value}\")\n",
    "\n",
    "print(f\"\\nüèÜ Key Insights:\")\n",
    "if mae_improvement > 0:\n",
    "    print(f\"   ‚úÖ Hyperparameter tuning improved MAE by {mae_improvement:.2f}%\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  Baseline was already well-tuned (improvement: {mae_improvement:.2f}%)\")\n",
    "\n",
    "print(f\"\\n   Top 3 Features:\")\n",
    "for i, row in importance_df.head(3).iterrows():\n",
    "    print(f\"     {i+1}. {row['feature']} ({row['importance']:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
