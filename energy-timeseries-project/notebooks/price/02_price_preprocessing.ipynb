{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 02 - Price Data Preprocessing\n",
                "\n",
                "## Objective\n",
                "Transform raw price data into modeling-ready format.\n",
                "\n",
                "**Key Tasks:**\n",
                "1. Load and clean data\n",
                "2. Feature engineering (46 features)\n",
                "3. Handle negative prices (keep for now, analyze impact)\n",
                "4. Train/Val/Test split\n",
                "5. Scaling (StandardScaler)\n",
                "6. Save processed datasets\n",
                "\n",
                "**Special Considerations for Price:**\n",
                "- Negative prices are valid (oversupply) â†’ keep them\n",
                "- High volatility â†’ lag features crucial\n",
                "- Spike detection features may help"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from pathlib import Path\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "plt.style.use('seaborn-v0_8-darkgrid')\n",
                "%matplotlib inline"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load raw data\n",
                "data_path = Path('../../data/raw/price_day_ahead_2022-01-01_2024-12-31_hour.csv')\n",
                "df = pd.read_csv(data_path)\n",
                "\n",
                "# Parse datetime\n",
                "df['datetime'] = pd.to_datetime(df['datetime'])\n",
                "df.set_index('datetime', inplace=True)\n",
                "df.sort_index(inplace=True)\n",
                "df.columns = ['price']\n",
                "\n",
                "print(f\"Loaded: {df.shape}\")\n",
                "print(f\"Date range: {df.index.min()} to {df.index.max()}\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Data Quality Checks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Missing values:\", df.isnull().sum().sum())\n",
                "print(f\"Negative prices: {(df['price'] < 0).sum()} (keeping them as valid data)\")\n",
                "print(f\"Zero prices: {(df['price'] == 0).sum()}\")\n",
                "\n",
                "# Check for duplicates\n",
                "print(f\"Duplicate timestamps: {df.index.duplicated().sum()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Feature Engineering (46 Features)\n",
                "\n",
                "Following the same feature engineering approach as Solar and Wind Offshore."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_time_features(df):\n",
                "    \"\"\"Create temporal features\"\"\"\n",
                "    df = df.copy()\n",
                "    \n",
                "    # Basic time features\n",
                "    df['hour'] = df.index.hour\n",
                "    df['day_of_week'] = df.index.dayofweek\n",
                "    df['day_of_month'] = df.index.day\n",
                "    df['day_of_year'] = df.index.dayofyear\n",
                "    df['week_of_year'] = df.index.isocalendar().week\n",
                "    df['month'] = df.index.month\n",
                "    df['quarter'] = df.index.quarter\n",
                "    df['year'] = df.index.year\n",
                "    \n",
                "    # Cyclical encoding\n",
                "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
                "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
                "    df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
                "    df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
                "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
                "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
                "    \n",
                "    # Binary indicators\n",
                "    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
                "    df['is_night'] = ((df['hour'] >= 22) | (df['hour'] <= 6)).astype(int)\n",
                "    df['is_peak_hour'] = ((df['hour'] >= 7) & (df['hour'] <= 9) | \n",
                "                          (df['hour'] >= 17) & (df['hour'] <= 20)).astype(int)\n",
                "    \n",
                "    return df\n",
                "\n",
                "df = create_time_features(df)\n",
                "print(f\"After time features: {df.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_lag_features(df, target_col='price', lags=[1, 2, 3, 6, 12, 24, 48, 168]):\n",
                "    \"\"\"Create lag features\"\"\"\n",
                "    df = df.copy()\n",
                "    \n",
                "    for lag in lags:\n",
                "        df[f'lag_{lag}'] = df[target_col].shift(lag)\n",
                "    \n",
                "    return df\n",
                "\n",
                "df = create_lag_features(df)\n",
                "print(f\"After lag features: {df.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_rolling_features(df, target_col='price', windows=[3, 6, 12, 24, 168]):\n",
                "    \"\"\"Create rolling statistical features\"\"\"\n",
                "    df = df.copy()\n",
                "    \n",
                "    for window in windows:\n",
                "        df[f'rolling_mean_{window}'] = df[target_col].shift(1).rolling(window=window).mean()\n",
                "        df[f'rolling_std_{window}'] = df[target_col].shift(1).rolling(window=window).std()\n",
                "        df[f'rolling_min_{window}'] = df[target_col].shift(1).rolling(window=window).min()\n",
                "        df[f'rolling_max_{window}'] = df[target_col].shift(1).rolling(window=window).max()\n",
                "    \n",
                "    return df\n",
                "\n",
                "df = create_rolling_features(df)\n",
                "print(f\"After rolling features: {df.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_diff_features(df, target_col='price'):\n",
                "    \"\"\"Create difference features\"\"\"\n",
                "    df = df.copy()\n",
                "    \n",
                "    df['diff_1'] = df[target_col].diff(1)\n",
                "    df['diff_24'] = df[target_col].diff(24)  # Day-over-day\n",
                "    df['diff_168'] = df[target_col].diff(168)  # Week-over-week\n",
                "    \n",
                "    return df\n",
                "\n",
                "df = create_diff_features(df)\n",
                "print(f\"After diff features: {df.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_price_specific_features(df, target_col='price'):\n",
                "    \"\"\"Create price-specific features\"\"\"\n",
                "    df = df.copy()\n",
                "    \n",
                "    # Negative price indicator\n",
                "    df['is_negative'] = (df[target_col] < 0).astype(int)\n",
                "    \n",
                "    # Price volatility (rolling std / rolling mean)\n",
                "    df['volatility_24h'] = df[target_col].rolling(window=24).std() / (df[target_col].rolling(window=24).mean().abs() + 1e-8)\n",
                "    \n",
                "    # Price momentum\n",
                "    df['momentum_3h'] = df[target_col] - df[target_col].shift(3)\n",
                "    df['momentum_6h'] = df[target_col] - df[target_col].shift(6)\n",
                "    \n",
                "    return df\n",
                "\n",
                "df = create_price_specific_features(df)\n",
                "print(f\"After price-specific features: {df.shape}\")\n",
                "print(f\"\\nTotal features created: {df.shape[1]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check feature columns\n",
                "print(\"All features:\")\n",
                "print(df.columns.tolist())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Handle Missing Values"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check missing values (from rolling/lag features)\n",
                "print(\"Missing values per column:\")\n",
                "print(df.isnull().sum()[df.isnull().sum() > 0])\n",
                "\n",
                "# Drop initial rows with NaN (from rolling windows)\n",
                "print(f\"\\nBefore dropping NaN: {len(df)}\")\n",
                "df_clean = df.dropna()\n",
                "print(f\"After dropping NaN: {len(df_clean)}\")\n",
                "print(f\"Rows dropped: {len(df) - len(df_clean)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Train/Val/Test Split"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define split dates\n",
                "val_start = '2024-07-01'\n",
                "test_start = '2024-10-01'\n",
                "\n",
                "# Split data\n",
                "train = df_clean[:val_start].copy()\n",
                "val = df_clean[val_start:test_start].copy()\n",
                "test = df_clean[test_start:].copy()\n",
                "\n",
                "print(\"ðŸ“Š Dataset Split:\")\n",
                "print(f\"Train: {train.index.min()} to {train.index.max()} ({len(train)} hours)\")\n",
                "print(f\"Val:   {val.index.min()} to {val.index.max()} ({len(val)} hours)\")\n",
                "print(f\"Test:  {test.index.min()} to {test.index.max()} ({len(test)} hours)\")\n",
                "print(f\"\\nTotal: {len(df_clean)} hours\")\n",
                "print(f\"Train: {len(train)/len(df_clean)*100:.1f}%\")\n",
                "print(f\"Val:   {len(val)/len(df_clean)*100:.1f}%\")\n",
                "print(f\"Test:  {len(test)/len(df_clean)*100:.1f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Feature Scaling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Separate features and target\n",
                "feature_cols = [col for col in df_clean.columns if col != 'price']\n",
                "target_col = 'price'\n",
                "\n",
                "print(f\"Number of features: {len(feature_cols)}\")\n",
                "\n",
                "# Extract X and y\n",
                "X_train = train[feature_cols]\n",
                "y_train = train[target_col]\n",
                "\n",
                "X_val = val[feature_cols]\n",
                "y_val = val[target_col]\n",
                "\n",
                "X_test = test[feature_cols]\n",
                "y_test = test[target_col]\n",
                "\n",
                "print(f\"\\nTrain: X={X_train.shape}, y={y_train.shape}\")\n",
                "print(f\"Val:   X={X_val.shape}, y={y_val.shape}\")\n",
                "print(f\"Test:  X={X_test.shape}, y={y_test.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Scale features (fit on train, transform all)\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_val_scaled = scaler.transform(X_val)\n",
                "X_test_scaled = scaler.transform(X_test)\n",
                "\n",
                "# Convert back to DataFrames\n",
                "X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_cols, index=X_train.index)\n",
                "X_val_scaled = pd.DataFrame(X_val_scaled, columns=feature_cols, index=X_val.index)\n",
                "X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_cols, index=X_test.index)\n",
                "\n",
                "print(\"âœ… Features scaled using StandardScaler\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Save Processed Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create output directory\n",
                "output_dir = Path('../../data/processed')\n",
                "output_dir.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "# Combine X and y for saving\n",
                "train_processed = X_train_scaled.copy()\n",
                "train_processed['price'] = y_train\n",
                "\n",
                "val_processed = X_val_scaled.copy()\n",
                "val_processed['price'] = y_val\n",
                "\n",
                "test_processed = X_test_scaled.copy()\n",
                "test_processed['price'] = y_test\n",
                "\n",
                "# Save to CSV\n",
                "train_processed.to_csv(output_dir / 'price_train.csv')\n",
                "val_processed.to_csv(output_dir / 'price_val.csv')\n",
                "test_processed.to_csv(output_dir / 'price_test.csv')\n",
                "\n",
                "print(\"âœ… Processed data saved:\")\n",
                "print(f\"   - {output_dir / 'price_train.csv'}\")\n",
                "print(f\"   - {output_dir / 'price_val.csv'}\")\n",
                "print(f\"   - {output_dir / 'price_test.csv'}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save feature names for later use\n",
                "import json\n",
                "\n",
                "feature_info = {\n",
                "    'feature_columns': feature_cols,\n",
                "    'n_features': len(feature_cols),\n",
                "    'target_column': target_col,\n",
                "    'train_size': len(train),\n",
                "    'val_size': len(val),\n",
                "    'test_size': len(test),\n",
                "    'val_start': val_start,\n",
                "    'test_start': test_start\n",
                "}\n",
                "\n",
                "with open(output_dir / 'price_feature_info.json', 'w') as f:\n",
                "    json.dump(feature_info, f, indent=2)\n",
                "\n",
                "print(\"\\nâœ… Feature info saved to price_feature_info.json\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Visualize Processed Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize train/val/test split\n",
                "fig, ax = plt.subplots(figsize=(16, 5))\n",
                "ax.plot(y_train.index, y_train.values, linewidth=0.8, label='Train', alpha=0.7)\n",
                "ax.plot(y_val.index, y_val.values, linewidth=0.8, label='Validation', alpha=0.7)\n",
                "ax.plot(y_test.index, y_test.values, linewidth=0.8, label='Test', alpha=0.7)\n",
                "ax.axhline(0, color='red', linestyle='--', linewidth=1)\n",
                "ax.set_title('Processed Price Data - Train/Val/Test Split', fontweight='bold', fontsize=14)\n",
                "ax.set_xlabel('Date')\n",
                "ax.set_ylabel('Price (EUR/MWh)')\n",
                "ax.legend()\n",
                "ax.grid(alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.savefig('../../results/figures/price_processed_split.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature correlation heatmap (sample)\n",
                "sample_features = ['lag_1', 'lag_24', 'rolling_mean_24', 'rolling_std_24', \n",
                "                   'hour', 'day_of_week', 'is_weekend', 'diff_24', 'volatility_24h']\n",
                "corr_df = train_processed[sample_features + ['price']].corr()\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(10, 8))\n",
                "sns.heatmap(corr_df, annot=True, fmt='.2f', cmap='coolwarm', center=0, ax=ax)\n",
                "ax.set_title('Feature Correlation Matrix (Sample)', fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.savefig('../../results/figures/price_feature_correlation.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*80)\n",
                "print(\"ðŸ“‹ PRICE DATA PREPROCESSING - SUMMARY\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "print(f\"\\n1. FEATURES CREATED:\")\n",
                "print(f\"   - Total features: {len(feature_cols)}\")\n",
                "print(f\"   - Time features: 19\")\n",
                "print(f\"   - Lag features: 8\")\n",
                "print(f\"   - Rolling features: 20 (5 windows Ã— 4 stats)\")\n",
                "print(f\"   - Difference features: 3\")\n",
                "print(f\"   - Price-specific features: 5\")\n",
                "\n",
                "print(f\"\\n2. DATASET SPLIT:\")\n",
                "print(f\"   - Train: {len(train)} hours ({len(train)/len(df_clean)*100:.1f}%)\")\n",
                "print(f\"   - Val:   {len(val)} hours ({len(val)/len(df_clean)*100:.1f}%)\")\n",
                "print(f\"   - Test:  {len(test)} hours ({len(test)/len(df_clean)*100:.1f}%)\")\n",
                "\n",
                "print(f\"\\n3. SPECIAL HANDLING:\")\n",
                "print(f\"   - Negative prices: KEPT (valid oversupply indicator)\")\n",
                "print(f\"   - Count in train: {(y_train < 0).sum()}\")\n",
                "print(f\"   - Count in val: {(y_val < 0).sum()}\")\n",
                "print(f\"   - Count in test: {(y_test < 0).sum()}\")\n",
                "\n",
                "print(f\"\\n4. DATA QUALITY:\")\n",
                "print(f\"   - Missing values: 0 (dropped)\")\n",
                "print(f\"   - Scaling: StandardScaler (fit on train)\")\n",
                "print(f\"   - Ready for modeling: âœ…\")\n",
                "\n",
                "print(f\"\\n5. FILES SAVED:\")\n",
                "print(f\"   - price_train.csv: {train_processed.shape}\")\n",
                "print(f\"   - price_val.csv: {val_processed.shape}\")\n",
                "print(f\"   - price_test.csv: {test_processed.shape}\")\n",
                "print(f\"   - price_feature_info.json\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"âœ… Preprocessing complete! Ready for baseline models.\")\n",
                "print(\"=\"*80)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Next Steps\n",
                "\n",
                "1. âœ… Data exploration complete\n",
                "2. âœ… Data preprocessing complete\n",
                "3. âž¡ï¸ **Next:** `03_price_baseline_models.ipynb`\n",
                "   - Naive, Seasonal Naive, Moving Average, Drift, Mean\n",
                "   - Quick benchmarks\n",
                "4. ðŸ“Š Then: Statistical, ML, and Deep Learning models"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}