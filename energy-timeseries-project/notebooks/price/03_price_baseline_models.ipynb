{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 03 - Price Baseline Models\n",
                "\n",
                "## Objective\n",
                "Establish baseline performance using simple forecasting methods.\n",
                "\n",
                "**Models:**\n",
                "1. Naive (last observation)\n",
                "2. Seasonal Naive (same hour yesterday)\n",
                "3. Moving Average (24h)\n",
                "4. Drift (linear trend)\n",
                "5. Mean (historical average)\n",
                "\n",
                "**Goal:** These baselines set the minimum performance threshold. \n",
                "Any advanced model must beat these simple approaches."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from pathlib import Path\n",
                "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "plt.style.use('seaborn-v0_8-darkgrid')\n",
                "%matplotlib inline"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Processed Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load processed datasets\n",
                "data_dir = Path('../../data/processed')\n",
                "\n",
                "train = pd.read_csv(data_dir / 'price_train.csv', index_col=0, parse_dates=True)\n",
                "val = pd.read_csv(data_dir / 'price_val.csv', index_col=0, parse_dates=True)\n",
                "test = pd.read_csv(data_dir / 'price_test.csv', index_col=0, parse_dates=True)\n",
                "\n",
                "print(f\"Train: {train.shape}\")\n",
                "print(f\"Val:   {val.shape}\")\n",
                "print(f\"Test:  {test.shape}\")\n",
                "\n",
                "# Extract target variable\n",
                "y_train = train['price']\n",
                "y_val = val['price']\n",
                "y_test = test['price']\n",
                "\n",
                "print(f\"\\nTarget variable ranges:\")\n",
                "print(f\"Train: [{y_train.min():.2f}, {y_train.max():.2f}]\")\n",
                "print(f\"Val:   [{y_val.min():.2f}, {y_val.max():.2f}]\")\n",
                "print(f\"Test:  [{y_test.min():.2f}, {y_test.max():.2f}]\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Baseline Models Implementation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate_model(y_true, y_pred, model_name):\n",
                "    \"\"\"Calculate evaluation metrics\"\"\"\n",
                "    mse = mean_squared_error(y_true, y_pred)\n",
                "    rmse = np.sqrt(mse)\n",
                "    mae = mean_absolute_error(y_true, y_pred)\n",
                "    r2 = r2_score(y_true, y_pred)\n",
                "    mape = np.mean(np.abs((y_true - y_pred) / (y_true.abs() + 1e-8))) * 100\n",
                "    \n",
                "    return {\n",
                "        'Model': model_name,\n",
                "        'MSE': mse,\n",
                "        'RMSE': rmse,\n",
                "        'MAE': mae,\n",
                "        'R¬≤': r2,\n",
                "        'MAPE': mape\n",
                "    }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.1 Naive Model (Last Observation)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Naive forecast: t+1 = t\n",
                "naive_test = y_train.iloc[-1] * np.ones(len(y_test))\n",
                "naive_results = evaluate_model(y_test, naive_test, 'Naive')\n",
                "\n",
                "print(\"Naive Model Results:\")\n",
                "print(f\"  R¬≤: {naive_results['R¬≤']:.4f}\")\n",
                "print(f\"  RMSE: {naive_results['RMSE']:.2f}\")\n",
                "print(f\"  MAE: {naive_results['MAE']:.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2 Seasonal Naive (Same Hour Yesterday)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Seasonal Naive: Use same hour from 24 hours ago\n",
                "# For test set, use last 24 hours from train\n",
                "last_24_train = y_train.iloc[-24:].values\n",
                "seasonal_naive_test = np.tile(last_24_train, int(np.ceil(len(y_test)/24)))[:len(y_test)]\n",
                "\n",
                "seasonal_naive_results = evaluate_model(y_test, seasonal_naive_test, 'Seasonal Naive (24h)')\n",
                "\n",
                "print(\"Seasonal Naive Model Results:\")\n",
                "print(f\"  R¬≤: {seasonal_naive_results['R¬≤']:.4f}\")\n",
                "print(f\"  RMSE: {seasonal_naive_results['RMSE']:.2f}\")\n",
                "print(f\"  MAE: {seasonal_naive_results['MAE']:.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.3 Moving Average (24-hour window)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Moving Average: Mean of last 24 hours\n",
                "ma_24_value = y_train.iloc[-24:].mean()\n",
                "ma_test = ma_24_value * np.ones(len(y_test))\n",
                "\n",
                "ma_results = evaluate_model(y_test, ma_test, 'Moving Average (24h)')\n",
                "\n",
                "print(\"Moving Average Model Results:\")\n",
                "print(f\"  R¬≤: {ma_results['R¬≤']:.4f}\")\n",
                "print(f\"  RMSE: {ma_results['RMSE']:.2f}\")\n",
                "print(f\"  MAE: {ma_results['MAE']:.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.4 Drift Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Drift: Linear trend from first to last observation\n",
                "drift_slope = (y_train.iloc[-1] - y_train.iloc[0]) / (len(y_train) - 1)\n",
                "drift_test = y_train.iloc[-1] + drift_slope * np.arange(1, len(y_test) + 1)\n",
                "\n",
                "drift_results = evaluate_model(y_test, drift_test, 'Drift')\n",
                "\n",
                "print(\"Drift Model Results:\")\n",
                "print(f\"  R¬≤: {drift_results['R¬≤']:.4f}\")\n",
                "print(f\"  RMSE: {drift_results['RMSE']:.2f}\")\n",
                "print(f\"  MAE: {drift_results['MAE']:.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.5 Mean Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Mean: Historical average\n",
                "mean_value = y_train.mean()\n",
                "mean_test = mean_value * np.ones(len(y_test))\n",
                "\n",
                "mean_results = evaluate_model(y_test, mean_test, 'Mean')\n",
                "\n",
                "print(\"Mean Model Results:\")\n",
                "print(f\"  R¬≤: {mean_results['R¬≤']:.4f}\")\n",
                "print(f\"  RMSE: {mean_results['RMSE']:.2f}\")\n",
                "print(f\"  MAE: {mean_results['MAE']:.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Results Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compile results\n",
                "results_df = pd.DataFrame([\n",
                "    naive_results,\n",
                "    seasonal_naive_results,\n",
                "    ma_results,\n",
                "    drift_results,\n",
                "    mean_results\n",
                "])\n",
                "\n",
                "# Sort by R¬≤\n",
                "results_df = results_df.sort_values('R¬≤', ascending=False)\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"BASELINE MODELS COMPARISON\")\n",
                "print(\"=\"*80)\n",
                "print(results_df.to_string(index=False))\n",
                "print(\"=\"*80)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize metrics\n",
                "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
                "\n",
                "# R¬≤\n",
                "axes[0, 0].barh(results_df['Model'], results_df['R¬≤'], color='steelblue', edgecolor='black')\n",
                "axes[0, 0].set_xlabel('R¬≤ Score')\n",
                "axes[0, 0].set_title('R¬≤ Score by Model', fontweight='bold')\n",
                "axes[0, 0].grid(alpha=0.3, axis='x')\n",
                "\n",
                "# RMSE\n",
                "axes[0, 1].barh(results_df['Model'], results_df['RMSE'], color='coral', edgecolor='black')\n",
                "axes[0, 1].set_xlabel('RMSE')\n",
                "axes[0, 1].set_title('RMSE by Model', fontweight='bold')\n",
                "axes[0, 1].grid(alpha=0.3, axis='x')\n",
                "\n",
                "# MAE\n",
                "axes[1, 0].barh(results_df['Model'], results_df['MAE'], color='seagreen', edgecolor='black')\n",
                "axes[1, 0].set_xlabel('MAE')\n",
                "axes[1, 0].set_title('MAE by Model', fontweight='bold')\n",
                "axes[1, 0].grid(alpha=0.3, axis='x')\n",
                "\n",
                "# MAPE\n",
                "axes[1, 1].barh(results_df['Model'], results_df['MAPE'], color='purple', edgecolor='black')\n",
                "axes[1, 1].set_xlabel('MAPE (%)')\n",
                "axes[1, 1].set_title('MAPE by Model', fontweight='bold')\n",
                "axes[1, 1].grid(alpha=0.3, axis='x')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('../../results/figures/price_baseline_metrics.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Forecast Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot forecasts (first 7 days of test)\n",
                "plot_days = 7\n",
                "plot_hours = plot_days * 24\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(16, 6))\n",
                "ax.plot(y_test.index[:plot_hours], y_test.values[:plot_hours], \n",
                "        linewidth=2, label='Actual', color='black', zorder=5)\n",
                "ax.plot(y_test.index[:plot_hours], naive_test[:plot_hours], \n",
                "        linewidth=1.5, label='Naive', alpha=0.7, linestyle='--')\n",
                "ax.plot(y_test.index[:plot_hours], seasonal_naive_test[:plot_hours], \n",
                "        linewidth=1.5, label='Seasonal Naive (24h)', alpha=0.7, linestyle='--')\n",
                "ax.plot(y_test.index[:plot_hours], ma_test[:plot_hours], \n",
                "        linewidth=1.5, label='Moving Average (24h)', alpha=0.7, linestyle='--')\n",
                "ax.axhline(0, color='red', linestyle='-', linewidth=1)\n",
                "ax.set_title(f'Baseline Models Forecast - First {plot_days} Days', fontweight='bold', fontsize=14)\n",
                "ax.set_xlabel('Date')\n",
                "ax.set_ylabel('Price (EUR/MWh)')\n",
                "ax.legend(loc='best')\n",
                "ax.grid(alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.savefig('../../results/figures/price_baseline_forecast.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Error Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Best baseline model\n",
                "best_model_name = results_df.iloc[0]['Model']\n",
                "best_r2 = results_df.iloc[0]['R¬≤']\n",
                "\n",
                "# Get predictions for best model\n",
                "if best_model_name == 'Naive':\n",
                "    best_pred = naive_test\n",
                "elif best_model_name == 'Seasonal Naive (24h)':\n",
                "    best_pred = seasonal_naive_test\n",
                "elif best_model_name == 'Moving Average (24h)':\n",
                "    best_pred = ma_test\n",
                "elif best_model_name == 'Drift':\n",
                "    best_pred = drift_test\n",
                "else:\n",
                "    best_pred = mean_test\n",
                "\n",
                "errors = y_test.values - best_pred\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Error distribution\n",
                "axes[0].hist(errors, bins=50, edgecolor='black', alpha=0.7)\n",
                "axes[0].axvline(0, color='red', linestyle='--', linewidth=2)\n",
                "axes[0].set_title(f'{best_model_name} - Error Distribution', fontweight='bold')\n",
                "axes[0].set_xlabel('Error (EUR/MWh)')\n",
                "axes[0].set_ylabel('Frequency')\n",
                "axes[0].grid(alpha=0.3)\n",
                "\n",
                "# Error over time\n",
                "axes[1].plot(y_test.index, errors, linewidth=0.5, alpha=0.7)\n",
                "axes[1].axhline(0, color='red', linestyle='--', linewidth=2)\n",
                "axes[1].set_title(f'{best_model_name} - Error Over Time', fontweight='bold')\n",
                "axes[1].set_xlabel('Date')\n",
                "axes[1].set_ylabel('Error (EUR/MWh)')\n",
                "axes[1].grid(alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('../../results/figures/price_baseline_errors.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nError statistics for {best_model_name}:\")\n",
                "print(f\"Mean error: {errors.mean():.2f}\")\n",
                "print(f\"Std error: {errors.std():.2f}\")\n",
                "print(f\"Min error: {errors.min():.2f}\")\n",
                "print(f\"Max error: {errors.max():.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Save Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save metrics\n",
                "results_dir = Path('../../results/metrics')\n",
                "results_dir.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "results_df.to_csv(results_dir / 'price_baseline_metrics.csv', index=False)\n",
                "print(f\"\\n‚úÖ Results saved to {results_dir / 'price_baseline_metrics.csv'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*80)\n",
                "print(\"üìã PRICE BASELINE MODELS - SUMMARY\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "print(f\"\\nüèÜ BEST BASELINE MODEL: {best_model_name}\")\n",
                "print(f\"   R¬≤: {best_r2:.4f}\")\n",
                "print(f\"   RMSE: {results_df.iloc[0]['RMSE']:.2f} EUR/MWh\")\n",
                "print(f\"   MAE: {results_df.iloc[0]['MAE']:.2f} EUR/MWh\")\n",
                "\n",
                "print(f\"\\nüìä ALL MODELS:\")\n",
                "for _, row in results_df.iterrows():\n",
                "    print(f\"   {row['Model']:25s} R¬≤={row['R¬≤']:7.4f}  RMSE={row['RMSE']:6.2f}\")\n",
                "\n",
                "print(f\"\\nüéØ BASELINE THRESHOLD:\")\n",
                "print(f\"   Advanced models must achieve R¬≤ > {best_r2:.4f}\")\n",
                "print(f\"   Expected R¬≤ for best models: 0.85 - 0.92 (price is challenging!)\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"‚úÖ Baseline models complete! Ready for statistical models.\")\n",
                "print(\"=\"*80)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Next Steps\n",
                "\n",
                "1. ‚úÖ Data exploration\n",
                "2. ‚úÖ Data preprocessing  \n",
                "3. ‚úÖ Baseline models\n",
                "4. ‚û°Ô∏è **Next:** `04_price_statistical_models.ipynb`\n",
                "   - SARIMA\n",
                "   - ETS (Exponential Smoothing)\n",
                "5. üìä Then: ML tree models and deep learning"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}