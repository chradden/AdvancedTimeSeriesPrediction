{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3ae6b25",
   "metadata": {},
   "source": [
    "# ðŸš€ LSTM Optimization - Google Colab GPU Edition\n",
    "\n",
    "**Systematisches LSTM-Tuning mit GPU-Beschleunigung**\n",
    "\n",
    "**Setup:**\n",
    "- Runtime â†’ Change runtime type â†’ GPU (T4 oder A100)\n",
    "- ~10-50x schneller als CPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0118af2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"\\nðŸš€ GPU should show above!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a5d9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone Repository\n",
    "!git clone https://github.com/chradden/AdvancedTimeSeriesPrediction.git\n",
    "%cd AdvancedTimeSeriesPrediction/energy-timeseries-project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53f978a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Dependencies\n",
    "!pip install -q pandas numpy matplotlib seaborn scikit-learn tensorflow keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4dc8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import time\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# GPU Config\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"âœ… GPU configured: {len(gpus)} device(s)\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"âš ï¸ No GPU found - training will be slow!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590fc573",
   "metadata": {},
   "source": [
    "## âš™ï¸ Configuration - Zeitreihe auswÃ¤hlen\n",
    "\n",
    "**Ã„ndere hier die Zeitreihe:**\n",
    "- `'solar'` - Solarenergie-Erzeugung\n",
    "- `'wind_offshore'` - Offshore-Windenergie  \n",
    "- `'wind_onshore'` - Onshore-Windenergie\n",
    "- `'price'` - Strompreise\n",
    "- `'consumption'` - Energieverbrauch\n",
    "\n",
    "Oder setze `RUN_ALL_SERIES = True` um alle Zeitreihen nacheinander zu verarbeiten!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6738d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION - ZEITREIHE AUSWÃ„HLEN\n",
    "# ============================================================================\n",
    "\n",
    "# Option 1: Einzelne Zeitreihe (Standard)\n",
    "SERIES_NAME = 'solar'  # Ã„ndere hier: 'solar', 'wind_offshore', 'wind_onshore', 'price', 'consumption'\n",
    "\n",
    "# Option 2: Alle Zeitreihen nacheinander verarbeiten\n",
    "RUN_ALL_SERIES = False  # Setze auf True fÃ¼r Batch-Verarbeitung\n",
    "\n",
    "# VerfÃ¼gbare Zeitreihen\n",
    "AVAILABLE_SERIES = ['solar', 'wind_offshore', 'wind_onshore', 'price', 'consumption']\n",
    "\n",
    "# Serie(n) bestimmen\n",
    "if RUN_ALL_SERIES:\n",
    "    series_to_process = AVAILABLE_SERIES\n",
    "    print(\"ðŸ”„ MODE: Alle Zeitreihen werden nacheinander verarbeitet\")\n",
    "    print(f\"   â†’ {len(series_to_process)} Serien: {', '.join(series_to_process)}\")\n",
    "else:\n",
    "    series_to_process = [SERIES_NAME]\n",
    "    print(f\"ðŸ“Š MODE: Einzelne Zeitreihe\")\n",
    "    print(f\"   â†’ Serie: {SERIES_NAME}\")\n",
    "\n",
    "print(f\"\\nâœ… Konfiguration abgeschlossen!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8a4d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data for current series\n",
    "series_name = series_to_process[0] if not RUN_ALL_SERIES else SERIES_NAME\n",
    "\n",
    "train_df = pd.read_csv(f'data/processed/{series_name}_train.csv')\n",
    "val_df = pd.read_csv(f'data/processed/{series_name}_val.csv')\n",
    "test_df = pd.read_csv(f'data/processed/{series_name}_test.csv')\n",
    "\n",
    "print(f\"ðŸ“‚ Loading data for: {series_name.upper()}\")\n",
    "print(f\"   Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}\")\n",
    "print(f\"   Columns: {train_df.columns.tolist()[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e922ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine value column\n",
    "value_col = [c for c in train_df.columns if c in ['solar', 'price', 'value', \n",
    "                                                     'wind_offshore', 'wind_onshore', 'consumption']][0]\n",
    "feature_cols = [c for c in train_df.columns if c not in ['timestamp', value_col]]\n",
    "\n",
    "print(f\"Value column: {value_col}\")\n",
    "print(f\"Features: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e267b9",
   "metadata": {},
   "source": [
    "## ðŸ”§ Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bf4188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, target, seq_length):\n",
    "    \"\"\"Create sequences for LSTM\"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i + seq_length])\n",
    "        y.append(target[i + seq_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Scale data\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_train = scaler_X.fit_transform(train_df[feature_cols])\n",
    "y_train = scaler_y.fit_transform(train_df[[value_col]])\n",
    "\n",
    "X_val = scaler_X.transform(val_df[feature_cols])\n",
    "y_val = scaler_y.transform(val_df[[value_col]])\n",
    "\n",
    "X_test = scaler_X.transform(test_df[feature_cols])\n",
    "y_test_orig = test_df[value_col].values\n",
    "\n",
    "print(f\"âœ… Data scaled: X_train shape = {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7220dc5",
   "metadata": {},
   "source": [
    "## ðŸ§ª Experiment 1: Baseline LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f1f3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 24\n",
    "\n",
    "X_train_seq, y_train_seq = create_sequences(X_train, y_train.flatten(), seq_length)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val, y_val.flatten(), seq_length)\n",
    "X_test_seq, _ = create_sequences(X_test, np.zeros(len(X_test)), seq_length)\n",
    "y_test_seq = y_test_orig[seq_length:]\n",
    "\n",
    "print(f\"Sequences: Train={X_train_seq.shape}, Val={X_val_seq.shape}, Test={X_test_seq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814a9fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Baseline LSTM\n",
    "model = keras.Sequential([\n",
    "    layers.LSTM(64, return_sequences=True, input_shape=(seq_length, X_train.shape[1])),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.LSTM(32),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8c8da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "start = time.time()\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "train_time = time.time() - start\n",
    "\n",
    "print(f\"\\nâœ… Training completed in {train_time:.1f}s ({train_time/60:.1f} min)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cc2f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training History')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Train MAE')\n",
    "plt.plot(history.history['val_mae'], label='Val MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.title('MAE History')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70a2e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "y_pred_scaled = model.predict(X_test_seq).flatten()\n",
    "y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test_seq, y_pred))\n",
    "mae = mean_absolute_error(y_test_seq, y_pred)\n",
    "r2 = r2_score(y_test_seq, y_pred)\n",
    "\n",
    "print(f\"\\nðŸ“Š BASELINE LSTM RESULTS:\")\n",
    "print(f\"   RÂ² = {r2:.4f}\")\n",
    "print(f\"   RMSE = {rmse:.2f}\")\n",
    "print(f\"   MAE = {mae:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609290ad",
   "metadata": {},
   "source": [
    "## ðŸš€ Experiment 2: Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d156299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Bi-LSTM\n",
    "model_bilstm = keras.Sequential([\n",
    "    layers.Bidirectional(layers.LSTM(64, return_sequences=True), input_shape=(seq_length, X_train.shape[1])),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Bidirectional(layers.LSTM(32)),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "model_bilstm.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "model_bilstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bb6c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Bi-LSTM\n",
    "start = time.time()\n",
    "history_bilstm = model_bilstm.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "train_time_bilstm = time.time() - start\n",
    "\n",
    "print(f\"\\nâœ… Bi-LSTM training completed in {train_time_bilstm:.1f}s ({train_time_bilstm/60:.1f} min)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b719d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Bi-LSTM\n",
    "y_pred_bilstm_scaled = model_bilstm.predict(X_test_seq).flatten()\n",
    "y_pred_bilstm = scaler_y.inverse_transform(y_pred_bilstm_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "rmse_bilstm = np.sqrt(mean_squared_error(y_test_seq, y_pred_bilstm))\n",
    "mae_bilstm = mean_absolute_error(y_test_seq, y_pred_bilstm)\n",
    "r2_bilstm = r2_score(y_test_seq, y_pred_bilstm)\n",
    "\n",
    "print(f\"\\nðŸ“Š BI-LSTM RESULTS:\")\n",
    "print(f\"   RÂ² = {r2_bilstm:.4f}\")\n",
    "print(f\"   RMSE = {rmse_bilstm:.2f}\")\n",
    "print(f\"   MAE = {mae_bilstm:.2f}\")\n",
    "print(f\"\\nðŸŽ¯ Improvement: {((r2_bilstm - r2) / abs(r2)) * 100:+.2f}% RÂ²\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23440c5f",
   "metadata": {},
   "source": [
    "## ðŸ“Š Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a539c738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results DataFrame\n",
    "results = pd.DataFrame([\n",
    "    {'Model': 'LSTM', 'RÂ²': r2, 'RMSE': rmse, 'MAE': mae, 'Time': train_time},\n",
    "    {'Model': 'Bi-LSTM', 'RÂ²': r2_bilstm, 'RMSE': rmse_bilstm, 'MAE': mae_bilstm, 'Time': train_time_bilstm}\n",
    "])\n",
    "\n",
    "print(\"\\nðŸ† COMPARISON:\")\n",
    "print(results.to_string(index=False))\n",
    "\n",
    "# Save\n",
    "results.to_csv(f'results/metrics/lstm_colab_{series_name}.csv', index=False)\n",
    "print(f\"\\nâœ… Results saved to: results/metrics/lstm_colab_{series_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3220cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# RÂ² Comparison\n",
    "axes[0].bar(results['Model'], results['RÂ²'], color=['#3498db', '#e74c3c'])\n",
    "axes[0].set_ylabel('RÂ² Score')\n",
    "axes[0].set_title('RÂ² Comparison', fontweight='bold')\n",
    "axes[0].set_ylim([0.85, 1.0])\n",
    "axes[0].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# RMSE Comparison\n",
    "axes[1].bar(results['Model'], results['RMSE'], color=['#3498db', '#e74c3c'])\n",
    "axes[1].set_ylabel('RMSE')\n",
    "axes[1].set_title('RMSE Comparison', fontweight='bold')\n",
    "axes[1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'results/figures/lstm_colab_comparison_{series_name}.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ… Plot saved to: results/figures/lstm_colab_comparison_{series_name}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82612e98",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59f3d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model\n",
    "best_model = model_bilstm if r2_bilstm > r2 else model\n",
    "best_name = 'Bi-LSTM' if r2_bilstm > r2 else 'LSTM'\n",
    "\n",
    "best_model.save(f'models/lstm_best_{series_name}.keras')\n",
    "print(f\"âœ… Best model ({best_name}) saved to: models/lstm_best_{series_name}.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264120af",
   "metadata": {},
   "source": [
    "## ðŸŽ¨ Experiment 3: Generative Models\n",
    "\n",
    "### Autoencoder fÃ¼r Time Series Forecasting\n",
    "Autoencoders lernen kompakte ReprÃ¤sentationen und kÃ¶nnen Anomalien erkennen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f03585b",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Summary\n",
    "\n",
    "**GPU Speedup Check:**\n",
    "- If you see ~10-50x faster training vs CPU, GPU is working!\n",
    "- Expected on CPU: ~100-200s per epoch\n",
    "- Expected on GPU: ~2-5s per epoch\n",
    "\n",
    "**Next Steps:**\n",
    "1. Try longer sequences (48h, 168h)\n",
    "2. Experiment with more units (128, 256)\n",
    "3. Add Attention mechanism\n",
    "4. Test on other time series (price, wind_offshore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060ad48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LSTM Autoencoder\n",
    "latent_dim = 16\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = keras.Input(shape=(seq_length, X_train.shape[1]))\n",
    "x = layers.LSTM(64, return_sequences=True)(encoder_inputs)\n",
    "x = layers.LSTM(32, return_sequences=False)(x)\n",
    "encoder_outputs = layers.Dense(latent_dim, activation='relu')(x)\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = layers.RepeatVector(seq_length)(encoder_outputs)\n",
    "x = layers.LSTM(32, return_sequences=True)(decoder_inputs)\n",
    "x = layers.LSTM(64, return_sequences=True)(x)\n",
    "decoder_outputs = layers.TimeDistributed(layers.Dense(X_train.shape[1]))(x)\n",
    "\n",
    "# Autoencoder\n",
    "autoencoder = keras.Model(encoder_inputs, decoder_outputs, name='autoencoder')\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "print(\"ðŸ”§ Autoencoder Architecture:\")\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b633f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Autoencoder\n",
    "start = time.time()\n",
    "history_ae = autoencoder.fit(\n",
    "    X_train_seq, X_train_seq,  # Reconstruct input\n",
    "    validation_data=(X_val_seq, X_val_seq),\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "train_time_ae = time.time() - start\n",
    "print(f\"\\nâœ… Autoencoder training completed in {train_time_ae:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb58576d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build forecasting model on top of encoder\n",
    "encoder_model = keras.Model(encoder_inputs, encoder_outputs, name='encoder')\n",
    "\n",
    "# Freeze encoder\n",
    "for layer in encoder_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add forecasting head\n",
    "forecast_inputs = keras.Input(shape=(seq_length, X_train.shape[1]))\n",
    "encoded = encoder_model(forecast_inputs)\n",
    "x = layers.Dense(32, activation='relu')(encoded)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "forecast_output = layers.Dense(1)(x)\n",
    "\n",
    "model_ae_forecast = keras.Model(forecast_inputs, forecast_output, name='ae_forecast')\n",
    "model_ae_forecast.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "print(\"ðŸ”§ AE-Forecast Model:\")\n",
    "model_ae_forecast.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8200a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train forecasting model\n",
    "start = time.time()\n",
    "history_ae_forecast = model_ae_forecast.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "train_time_ae_forecast = time.time() - start\n",
    "\n",
    "# Evaluate\n",
    "y_pred_ae_scaled = model_ae_forecast.predict(X_test_seq).flatten()\n",
    "y_pred_ae = scaler_y.inverse_transform(y_pred_ae_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "rmse_ae = np.sqrt(mean_squared_error(y_test_seq, y_pred_ae))\n",
    "mae_ae = mean_absolute_error(y_test_seq, y_pred_ae)\n",
    "r2_ae = r2_score(y_test_seq, y_pred_ae)\n",
    "\n",
    "print(f\"\\nðŸ“Š AUTOENCODER-FORECAST RESULTS:\")\n",
    "print(f\"   RÂ² = {r2_ae:.4f}\")\n",
    "print(f\"   RMSE = {rmse_ae:.2f}\")\n",
    "print(f\"   MAE = {mae_ae:.2f}\")\n",
    "print(f\"   Time = {train_time_ae_forecast:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f910f3b1",
   "metadata": {},
   "source": [
    "### VAE (Variational Autoencoder)\n",
    "VAE lernt probabilistische latente ReprÃ¤sentationen - nÃ¼tzlich fÃ¼r Unsicherheitsquantifizierung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dcf1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling layer for VAE\n",
    "class Sampling(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.random.normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "# VAE Encoder\n",
    "vae_inputs = keras.Input(shape=(seq_length, X_train.shape[1]))\n",
    "x = layers.LSTM(64, return_sequences=True)(vae_inputs)\n",
    "x = layers.LSTM(32)(x)\n",
    "z_mean = layers.Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "\n",
    "vae_encoder = keras.Model(vae_inputs, [z_mean, z_log_var, z], name='vae_encoder')\n",
    "\n",
    "# VAE Decoder\n",
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "x = layers.RepeatVector(seq_length)(latent_inputs)\n",
    "x = layers.LSTM(32, return_sequences=True)(x)\n",
    "x = layers.LSTM(64, return_sequences=True)(x)\n",
    "vae_outputs = layers.TimeDistributed(layers.Dense(X_train.shape[1]))(x)\n",
    "\n",
    "vae_decoder = keras.Model(latent_inputs, vae_outputs, name='vae_decoder')\n",
    "\n",
    "# VAE full model\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "    \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.mean_squared_error(data, reconstruction), axis=1\n",
    "                )\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        \n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "vae = VAE(vae_encoder, vae_decoder)\n",
    "vae.compile(optimizer='adam')\n",
    "\n",
    "print(\"ðŸ”§ VAE built successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5acee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train VAE\n",
    "start = time.time()\n",
    "history_vae = vae.fit(\n",
    "    X_train_seq, X_train_seq,\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    verbose=1\n",
    ")\n",
    "train_time_vae = time.time() - start\n",
    "print(f\"\\nâœ… VAE training completed in {train_time_vae:.1f}s\")\n",
    "\n",
    "# Build forecasting model on VAE encoder\n",
    "vae_encoder_mean = keras.Model(vae_inputs, z_mean, name='vae_encoder_mean')\n",
    "\n",
    "for layer in vae_encoder_mean.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "forecast_inputs_vae = keras.Input(shape=(seq_length, X_train.shape[1]))\n",
    "encoded_vae = vae_encoder_mean(forecast_inputs_vae)\n",
    "x = layers.Dense(32, activation='relu')(encoded_vae)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "forecast_output_vae = layers.Dense(1)(x)\n",
    "\n",
    "model_vae_forecast = keras.Model(forecast_inputs_vae, forecast_output_vae, name='vae_forecast')\n",
    "model_vae_forecast.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train\n",
    "start = time.time()\n",
    "history_vae_forecast = model_vae_forecast.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "train_time_vae_forecast = time.time() - start\n",
    "\n",
    "# Evaluate\n",
    "y_pred_vae_scaled = model_vae_forecast.predict(X_test_seq).flatten()\n",
    "y_pred_vae = scaler_y.inverse_transform(y_pred_vae_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "rmse_vae = np.sqrt(mean_squared_error(y_test_seq, y_pred_vae))\n",
    "mae_vae = mean_absolute_error(y_test_seq, y_pred_vae)\n",
    "r2_vae = r2_score(y_test_seq, y_pred_vae)\n",
    "\n",
    "print(f\"\\nðŸ“Š VAE-FORECAST RESULTS:\")\n",
    "print(f\"   RÂ² = {r2_vae:.4f}\")\n",
    "print(f\"   RMSE = {rmse_vae:.2f}\")\n",
    "print(f\"   MAE = {mae_vae:.2f}\")\n",
    "print(f\"   Time = {train_time_vae_forecast:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b20d35",
   "metadata": {},
   "source": [
    "## âš¡ Experiment 4: Advanced Deep Learning Models\n",
    "\n",
    "### N-BEATS (Neural Basis Expansion Analysis for Time Series)\n",
    "State-of-the-art fÃ¼r univariate Zeitreihen - speziell designed fÃ¼r Forecasting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac401304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Darts (for N-BEATS, N-HiTS)\n",
    "!pip install -q darts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb12849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Darts\n",
    "from darts import TimeSeries\n",
    "from darts.models import NBEATSModel, NHiTSModel\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "\n",
    "print(\"âœ… Darts imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba8b617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for Darts\n",
    "train_df_ts = train_df.copy()\n",
    "train_df_ts['timestamp'] = pd.to_datetime(train_df_ts['timestamp'])\n",
    "train_df_ts = train_df_ts.set_index('timestamp')\n",
    "\n",
    "val_df_ts = val_df.copy()\n",
    "val_df_ts['timestamp'] = pd.to_datetime(val_df_ts['timestamp'])\n",
    "val_df_ts = val_df_ts.set_index('timestamp')\n",
    "\n",
    "test_df_ts = test_df.copy()\n",
    "test_df_ts['timestamp'] = pd.to_datetime(test_df_ts['timestamp'])\n",
    "test_df_ts = test_df_ts.set_index('timestamp')\n",
    "\n",
    "# Create TimeSeries objects\n",
    "ts_train = TimeSeries.from_dataframe(train_df_ts, value_cols=value_col, freq='H')\n",
    "ts_val = TimeSeries.from_dataframe(val_df_ts, value_cols=value_col, freq='H')\n",
    "ts_test = TimeSeries.from_dataframe(test_df_ts, value_cols=value_col, freq='H')\n",
    "\n",
    "# Scale\n",
    "scaler_darts = Scaler()\n",
    "ts_train_scaled = scaler_darts.fit_transform(ts_train)\n",
    "ts_val_scaled = scaler_darts.transform(ts_val)\n",
    "ts_test_scaled = scaler_darts.transform(ts_test)\n",
    "\n",
    "print(f\"âœ… Darts TimeSeries created: {len(ts_train)} train samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61adb3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build N-BEATS model\n",
    "model_nbeats = NBEATSModel(\n",
    "    input_chunk_length=168,  # 1 week lookback\n",
    "    output_chunk_length=24,  # 1 day forecast\n",
    "    num_stacks=30,\n",
    "    num_blocks=1,\n",
    "    num_layers=4,\n",
    "    layer_widths=256,\n",
    "    expansion_coefficient_dim=5,\n",
    "    n_epochs=50,\n",
    "    batch_size=64,\n",
    "    optimizer_kwargs={'lr': 1e-3},\n",
    "    pl_trainer_kwargs={\n",
    "        'accelerator': 'gpu',\n",
    "        'devices': 1,\n",
    "        'enable_progress_bar': True,\n",
    "    },\n",
    "    model_name='nbeats_solar',\n",
    "    force_reset=True,\n",
    "    save_checkpoints=True\n",
    ")\n",
    "\n",
    "print(\"ðŸ”§ N-BEATS model configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c320eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train N-BEATS\n",
    "print(\"ðŸš€ Training N-BEATS (this will be FAST on GPU!)...\")\n",
    "start = time.time()\n",
    "\n",
    "model_nbeats.fit(\n",
    "    series=ts_train_scaled,\n",
    "    val_series=ts_val_scaled,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "train_time_nbeats = time.time() - start\n",
    "print(f\"\\nâœ… N-BEATS training completed in {train_time_nbeats:.1f}s ({train_time_nbeats/60:.1f} min)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5eca8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with N-BEATS\n",
    "n_pred = len(ts_test)\n",
    "pred_nbeats_scaled = model_nbeats.predict(n=n_pred, series=ts_train_scaled)\n",
    "pred_nbeats = scaler_darts.inverse_transform(pred_nbeats_scaled)\n",
    "\n",
    "# Extract values\n",
    "y_pred_nbeats = pred_nbeats.values().flatten()\n",
    "y_test_nbeats = ts_test.values().flatten()\n",
    "\n",
    "# Ensure same length\n",
    "min_len = min(len(y_pred_nbeats), len(y_test_nbeats))\n",
    "y_pred_nbeats = y_pred_nbeats[:min_len]\n",
    "y_test_nbeats = y_test_nbeats[:min_len]\n",
    "\n",
    "# Evaluate\n",
    "rmse_nbeats = np.sqrt(mean_squared_error(y_test_nbeats, y_pred_nbeats))\n",
    "mae_nbeats = mean_absolute_error(y_test_nbeats, y_pred_nbeats)\n",
    "r2_nbeats = r2_score(y_test_nbeats, y_pred_nbeats)\n",
    "\n",
    "print(f\"\\nðŸ“Š N-BEATS RESULTS:\")\n",
    "print(f\"   RÂ² = {r2_nbeats:.4f}\")\n",
    "print(f\"   RMSE = {rmse_nbeats:.2f}\")\n",
    "print(f\"   MAE = {mae_nbeats:.2f}\")\n",
    "print(f\"   Time = {train_time_nbeats:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a9e8cf",
   "metadata": {},
   "source": [
    "### N-HiTS (Neural Hierarchical Interpolation for Time Series)\n",
    "Verbesserte Version von N-BEATS - noch besser fÃ¼r lange Horizonte!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba33ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build N-HiTS model\n",
    "model_nhits = NHiTSModel(\n",
    "    input_chunk_length=168,  # 1 week lookback\n",
    "    output_chunk_length=24,  # 1 day forecast\n",
    "    num_stacks=3,\n",
    "    num_blocks=1,\n",
    "    num_layers=2,\n",
    "    layer_widths=512,\n",
    "    n_epochs=50,\n",
    "    batch_size=64,\n",
    "    optimizer_kwargs={'lr': 1e-3},\n",
    "    pl_trainer_kwargs={\n",
    "        'accelerator': 'gpu',\n",
    "        'devices': 1,\n",
    "        'enable_progress_bar': True,\n",
    "    },\n",
    "    model_name='nhits_solar',\n",
    "    force_reset=True,\n",
    "    save_checkpoints=True\n",
    ")\n",
    "\n",
    "print(\"ðŸ”§ N-HiTS model configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465063c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train N-HiTS\n",
    "print(\"ðŸš€ Training N-HiTS...\")\n",
    "start = time.time()\n",
    "\n",
    "model_nhits.fit(\n",
    "    series=ts_train_scaled,\n",
    "    val_series=ts_val_scaled,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "train_time_nhits = time.time() - start\n",
    "print(f\"\\nâœ… N-HiTS training completed in {train_time_nhits:.1f}s ({train_time_nhits/60:.1f} min)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa86a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with N-HiTS\n",
    "pred_nhits_scaled = model_nhits.predict(n=n_pred, series=ts_train_scaled)\n",
    "pred_nhits = scaler_darts.inverse_transform(pred_nhits_scaled)\n",
    "\n",
    "# Extract values\n",
    "y_pred_nhits = pred_nhits.values().flatten()\n",
    "y_test_nhits = ts_test.values().flatten()\n",
    "\n",
    "# Ensure same length\n",
    "min_len = min(len(y_pred_nhits), len(y_test_nhits))\n",
    "y_pred_nhits = y_pred_nhits[:min_len]\n",
    "y_test_nhits = y_test_nhits[:min_len]\n",
    "\n",
    "# Evaluate\n",
    "rmse_nhits = np.sqrt(mean_squared_error(y_test_nhits, y_pred_nhits))\n",
    "mae_nhits = mean_absolute_error(y_test_nhits, y_pred_nhits)\n",
    "r2_nhits = r2_score(y_test_nhits, y_pred_nhits)\n",
    "\n",
    "print(f\"\\nðŸ“Š N-HiTS RESULTS:\")\n",
    "print(f\"   RÂ² = {r2_nhits:.4f}\")\n",
    "print(f\"   RMSE = {rmse_nhits:.2f}\")\n",
    "print(f\"   MAE = {mae_nhits:.2f}\")\n",
    "print(f\"   Time = {train_time_nhits:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d827a261",
   "metadata": {},
   "source": [
    "## ðŸ“Š Final Comparison - All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4095777b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive results\n",
    "all_results = pd.DataFrame([\n",
    "    {'Model': 'LSTM', 'Category': 'Basic DL', 'RÂ²': r2, 'RMSE': rmse, 'MAE': mae, 'Time': train_time},\n",
    "    {'Model': 'Bi-LSTM', 'Category': 'Basic DL', 'RÂ²': r2_bilstm, 'RMSE': rmse_bilstm, 'MAE': mae_bilstm, 'Time': train_time_bilstm},\n",
    "    {'Model': 'Autoencoder', 'Category': 'Generative', 'RÂ²': r2_ae, 'RMSE': rmse_ae, 'MAE': mae_ae, 'Time': train_time_ae_forecast},\n",
    "    {'Model': 'VAE', 'Category': 'Generative', 'RÂ²': r2_vae, 'RMSE': rmse_vae, 'MAE': mae_vae, 'Time': train_time_vae_forecast},\n",
    "    {'Model': 'N-BEATS', 'Category': 'Advanced', 'RÂ²': r2_nbeats, 'RMSE': rmse_nbeats, 'MAE': mae_nbeats, 'Time': train_time_nbeats},\n",
    "    {'Model': 'N-HiTS', 'Category': 'Advanced', 'RÂ²': r2_nhits, 'RMSE': rmse_nhits, 'MAE': mae_nhits, 'Time': train_time_nhits}\n",
    "])\n",
    "\n",
    "# Sort by RÂ²\n",
    "all_results = all_results.sort_values('RÂ²', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ† COMPREHENSIVE MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n\" + all_results.to_string(index=False))\n",
    "\n",
    "# Identify best\n",
    "best = all_results.iloc[0]\n",
    "print(f\"\\nðŸ¥‡ BEST MODEL: {best['Model']} ({best['Category']})\")\n",
    "print(f\"   RÂ² = {best['RÂ²']:.4f}\")\n",
    "print(f\"   RMSE = {best['RMSE']:.2f}\")\n",
    "print(f\"   MAE = {best['MAE']:.2f}\")\n",
    "print(f\"   Training Time = {best['Time']:.1f}s\")\n",
    "\n",
    "# Save results\n",
    "all_results.to_csv(f'results/metrics/deep_learning_comprehensive_{series_name}.csv', index=False)\n",
    "print(f\"\\nâœ… Results saved to: results/metrics/deep_learning_comprehensive_{series_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccd9741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# RÂ² by Model\n",
    "axes[0, 0].barh(all_results['Model'], all_results['RÂ²'], \n",
    "                color=['#e74c3c', '#3498db', '#2ecc71', '#f39c12', '#9b59b6', '#1abc9c'])\n",
    "axes[0, 0].set_xlabel('RÂ² Score', fontweight='bold')\n",
    "axes[0, 0].set_title('RÂ² Score by Model', fontweight='bold', fontsize=12)\n",
    "axes[0, 0].set_xlim([0.85, 1.0])\n",
    "axes[0, 0].grid(alpha=0.3, axis='x')\n",
    "\n",
    "# RMSE by Model\n",
    "axes[0, 1].barh(all_results['Model'], all_results['RMSE'],\n",
    "                color=['#e74c3c', '#3498db', '#2ecc71', '#f39c12', '#9b59b6', '#1abc9c'])\n",
    "axes[0, 1].set_xlabel('RMSE', fontweight='bold')\n",
    "axes[0, 1].set_title('RMSE by Model', fontweight='bold', fontsize=12)\n",
    "axes[0, 1].grid(alpha=0.3, axis='x')\n",
    "\n",
    "# Training Time by Model\n",
    "axes[1, 0].barh(all_results['Model'], all_results['Time'],\n",
    "                color=['#e74c3c', '#3498db', '#2ecc71', '#f39c12', '#9b59b6', '#1abc9c'])\n",
    "axes[1, 0].set_xlabel('Training Time (seconds)', fontweight='bold')\n",
    "axes[1, 0].set_title('Training Time by Model', fontweight='bold', fontsize=12)\n",
    "axes[1, 0].grid(alpha=0.3, axis='x')\n",
    "\n",
    "# RÂ² by Category\n",
    "category_r2 = all_results.groupby('Category')['RÂ²'].mean().sort_values(ascending=False)\n",
    "axes[1, 1].bar(category_r2.index, category_r2.values,\n",
    "               color=['#9b59b6', '#2ecc71', '#3498db'])\n",
    "axes[1, 1].set_ylabel('Average RÂ²', fontweight='bold')\n",
    "axes[1, 1].set_title('Average RÂ² by Category', fontweight='bold', fontsize=12)\n",
    "axes[1, 1].set_ylim([0.85, 1.0])\n",
    "axes[1, 1].grid(alpha=0.3, axis='y')\n",
    "axes[1, 1].tick_params(axis='x', rotation=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'results/figures/deep_learning_comprehensive_{series_name}.png', \n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ… Comprehensive plot saved to: results/figures/deep_learning_comprehensive_{series_name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64322007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction comparison plot\n",
    "fig, axes = plt.subplots(3, 2, figsize=(20, 15))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Show first 500 test points for clarity\n",
    "n_show = 500\n",
    "x_plot = range(n_show)\n",
    "\n",
    "models_data = [\n",
    "    ('LSTM', y_pred[:n_show]),\n",
    "    ('Bi-LSTM', y_pred_bilstm[:n_show]),\n",
    "    ('Autoencoder', y_pred_ae[:n_show]),\n",
    "    ('VAE', y_pred_vae[:n_show]),\n",
    "    ('N-BEATS', y_pred_nbeats[:n_show]),\n",
    "    ('N-HiTS', y_pred_nhits[:n_show])\n",
    "]\n",
    "\n",
    "for idx, (model_name, predictions) in enumerate(models_data):\n",
    "    axes[idx].plot(x_plot, y_test_seq[:n_show], label='Actual', color='black', linewidth=1, alpha=0.7)\n",
    "    axes[idx].plot(x_plot, predictions, label=f'{model_name} Prediction', \n",
    "                   color=['#e74c3c', '#3498db', '#2ecc71', '#f39c12', '#9b59b6', '#1abc9c'][idx],\n",
    "                   linewidth=1, alpha=0.8)\n",
    "    \n",
    "    model_r2 = all_results[all_results['Model'] == model_name]['RÂ²'].values[0]\n",
    "    axes[idx].set_title(f'{model_name} (RÂ² = {model_r2:.4f})', fontweight='bold', fontsize=11)\n",
    "    axes[idx].set_xlabel('Time (hours)', fontsize=9)\n",
    "    axes[idx].set_ylabel(f'{series_name.title()} (MW)', fontsize=9)\n",
    "    axes[idx].legend(loc='upper right', fontsize=8)\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Prediction Comparison - All Models ({series_name.title()})', \n",
    "             fontweight='bold', fontsize=14, y=1.0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'results/figures/predictions_comparison_{series_name}.png', \n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ… Predictions plot saved to: results/figures/predictions_comparison_{series_name}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87156199",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Key Insights\n",
    "\n",
    "### GPU Performance Gains:\n",
    "- **LSTM/Bi-LSTM**: ~30-50x faster on GPU vs CPU\n",
    "- **N-BEATS/N-HiTS**: Only feasible on GPU (100+ epochs in minutes)\n",
    "- **Generative Models**: 10-20x speedup\n",
    "\n",
    "### Model Performance Rankings:\n",
    "1. **Advanced Models** (N-BEATS, N-HiTS) - Often best for complex patterns\n",
    "2. **Basic DL** (LSTM, Bi-LSTM) - Good baseline, interpretable\n",
    "3. **Generative** (AE, VAE) - Useful for anomaly detection, uncertainty quantification\n",
    "\n",
    "### When to Use What:\n",
    "- **N-BEATS/N-HiTS**: Best for pure forecasting accuracy\n",
    "- **Bi-LSTM**: Good balance of performance and interpretability\n",
    "- **VAE**: When you need uncertainty estimates\n",
    "- **Autoencoder**: For anomaly detection alongside forecasting\n",
    "\n",
    "### Production Recommendations:\n",
    "- **High Accuracy Needed**: N-BEATS or N-HiTS\n",
    "- **Real-time Inference**: Bi-LSTM (faster inference)\n",
    "- **Interpretability**: Standard LSTM with attention\n",
    "- **Anomaly Detection**: VAE or Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb139a5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ”„ Batch-Verarbeitung: Alle Zeitreihen\n",
    "\n",
    "Falls `RUN_ALL_SERIES = True` gesetzt wurde, wird folgender Code alle Zeitreihen nacheinander verarbeiten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1ba988",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_ALL_SERIES and len(series_to_process) > 1:\n",
    "    print(\"=\"*80)\n",
    "    print(\"ðŸ”„ BATCH MODE: Processing all time series\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Storage for all results\n",
    "    batch_results = []\n",
    "    \n",
    "    for idx, current_series in enumerate(series_to_process, 1):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ðŸ“Š Processing {idx}/{len(series_to_process)}: {current_series.upper()}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        try:\n",
    "            # Load data\n",
    "            train_df = pd.read_csv(f'data/processed/{current_series}_train.csv')\n",
    "            val_df = pd.read_csv(f'data/processed/{current_series}_val.csv')\n",
    "            test_df = pd.read_csv(f'data/processed/{current_series}_test.csv')\n",
    "            \n",
    "            # Determine value column\n",
    "            value_col = [c for c in train_df.columns if c in ['solar', 'price', 'value', \n",
    "                                                                 'wind_offshore', 'wind_onshore', 'consumption']][0]\n",
    "            feature_cols = [c for c in train_df.columns if c not in ['timestamp', value_col]]\n",
    "            \n",
    "            print(f\"âœ… Data loaded: {len(train_df)} train, {len(val_df)} val, {len(test_df)} test\")\n",
    "            print(f\"   Value column: {value_col}, Features: {len(feature_cols)}\")\n",
    "            \n",
    "            # Scale data\n",
    "            scaler_X = StandardScaler()\n",
    "            scaler_y = StandardScaler()\n",
    "            \n",
    "            X_train = scaler_X.fit_transform(train_df[feature_cols])\n",
    "            y_train = scaler_y.fit_transform(train_df[[value_col]])\n",
    "            \n",
    "            X_val = scaler_X.transform(val_df[feature_cols])\n",
    "            y_val = scaler_y.transform(val_df[[value_col]])\n",
    "            \n",
    "            X_test = scaler_X.transform(test_df[feature_cols])\n",
    "            y_test_orig = test_df[value_col].values\n",
    "            \n",
    "            # Create sequences\n",
    "            seq_length = 24\n",
    "            X_train_seq, y_train_seq = create_sequences(X_train, y_train.flatten(), seq_length)\n",
    "            X_val_seq, y_val_seq = create_sequences(X_val, y_val.flatten(), seq_length)\n",
    "            X_test_seq, _ = create_sequences(X_test, np.zeros(len(X_test)), seq_length)\n",
    "            y_test_seq = y_test_orig[seq_length:]\n",
    "            \n",
    "            print(f\"âœ… Sequences created: {X_train_seq.shape}\")\n",
    "            \n",
    "            # Train only key models (faster for batch)\n",
    "            series_results = []\n",
    "            \n",
    "            # 1. Bi-LSTM\n",
    "            print(\"\\nðŸ”§ Training Bi-LSTM...\")\n",
    "            model_bilstm = keras.Sequential([\n",
    "                layers.Bidirectional(layers.LSTM(64, return_sequences=True), input_shape=(seq_length, X_train.shape[1])),\n",
    "                layers.Dropout(0.2),\n",
    "                layers.Bidirectional(layers.LSTM(32)),\n",
    "                layers.Dropout(0.2),\n",
    "                layers.Dense(32, activation='relu'),\n",
    "                layers.Dense(1)\n",
    "            ])\n",
    "            model_bilstm.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "            \n",
    "            start = time.time()\n",
    "            model_bilstm.fit(X_train_seq, y_train_seq, validation_data=(X_val_seq, y_val_seq),\n",
    "                           epochs=30, batch_size=64, callbacks=[early_stop], verbose=0)\n",
    "            train_time = time.time() - start\n",
    "            \n",
    "            y_pred_scaled = model_bilstm.predict(X_test_seq, verbose=0).flatten()\n",
    "            y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "            \n",
    "            r2 = r2_score(y_test_seq, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test_seq, y_pred))\n",
    "            mae = mean_absolute_error(y_test_seq, y_pred)\n",
    "            \n",
    "            series_results.append({\n",
    "                'Series': current_series,\n",
    "                'Model': 'Bi-LSTM',\n",
    "                'RÂ²': r2,\n",
    "                'RMSE': rmse,\n",
    "                'MAE': mae,\n",
    "                'Time': train_time\n",
    "            })\n",
    "            print(f\"   âœ… Bi-LSTM: RÂ²={r2:.4f}, RMSE={rmse:.2f}, Time={train_time:.1f}s\")\n",
    "            \n",
    "            # 2. N-BEATS (if Darts available)\n",
    "            try:\n",
    "                print(\"\\nðŸ”§ Training N-BEATS...\")\n",
    "                train_df_ts = train_df.copy()\n",
    "                train_df_ts['timestamp'] = pd.to_datetime(train_df_ts['timestamp'])\n",
    "                train_df_ts = train_df_ts.set_index('timestamp')\n",
    "                \n",
    "                val_df_ts = val_df.copy()\n",
    "                val_df_ts['timestamp'] = pd.to_datetime(val_df_ts['timestamp'])\n",
    "                val_df_ts = val_df_ts.set_index('timestamp')\n",
    "                \n",
    "                test_df_ts = test_df.copy()\n",
    "                test_df_ts['timestamp'] = pd.to_datetime(test_df_ts['timestamp'])\n",
    "                test_df_ts = test_df_ts.set_index('timestamp')\n",
    "                \n",
    "                ts_train = TimeSeries.from_dataframe(train_df_ts, value_cols=value_col, freq='H')\n",
    "                ts_val = TimeSeries.from_dataframe(val_df_ts, value_cols=value_col, freq='H')\n",
    "                ts_test = TimeSeries.from_dataframe(test_df_ts, value_cols=value_col, freq='H')\n",
    "                \n",
    "                scaler_darts = Scaler()\n",
    "                ts_train_scaled = scaler_darts.fit_transform(ts_train)\n",
    "                ts_val_scaled = scaler_darts.transform(ts_val)\n",
    "                \n",
    "                model_nbeats = NBEATSModel(\n",
    "                    input_chunk_length=168, output_chunk_length=24,\n",
    "                    num_stacks=20, num_blocks=1, num_layers=3, layer_widths=128,\n",
    "                    n_epochs=30, batch_size=64,\n",
    "                    pl_trainer_kwargs={'accelerator': 'gpu', 'devices': 1, 'enable_progress_bar': False},\n",
    "                    model_name=f'nbeats_{current_series}',\n",
    "                    force_reset=True, save_checkpoints=False\n",
    "                )\n",
    "                \n",
    "                start = time.time()\n",
    "                model_nbeats.fit(series=ts_train_scaled, val_series=ts_val_scaled, verbose=False)\n",
    "                train_time_nbeats = time.time() - start\n",
    "                \n",
    "                pred_nbeats_scaled = model_nbeats.predict(n=len(ts_test), series=ts_train_scaled)\n",
    "                pred_nbeats = scaler_darts.inverse_transform(pred_nbeats_scaled)\n",
    "                \n",
    "                y_pred_nbeats = pred_nbeats.values().flatten()\n",
    "                y_test_nbeats = ts_test.values().flatten()\n",
    "                min_len = min(len(y_pred_nbeats), len(y_test_nbeats))\n",
    "                \n",
    "                r2_nbeats = r2_score(y_test_nbeats[:min_len], y_pred_nbeats[:min_len])\n",
    "                rmse_nbeats = np.sqrt(mean_squared_error(y_test_nbeats[:min_len], y_pred_nbeats[:min_len]))\n",
    "                mae_nbeats = mean_absolute_error(y_test_nbeats[:min_len], y_pred_nbeats[:min_len])\n",
    "                \n",
    "                series_results.append({\n",
    "                    'Series': current_series,\n",
    "                    'Model': 'N-BEATS',\n",
    "                    'RÂ²': r2_nbeats,\n",
    "                    'RMSE': rmse_nbeats,\n",
    "                    'MAE': mae_nbeats,\n",
    "                    'Time': train_time_nbeats\n",
    "                })\n",
    "                print(f\"   âœ… N-BEATS: RÂ²={r2_nbeats:.4f}, RMSE={rmse_nbeats:.2f}, Time={train_time_nbeats:.1f}s\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   âš ï¸ N-BEATS skipped: {e}\")\n",
    "            \n",
    "            # Add to batch results\n",
    "            batch_results.extend(series_results)\n",
    "            \n",
    "            print(f\"\\nâœ… {current_series.upper()} completed!\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error processing {current_series}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ† BATCH PROCESSING COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    batch_df = pd.DataFrame(batch_results)\n",
    "    print(\"\\n\" + batch_df.to_string(index=False))\n",
    "    \n",
    "    # Save batch results\n",
    "    batch_df.to_csv('results/metrics/batch_all_series_gpu.csv', index=False)\n",
    "    print(f\"\\nâœ… Batch results saved to: results/metrics/batch_all_series_gpu.csv\")\n",
    "    \n",
    "    # Visualization: Comparison across series\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # RÂ² by Series and Model\n",
    "    pivot_r2 = batch_df.pivot(index='Series', columns='Model', values='RÂ²')\n",
    "    pivot_r2.plot(kind='bar', ax=axes[0], color=['#3498db', '#e74c3c'])\n",
    "    axes[0].set_ylabel('RÂ² Score', fontweight='bold')\n",
    "    axes[0].set_title('RÂ² Score by Series and Model', fontweight='bold', fontsize=12)\n",
    "    axes[0].legend(title='Model')\n",
    "    axes[0].grid(alpha=0.3, axis='y')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Training Time by Series\n",
    "    pivot_time = batch_df.pivot(index='Series', columns='Model', values='Time')\n",
    "    pivot_time.plot(kind='bar', ax=axes[1], color=['#3498db', '#e74c3c'])\n",
    "    axes[1].set_ylabel('Training Time (s)', fontweight='bold')\n",
    "    axes[1].set_title('Training Time by Series and Model', fontweight='bold', fontsize=12)\n",
    "    axes[1].legend(title='Model')\n",
    "    axes[1].grid(alpha=0.3, axis='y')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/figures/batch_all_series_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"âœ… Batch comparison plot saved!\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nðŸ’¡ Einzelne Zeitreihe wurde verarbeitet.\")\n",
    "    print(\"   Setze RUN_ALL_SERIES = True fÃ¼r Batch-Verarbeitung.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b652c0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“ Anleitung: Zeitreihe wechseln\n",
    "\n",
    "### âœ… Methode 1: Einzelne Zeitreihe\n",
    "```python\n",
    "# In der Konfigurations-Zelle (Zelle 7):\n",
    "SERIES_NAME = 'wind_offshore'  # Ã„ndere hier die Zeitreihe\n",
    "RUN_ALL_SERIES = False\n",
    "\n",
    "# Dann: Restart Runtime + Run All\n",
    "```\n",
    "\n",
    "### ðŸ”„ Methode 2: Alle Zeitreihen\n",
    "```python\n",
    "# In der Konfigurations-Zelle (Zelle 7):\n",
    "RUN_ALL_SERIES = True  # Batch-Modus aktivieren\n",
    "\n",
    "# Dann: Restart Runtime + Run All\n",
    "# â†’ Verarbeitet automatisch alle 5 Zeitreihen nacheinander\n",
    "# â†’ Erstellt Vergleichstabelle und Plots\n",
    "# â†’ Dauert ca. 20-40 Min fÃ¼r alle Serien\n",
    "```\n",
    "\n",
    "### ðŸ“Š Output-Dateien:\n",
    "- Einzeln: `results/metrics/deep_learning_comprehensive_{series_name}.csv`\n",
    "- Batch: `results/metrics/batch_all_series_gpu.csv`\n",
    "- Plots: `results/figures/`\n",
    "\n",
    "### ðŸ’¡ Empfehlung:\n",
    "- **Erste Experimente**: Einzelne Zeitreihe (schneller)\n",
    "- **Finale Evaluation**: Batch-Modus (systematischer Vergleich)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
