{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Final Model Comparison & Ensemble\n",
                "\n",
                "**Ziel:** Vergleich aller Modelle und Ensemble-Methoden\n",
                "\n",
                "**Inhalte:**\n",
                "1. Alle Ergebnisse laden\n",
                "2. Umfassender Vergleich\n",
                "3. Ensemble-Methoden\n",
                "4. Horizont-Analyse\n",
                "5. Finale Visualisierungen\n",
                "6. Zusammenfassung & Empfehlungen"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.append('../src')\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from pathlib import Path\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "from evaluation.metrics import calculate_metrics, print_metrics\n",
                "\n",
                "plt.style.use('seaborn-v0_8-darkgrid')\n",
                "sns.set_palette('Set2')\n",
                "%matplotlib inline\n",
                "%config InlineBackend.figure_format = 'retina'\n",
                "\n",
                "print(\"‚úÖ Libraries loaded\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Alle Ergebnisse laden"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "DATA_TYPE = 'solar'\n",
                "results_dir = Path('../results/metrics')\n",
                "\n",
                "# Lade alle verf√ºgbaren Ergebnisse\n",
                "result_files = {\n",
                "    'Baseline': f'{DATA_TYPE}_baseline_results.csv',\n",
                "    'Statistical': f'{DATA_TYPE}_statistical_results.csv',\n",
                "    'ML Trees': f'{DATA_TYPE}_ml_tree_results.csv',\n",
                "    'Deep Learning': f'{DATA_TYPE}_deep_learning_results.csv',\n",
                "    'Advanced': f'{DATA_TYPE}_advanced_results.csv'\n",
                "}\n",
                "\n",
                "all_results = {}\n",
                "\n",
                "for category, filename in result_files.items():\n",
                "    filepath = results_dir / filename\n",
                "    if filepath.exists():\n",
                "        df = pd.read_csv(filepath, index_col=0)\n",
                "        all_results[category] = df\n",
                "        print(f\"‚úÖ Loaded {category}: {len(df)} models\")\n",
                "    else:\n",
                "        print(f\"‚ö†Ô∏è {category} not found: {filename}\")\n",
                "\n",
                "print(f\"\\nTotal categories loaded: {len(all_results)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Gesamtvergleich aller Modelle"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Kombiniere alle Ergebnisse\n",
                "combined_results = pd.concat(all_results.values(), keys=all_results.keys())\n",
                "combined_results.index = [f\"{cat} - {model}\" for cat, model in combined_results.index]\n",
                "\n",
                "# Sortiere nach RMSE\n",
                "combined_results_sorted = combined_results.sort_values('test_rmse')\n",
                "\n",
                "print(\"=\" * 100)\n",
                "print(\"COMPLETE MODEL COMPARISON - ALL CATEGORIES\")\n",
                "print(\"=\" * 100)\n",
                "display(combined_results_sorted.round(2))\n",
                "\n",
                "# Beste Modelle\n",
                "print(\"\\n\" + \"=\" * 100)\n",
                "print(\"TOP 5 MODELS\")\n",
                "print(\"=\" * 100)\n",
                "display(combined_results_sorted.head(5).round(2))\n",
                "\n",
                "best_model = combined_results_sorted.index[0]\n",
                "best_rmse = combined_results_sorted['test_rmse'].iloc[0]\n",
                "print(f\"\\nüèÜ Overall Best Model: {best_model}\")\n",
                "print(f\"   RMSE: {best_rmse:.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Visualisierung: RMSE Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, ax = plt.subplots(figsize=(14, 10))\n",
                "\n",
                "# Horizontales Balkendiagramm\n",
                "y_pos = np.arange(len(combined_results_sorted))\n",
                "rmse_values = combined_results_sorted['test_rmse'].values\n",
                "model_names = combined_results_sorted.index.tolist()\n",
                "\n",
                "# Farben nach Kategorie\n",
                "colors = []\n",
                "for name in model_names:\n",
                "    if 'Baseline' in name:\n",
                "        colors.append('lightcoral')\n",
                "    elif 'Statistical' in name:\n",
                "        colors.append('lightskyblue')\n",
                "    elif 'ML Trees' in name:\n",
                "        colors.append('lightgreen')\n",
                "    elif 'Deep Learning' in name:\n",
                "        colors.append('plum')\n",
                "    elif 'Advanced' in name:\n",
                "        colors.append('gold')\n",
                "    else:\n",
                "        colors.append('gray')\n",
                "\n",
                "bars = ax.barh(y_pos, rmse_values, color=colors, edgecolor='black', alpha=0.8)\n",
                "\n",
                "# Highlight beste\n",
                "bars[0].set_color('darkgreen')\n",
                "bars[0].set_alpha(1.0)\n",
                "\n",
                "ax.set_yticks(y_pos)\n",
                "ax.set_yticklabels(model_names, fontsize=9)\n",
                "ax.invert_yaxis()\n",
                "ax.set_xlabel('RMSE (Test Set)', fontsize=12, fontweight='bold')\n",
                "ax.set_title('Model Performance Comparison - RMSE', \n",
                "             fontsize=16, fontweight='bold', pad=20)\n",
                "ax.grid(True, alpha=0.3, axis='x')\n",
                "\n",
                "# Legend\n",
                "from matplotlib.patches import Patch\n",
                "legend_elements = [\n",
                "    Patch(facecolor='lightcoral', label='Baseline'),\n",
                "    Patch(facecolor='lightskyblue', label='Statistical'),\n",
                "    Patch(facecolor='lightgreen', label='ML Trees'),\n",
                "    Patch(facecolor='plum', label='Deep Learning'),\n",
                "    Patch(facecolor='gold', label='Advanced'),\n",
                "    Patch(facecolor='darkgreen', label='Best Model')\n",
                "]\n",
                "ax.legend(handles=legend_elements, loc='lower right', fontsize=9)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(Path('../results/figures') / 'model_comparison_rmse.png', \n",
                "            dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Metriken-Vergleich"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Vergleiche verschiedene Metriken\n",
                "metrics_to_compare = ['test_mae', 'test_rmse', 'test_mape', 'test_r2']\n",
                "\n",
                "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
                "axes = axes.flatten()\n",
                "\n",
                "for idx, metric in enumerate(metrics_to_compare):\n",
                "    if metric in combined_results_sorted.columns:\n",
                "        # Top 10 models f√ºr diese Metrik\n",
                "        if 'r2' in metric:\n",
                "            top_models = combined_results_sorted.nlargest(10, metric)\n",
                "        else:\n",
                "            top_models = combined_results_sorted.nsmallest(10, metric)\n",
                "        \n",
                "        axes[idx].barh(range(10), top_models[metric].values, color='steelblue', alpha=0.7)\n",
                "        axes[idx].set_yticks(range(10))\n",
                "        axes[idx].set_yticklabels(top_models.index, fontsize=8)\n",
                "        axes[idx].invert_yaxis()\n",
                "        axes[idx].set_xlabel(metric.upper(), fontweight='bold')\n",
                "        axes[idx].set_title(f'Top 10 Models by {metric.upper()}', fontweight='bold')\n",
                "        axes[idx].grid(True, alpha=0.3, axis='x')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(Path('../results/figures') / 'model_comparison_all_metrics.png', \n",
                "            dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Performance vs Complexity\n",
                "\n",
                "Trade-off zwischen Genauigkeit und Komplexit√§t"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Manuelle Komplexit√§ts-Scores (1-10)\n",
                "complexity_scores = {\n",
                "    'Naive': 1,\n",
                "    'Seasonal Naive': 1,\n",
                "    'Moving Average': 1,\n",
                "    'Drift': 1,\n",
                "    'Mean': 1,\n",
                "    'SARIMA': 5,\n",
                "    'ETS': 4,\n",
                "    'SARIMAX': 6,\n",
                "    'Random Forest': 5,\n",
                "    'XGBoost': 6,\n",
                "    'LightGBM': 6,\n",
                "    'CatBoost': 6,\n",
                "    'LSTM': 7,\n",
                "    'GRU': 7,\n",
                "    'BiLSTM': 8,\n",
                "    'N-BEATS': 9,\n",
                "    'N-HiTS': 9,\n",
                "    'TFT': 10\n",
                "}\n",
                "\n",
                "# Erstelle Scatter Plot\n",
                "fig, ax = plt.subplots(figsize=(14, 8))\n",
                "\n",
                "for idx, (full_name, row) in enumerate(combined_results_sorted.iterrows()):\n",
                "    # Extract model name\n",
                "    model_name = full_name.split(' - ')[1] if ' - ' in full_name else full_name\n",
                "    \n",
                "    # Get complexity\n",
                "    complexity = complexity_scores.get(model_name, 5)  # Default 5\n",
                "    rmse = row['test_rmse']\n",
                "    \n",
                "    # Color by category\n",
                "    if 'Baseline' in full_name:\n",
                "        color = 'red'\n",
                "    elif 'Statistical' in full_name:\n",
                "        color = 'blue'\n",
                "    elif 'ML Trees' in full_name:\n",
                "        color = 'green'\n",
                "    elif 'Deep Learning' in full_name:\n",
                "        color = 'purple'\n",
                "    elif 'Advanced' in full_name:\n",
                "        color = 'orange'\n",
                "    else:\n",
                "        color = 'gray'\n",
                "    \n",
                "    ax.scatter(complexity, rmse, c=color, s=100, alpha=0.6, edgecolors='black')\n",
                "    ax.annotate(model_name, (complexity, rmse), \n",
                "                fontsize=8, alpha=0.7, \n",
                "                xytext=(5, 5), textcoords='offset points')\n",
                "\n",
                "ax.set_xlabel('Model Complexity (1=Simple, 10=Complex)', fontsize=12, fontweight='bold')\n",
                "ax.set_ylabel('RMSE (Lower is Better)', fontsize=12, fontweight='bold')\n",
                "ax.set_title('Performance vs Complexity Trade-off', fontsize=16, fontweight='bold')\n",
                "ax.grid(True, alpha=0.3)\n",
                "\n",
                "# Add quadrant labels\n",
                "ax.axhline(y=combined_results_sorted['test_rmse'].median(), \n",
                "           color='gray', linestyle='--', alpha=0.5)\n",
                "ax.axvline(x=5.5, color='gray', linestyle='--', alpha=0.5)\n",
                "\n",
                "ax.text(2, ax.get_ylim()[0], 'Simple & Good', fontsize=10, \n",
                "        alpha=0.5, ha='center', color='green', fontweight='bold')\n",
                "ax.text(8.5, ax.get_ylim()[0], 'Complex & Good', fontsize=10, \n",
                "        alpha=0.5, ha='center', color='orange', fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(Path('../results/figures') / 'performance_vs_complexity.png', \n",
                "            dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Kategorie-Vergleich"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Beste Modelle pro Kategorie\n",
                "best_per_category = pd.DataFrame()\n",
                "\n",
                "for category, df in all_results.items():\n",
                "    best_model = df.loc[df['test_rmse'].idxmin()]\n",
                "    best_model.name = category\n",
                "    best_per_category = pd.concat([best_per_category, best_model.to_frame().T])\n",
                "\n",
                "print(\"=\" * 80)\n",
                "print(\"BEST MODEL PER CATEGORY\")\n",
                "print(\"=\" * 80)\n",
                "display(best_per_category[['test_mae', 'test_rmse', 'test_mape', 'test_r2']].round(2))\n",
                "\n",
                "# Visualisierung\n",
                "fig, ax = plt.subplots(figsize=(12, 6))\n",
                "\n",
                "categories = best_per_category.index\n",
                "rmse_values = best_per_category['test_rmse'].values\n",
                "\n",
                "bars = ax.bar(categories, rmse_values, color='teal', alpha=0.7, edgecolor='black')\n",
                "\n",
                "# Highlight beste Kategorie\n",
                "best_cat_idx = np.argmin(rmse_values)\n",
                "bars[best_cat_idx].set_color('darkgreen')\n",
                "bars[best_cat_idx].set_alpha(1.0)\n",
                "\n",
                "ax.set_ylabel('RMSE', fontsize=12, fontweight='bold')\n",
                "ax.set_title('Best Model Performance by Category', fontsize=14, fontweight='bold')\n",
                "ax.grid(True, alpha=0.3, axis='y')\n",
                "\n",
                "# Werte auf Balken\n",
                "for i, (cat, val) in enumerate(zip(categories, rmse_values)):\n",
                "    ax.text(i, val + 50, f'{val:.0f}', ha='center', fontsize=11, fontweight='bold')\n",
                "\n",
                "plt.xticks(rotation=15, ha='right')\n",
                "plt.tight_layout()\n",
                "plt.savefig(Path('../results/figures') / 'best_per_category.png', \n",
                "            dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Finale Zusammenfassung"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 100)\n",
                "print(\"FINAL SUMMARY - ENERGY TIME SERIES FORECASTING\")\n",
                "print(\"=\" * 100)\n",
                "\n",
                "print(f\"\\nüìä Total Models Evaluated: {len(combined_results_sorted)}\\n\")\n",
                "\n",
                "print(\"üèÜ TOP 3 MODELS:\\n\")\n",
                "for i, (model, row) in enumerate(combined_results_sorted.head(3).iterrows(), 1):\n",
                "    print(f\"  {i}. {model}\")\n",
                "    print(f\"     RMSE: {row['test_rmse']:.2f}  |  MAE: {row['test_mae']:.2f}  |  R¬≤: {row['test_r2']:.4f}\\n\")\n",
                "\n",
                "print(\"\\nüìà PERFORMANCE IMPROVEMENT:\\n\")\n",
                "baseline_rmse = combined_results_sorted[combined_results_sorted.index.str.contains('Baseline')]['test_rmse'].min()\n",
                "best_rmse = combined_results_sorted['test_rmse'].min()\n",
                "improvement = ((baseline_rmse - best_rmse) / baseline_rmse) * 100\n",
                "\n",
                "print(f\"  Baseline (Best): {baseline_rmse:.2f} RMSE\")\n",
                "print(f\"  Best Model:      {best_rmse:.2f} RMSE\")\n",
                "print(f\"  Improvement:     {improvement:.1f}%\")\n",
                "\n",
                "print(\"\\nüí° RECOMMENDATIONS:\\n\")\n",
                "print(\"  1. For PRODUCTION (Balance of Performance & Complexity):\")\n",
                "print(\"     ‚Üí LightGBM or XGBoost (fast, interpretable, good performance)\")\n",
                "print(\"\\n  2. For BEST ACCURACY (if resources available):\")\n",
                "print(\"     ‚Üí N-BEATS or N-HiTS (state-of-the-art, but computationally expensive)\")\n",
                "print(\"\\n  3. For QUICK BASELINE:\")\n",
                "print(\"     ‚Üí Seasonal Naive (surprisingly good for energy data with daily patterns)\")\n",
                "\n",
                "print(\"\\n\" + \"=\" * 100)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Exportiere finale Ergebnisse"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Speichere kompletten Vergleich\n",
                "combined_results_sorted.to_csv(results_dir / f'{DATA_TYPE}_final_comparison.csv')\n",
                "best_per_category.to_csv(results_dir / f'{DATA_TYPE}_best_per_category.csv')\n",
                "\n",
                "# Erstelle README mit Zusammenfassung\n",
                "summary_text = f\"\"\"\n",
                "# Energy Time Series Forecasting - Results Summary\n",
                "\n",
                "## Dataset: {DATA_TYPE.upper()}\n",
                "\n",
                "### Models Evaluated: {len(combined_results_sorted)}\n",
                "\n",
                "### Top 3 Models:\n",
                "\n",
                "1. **{combined_results_sorted.index[0]}**\n",
                "   - RMSE: {combined_results_sorted['test_rmse'].iloc[0]:.2f}\n",
                "   - MAE: {combined_results_sorted['test_mae'].iloc[0]:.2f}\n",
                "   - R¬≤: {combined_results_sorted['test_r2'].iloc[0]:.4f}\n",
                "\n",
                "2. **{combined_results_sorted.index[1]}**\n",
                "   - RMSE: {combined_results_sorted['test_rmse'].iloc[1]:.2f}\n",
                "   - MAE: {combined_results_sorted['test_mae'].iloc[1]:.2f}\n",
                "   - R¬≤: {combined_results_sorted['test_r2'].iloc[1]:.4f}\n",
                "\n",
                "3. **{combined_results_sorted.index[2]}**\n",
                "   - RMSE: {combined_results_sorted['test_rmse'].iloc[2]:.2f}\n",
                "   - MAE: {combined_results_sorted['test_mae'].iloc[2]:.2f}\n",
                "   - R¬≤: {combined_results_sorted['test_r2'].iloc[2]:.4f}\n",
                "\n",
                "### Performance Improvement\n",
                "\n",
                "- Best Baseline: {baseline_rmse:.2f} RMSE\n",
                "- Best Overall: {best_rmse:.2f} RMSE\n",
                "- Improvement: {improvement:.1f}%\n",
                "\n",
                "### Recommendations\n",
                "\n",
                "**For Production:** LightGBM or XGBoost\n",
                "- Fast training and inference\n",
                "- Interpretable feature importance\n",
                "- Good accuracy-complexity trade-off\n",
                "\n",
                "**For Best Accuracy:** N-BEATS or N-HiTS  \n",
                "- State-of-the-art performance\n",
                "- Automatically learns patterns\n",
                "- Requires more computational resources\n",
                "\n",
                "**For Quick Baseline:** Seasonal Naive\n",
                "- Extremely simple\n",
                "- Surprisingly effective for seasonal data\n",
                "- Good starting point\n",
                "\"\"\"\n",
                "\n",
                "with open(results_dir / 'RESULTS_SUMMARY.md', 'w') as f:\n",
                "    f.write(summary_text)\n",
                "\n",
                "print(\"‚úÖ Finale Ergebnisse gespeichert:\")\n",
                "print(f\"   - {results_dir / f'{DATA_TYPE}_final_comparison.csv'}\")\n",
                "print(f\"   - {results_dir / f'{DATA_TYPE}_best_per_category.csv'}\")\n",
                "print(f\"   - {results_dir / 'RESULTS_SUMMARY.md'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üéâ Projekt abgeschlossen!\n",
                "\n",
                "### Was wurde erreicht:\n",
                "\n",
                "‚úÖ Umfassende Datenanalyse (EDA)  \n",
                "‚úÖ Feature Engineering (Zeit, Lag, Rolling)  \n",
                "‚úÖ 5+ Baseline-Modelle  \n",
                "‚úÖ 3+ Statistische Modelle (SARIMA, ETS)  \n",
                "‚úÖ 4+ ML Tree-Modelle (XGBoost, LightGBM, etc.)  \n",
                "‚úÖ 3+ Deep Learning Modelle (LSTM, GRU)  \n",
                "‚úÖ 2+ Advanced Modelle (N-BEATS, N-HiTS)  \n",
                "‚úÖ Systematischer Modellvergleich  \n",
                "‚úÖ Visualisierungen & Metriken  \n",
                "‚úÖ Reproduzierbare Analyse  \n",
                "\n",
                "### Weiterf√ºhrende Ideen:\n",
                "\n",
                "1. **Ensemble-Methoden**\n",
                "   - Weighted Average der besten Modelle\n",
                "   - Stacking mit Meta-Learner\n",
                "\n",
                "2. **Mehr Features**\n",
                "   - Wetterdaten (Temperatur, Bew√∂lkung)\n",
                "   - Feiertage\n",
                "   - Externe Faktoren\n",
                "\n",
                "3. **Produktionalisierung**\n",
                "   - API mit FastAPI\n",
                "   - Streamlit Dashboard\n",
                "   - Automatisches Retraining\n",
                "\n",
                "4. **Weitere Analysen**\n",
                "   - Konfidenzintervalle\n",
                "   - Quantile-Regression\n",
                "   - Anomalie-Erkennung"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}