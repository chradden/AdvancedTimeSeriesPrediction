<!DOCTYPE html>
<html lang="de">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Advanced Time Series Forecasting - PrÃ¤sentation</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            overflow: hidden;
        }
        .slide {
            display: none;
            width: 100vw;
            height: 100vh;
            padding: 50px 80px;
            background: white;
            overflow-y: auto;
        }
        .slide.active { display: block; }
        
        /* Typography */
        .slide h1 { 
            color: #667eea; 
            font-size: 2.5em; 
            margin-bottom: 25px;
            border-bottom: 4px solid #764ba2;
            padding-bottom: 15px;
            font-weight: 700;
        }
        .slide h2 { 
            color: #764ba2; 
            font-size: 2em; 
            margin: 25px 0 15px;
            font-weight: 600;
        }
        .slide h3 { 
            color: #667eea; 
            font-size: 1.5em; 
            margin: 20px 0 10px;
            font-weight: 600;
        }
        .slide h4 {
            color: #555;
            font-size: 1.2em;
            margin: 15px 0 10px;
            font-weight: 600;
        }
        
        /* Tables */
        .slide table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            box-shadow: 0 4px 10px rgba(0,0,0,0.1);
            font-size: 0.9em;
        }
        .slide th, .slide td {
            padding: 12px 15px;
            text-align: left;
            border-bottom: 1px solid #e0e0e0;
        }
        .slide th {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            font-weight: 600;
            text-transform: uppercase;
            font-size: 0.85em;
            letter-spacing: 0.5px;
        }
        .slide tbody tr:hover { 
            background: #f8f9fa;
            transition: background 0.2s;
        }
        .slide tbody tr:nth-child(even) {
            background: #f9f9f9;
        }
        
        /* Code */
        .slide code {
            background: #f4f4f4;
            padding: 3px 8px;
            border-radius: 4px;
            font-family: 'SF Mono', Monaco, 'Courier New', monospace;
            font-size: 0.9em;
            color: #d63384;
        }
        .slide pre {
            background: #2d2d2d;
            color: #f8f8f2;
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            margin: 20px 0;
            font-size: 0.85em;
            line-height: 1.5;
            box-shadow: 0 4px 10px rgba(0,0,0,0.2);
        }
        .slide pre code {
            background: none;
            color: inherit;
            padding: 0;
        }
        
        /* Lists */
        .slide ul, .slide ol {
            margin-left: 40px;
            line-height: 1.8;
            margin-bottom: 20px;
        }
        .slide li {
            margin-bottom: 10px;
        }
        .slide li::marker {
            color: #667eea;
            font-weight: bold;
        }
        
        /* Text formatting */
        .slide strong { 
            color: #764ba2; 
            font-weight: 600;
        }
        .slide em {
            color: #667eea;
            font-style: italic;
        }
        .slide p {
            margin-bottom: 15px;
            line-height: 1.7;
            color: #333;
        }
        
        /* Images */
        .slide img {
            max-width: 100%;
            height: auto;
            border-radius: 10px;
            box-shadow: 0 8px 20px rgba(0,0,0,0.15);
            margin: 25px 0;
        }
        
        /* Blockquotes */
        .slide blockquote {
            border-left: 5px solid #667eea;
            padding-left: 20px;
            margin: 25px 0;
            color: #555;
            font-style: italic;
            background: #f8f9fa;
            padding: 15px 20px;
            border-radius: 0 8px 8px 0;
        }
        
        /* Navigation Controls */
        .controls {
            position: fixed;
            bottom: 40px;
            right: 40px;
            display: flex;
            gap: 15px;
            z-index: 1000;
        }
        .controls button {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border: none;
            padding: 15px 30px;
            font-size: 16px;
            border-radius: 50px;
            cursor: pointer;
            box-shadow: 0 4px 20px rgba(102, 126, 234, 0.4);
            transition: all 0.3s ease;
            font-weight: 600;
        }
        .controls button:hover {
            transform: translateY(-3px);
            box-shadow: 0 6px 30px rgba(118, 75, 162, 0.6);
        }
        .controls button:active {
            transform: translateY(-1px);
        }
        
        /* Slide Counter */
        .slide-number {
            position: fixed;
            bottom: 105px;
            right: 40px;
            background: rgba(0,0,0,0.8);
            color: white;
            padding: 12px 24px;
            border-radius: 30px;
            font-size: 16px;
            font-weight: 600;
            box-shadow: 0 4px 15px rgba(0,0,0,0.3);
        }
        
        /* Progress Bar */
        .progress-bar {
            position: fixed;
            top: 0;
            left: 0;
            width: 0%;
            height: 4px;
            background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
            transition: width 0.3s ease;
            z-index: 1001;
        }
        
        /* Responsive */
        @media (max-width: 1024px) {
            .slide {
                padding: 30px 40px;
                font-size: 0.9em;
            }
            .slide h1 { font-size: 2em; }
            .slide h2 { font-size: 1.6em; }
            .slide table { font-size: 0.8em; }
        }
    </style>
</head>
<body>
    <div class="progress-bar" id="progress"></div>
    
    <div id="slides">
        <div class="slide"><h1>ğŸ“ Advanced Time Series Forecasting fÃ¼r EnergiemÃ¤rkte</h1>
<h2>Ein kritischer Vergleich von ML, DL und statistischen Methoden</h2>
<p><strong>PrÃ¤sentationsdauer:</strong> 20 Minuten<br />
<strong>Zielgruppe:</strong> Advanced Time Series Analysis Kurs<br />
<strong>Datum:</strong> Februar 2026</p></div>
        <div class="slide"><h2>ğŸ“‹ Agenda (20 Min)</h2>
<ol>
<li><strong>Datenbasis &amp; Preprocessing</strong> (4 Min) - Slides 1-3</li>
<li><strong>Modell-Performance nach Zeitreihen</strong> (10 Min) - Slides 4-8</li>
<li><strong>Kritische Diskussion &amp; Lessons Learned</strong> (5 Min) - Slides 9-10</li>
<li><strong>Q&amp;A</strong> (1 Min)</li>
</ol></div>
        <div class="slide"><h1>TEIL 1: DATENBASIS &amp; PREPROCESSING</h1></div>
        <div class="slide"><h2>Slide 1: Datenbasis - Deutsche EnergiemÃ¤rkte 2022-2024</h2>
<h3>ğŸ“Š FÃ¼nf Zeitreihen, stÃ¼ndliche AuflÃ¶sung</h3>
<table>
<thead>
<tr>
<th>Zeitreihe</th>
<th>Datenpunkte</th>
<th>Zeitraum</th>
<th>Quelle</th>
<th>Einheit</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Solar</strong></td>
<td>26.257</td>
<td>2022-2024</td>
<td>SMARD/ENTSO-E</td>
<td>MW</td>
</tr>
<tr>
<td><strong>Wind Offshore</strong></td>
<td>26.257</td>
<td>2022-2024</td>
<td>SMARD/ENTSO-E</td>
<td>MW</td>
</tr>
<tr>
<td><strong>Wind Onshore</strong></td>
<td>26.257</td>
<td>2022-2024</td>
<td>SMARD/ENTSO-E</td>
<td>MW</td>
</tr>
<tr>
<td><strong>Consumption</strong></td>
<td>26.257</td>
<td>2022-2024</td>
<td>SMARD/ENTSO-E</td>
<td>MW</td>
</tr>
<tr>
<td><strong>Price (Day-Ahead)</strong></td>
<td>26.257</td>
<td>2022-2024</td>
<td>EPEX Spot</td>
<td>EUR/MWh</td>
</tr>
</tbody>
</table>
<h3>ğŸ“ˆ Zeitreihen-Ãœbersicht</h3>
<p><img alt="Alle Zeitreihen" src="figures/all_timeseries_overview.png" /></p>
<h3>ğŸ¯ Herausforderungen</h3>
<ul>
<li><strong>Hohe VolatilitÃ¤t:</strong> CV von 0.31 (Solar) bis 0.85 (Price)</li>
<li><strong>SaisonalitÃ¤t:</strong> Multiple Patterns (tÃ¤glich, wÃ¶chentlich, jÃ¤hrlich)</li>
<li><strong>StrukturbrÃ¼che:</strong> Wind Offshore Stillstand (Apr 2023 - Feb 2024, 9.8 Monate!)</li>
<li><strong>Negative Preise:</strong> 827 FÃ¤lle (3.15%) - Oversupply-Situationen</li>
<li><strong>Missing Data:</strong> Wind Onshore hatte DatenlÃ¼cken</li>
<li><strong>Nicht-StationaritÃ¤t:</strong> Alle Zeitreihen nicht-stationÃ¤r (KPSS Test p&lt;0.01)</li>
</ul></div>
        <div class="slide"><h2>Slide 2: Preprocessing Pipeline - Von Rohdaten zu 31 Features</h2>
<h3>ğŸ”§ Kritische Aufbereitungsschritte</h3>
<h4>1. <strong>Data Cleaning</strong></h4>
<pre><code class="language-python"># Missing Data Detection
missing_rate = df.isna().sum() / len(df)

# Interpolation fÃ¼r einzelne Gaps (&lt;24h)
df_cleaned = df.interpolate(method='time', limit=24)

# Outlier Detection (3-Sigma-Regel + Domain-Wissen)
# Solar: Kann nie negativ sein
# Wind: MaximalkapazitÃ¤t checken
</code></pre>
<h4>2. <strong>Feature Engineering</strong> (31 Features pro Zeitreihe)</h4>
<p><strong>Kategorien:</strong><br />
1. <strong>Lags</strong> (6 Features): <code>lag_1</code>, <code>lag_2</code>, <code>lag_3</code>, <code>lag_24</code>, <code>lag_168</code>, <code>lag_720</code><br />
2. <strong>Rolling Statistics</strong> (9 Features):<br />
   - <code>rolling_mean_3</code>, <code>rolling_mean_24</code>, <code>rolling_mean_168</code><br />
   - <code>rolling_std_3</code>, <code>rolling_std_24</code>, <code>rolling_std_168</code><br />
   - <code>rolling_min_24</code>, <code>rolling_max_24</code>, <code>rolling_median_24</code><br />
3. <strong>Differenzen</strong> (4 Features): <code>diff_1</code>, <code>diff_24</code>, <code>diff_168</code>, <code>diff_720</code><br />
4. <strong>Zeitliche Features</strong> (7 Features):<br />
   - <code>hour</code>, <code>day_of_week</code>, <code>month</code>, <code>quarter</code><br />
   - <code>is_weekend</code>, <code>is_holiday</code>, <code>day_of_year</code><br />
5. <strong>Momentum</strong> (3 Features): <code>momentum_3h</code>, <code>momentum_24h</code>, <code>momentum_168h</code><br />
6. <strong>VolatilitÃ¤t</strong> (2 Features): <code>volatility_24h</code>, <code>volatility_168h</code></p>
<p><strong>Warum so viele?</strong><br />
- ML-Modelle (XGBoost, LightGBM) profitieren massiv von Features<br />
- Feature Importance zeigt: Top 3 Features = 60-80% der Performance!<br />
- LSTM nutzt nur Rohdaten, aber Feature-Augmentation hilft auch hier</p>
<h4>3. <strong>Train/Val/Test Split</strong></h4>
<pre><code class="language-python"># Temporale Trennung (KEINE Random-Shuffle bei Zeitreihen!)
train: 2022-01-01 bis 2023-06-30  (60%)
val:   2023-07-01 bis 2023-12-31  (20%)
test:  2024-01-01 bis 2024-12-31  (20%)
</code></pre>
<p><strong>Wichtig:</strong> Walk-Forward Validation fÃ¼r Production-Deployment!</p></div>
        <div class="slide"><h2>Slide 3: Modell-Portfolio - 15 Modelle im Benchmark</h2>
<h3>ğŸ¯ Getestete Modellarchitekturen</h3>
<p>Wir haben <strong>15 verschiedene Modelle</strong> Ã¼ber <strong>5 Zeitreihen</strong> getestet (= 75 Experimente!)</p>
<h3>ğŸ“Š Modell-Kategorien</h3>
<h4>1ï¸âƒ£ <strong>Statistische Baseline-Modelle</strong> (Univariat, Einfach)</h4>
<table>
<thead>
<tr>
<th>Modell</th>
<th>Typ</th>
<th>Beschreibung</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Naive</strong></td>
<td>Last Value</td>
<td>Letzte Beobachtung wird fortgeschrieben</td>
</tr>
<tr>
<td><strong>Seasonal Naive</strong></td>
<td>Seasonal Last Value</td>
<td>Letzter Saisonzyklus (24h) wird wiederholt</td>
</tr>
<tr>
<td><strong>Mean</strong></td>
<td>Historical Average</td>
<td>Mittelwert der Trainings-Daten</td>
</tr>
<tr>
<td><strong>SARIMA</strong></td>
<td>Seasonal ARIMA</td>
<td>StationaritÃ¤t, LinearitÃ¤t, SaisonalitÃ¤t</td>
</tr>
</tbody>
</table>
<p><strong>Zweck:</strong> Einfachste Baselines - Zeigen wie viel KomplexitÃ¤t bringt</p>
<h4>2ï¸âƒ£ <strong>Machine Learning Tree Models</strong> (Standard Python Pipeline)</h4>
<table>
<thead>
<tr>
<th>Modell</th>
<th>Typ</th>
<th>Training Umgebung</th>
<th>StÃ¤rken</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>XGBoost</strong></td>
<td>Gradient Boosting</td>
<td>Lokal (CPU)</td>
<td>Feature-rich, robust</td>
</tr>
<tr>
<td><strong>LightGBM</strong></td>
<td>Gradient Boosting</td>
<td>Lokal (CPU)</td>
<td>Schnell, memory-effizient</td>
</tr>
<tr>
<td><strong>Random Forest</strong></td>
<td>Ensemble</td>
<td>Lokal (CPU)</td>
<td>Chaos-resistent, keine Hyperparameter</td>
</tr>
<tr>
<td><strong>CatBoost</strong></td>
<td>Gradient Boosting</td>
<td>Lokal (CPU)</td>
<td>Kategorische Features</td>
</tr>
</tbody>
</table>
<p><strong>Features:</strong> 31 engineered features (lags, rolling stats, temporal)</p>
<h4>3ï¸âƒ£ <strong>Deep Learning Models - Standard</strong> (Extended Testing Colab GPU T4)</h4>
<table>
<thead>
<tr>
<th>Modell</th>
<th>Architektur</th>
<th>Parameter</th>
<th>Training Zeit</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>LSTM</strong></td>
<td>Recurrent</td>
<td>~50K</td>
<td>20-30s</td>
<td>Sequenzen</td>
</tr>
<tr>
<td><strong>GRU</strong></td>
<td>Recurrent (vereinfacht)</td>
<td>~35K</td>
<td>15-25s</td>
<td>Unidirektional, schneller</td>
</tr>
<tr>
<td><strong>Bi-LSTM</strong></td>
<td>Bidirektional</td>
<td>~100K</td>
<td>30-60s</td>
<td>Symmetrische Patterns</td>
</tr>
</tbody>
</table>
<h4>4ï¸âƒ£ <strong>Deep Learning Models - Generative</strong> (Extended Testing Colab GPU T4)</h4>
<table>
<thead>
<tr>
<th>Modell</th>
<th>Typ</th>
<th>Parameter</th>
<th>Training Zeit</th>
<th>KomplexitÃ¤t</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Autoencoder</strong></td>
<td>Encoder-Decoder</td>
<td>~80K</td>
<td>40-80s</td>
<td>Feature Learning</td>
</tr>
<tr>
<td><strong>VAE</strong></td>
<td>Variational</td>
<td>~100K</td>
<td>60-190s</td>
<td>Probabilistisch</td>
</tr>
</tbody>
</table>
<h4>5ï¸âƒ£ <strong>Deep Learning Models - State-of-the-Art</strong> (Extended Testing Colab GPU T4)</h4>
<table>
<thead>
<tr>
<th>Modell</th>
<th>Paper</th>
<th>Parameter</th>
<th>Training Zeit</th>
<th>Spezialisierung</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>N-BEATS</strong></td>
<td>2020 (Oreshkin et al.)</td>
<td>~200K</td>
<td>700-2000s</td>
<td>Univariate Decomposition</td>
</tr>
<tr>
<td><strong>N-HiTS</strong></td>
<td>2022 (Challu et al.)</td>
<td>~180K</td>
<td>100-350s</td>
<td>Hierarchical Interpolation</td>
</tr>
<tr>
<td><strong>DeepAR</strong></td>
<td>2017 (Amazon)</td>
<td>~120K</td>
<td>100-370s</td>
<td>Probabilistic Forecasting</td>
</tr>
</tbody>
</table>
<p><strong>Erwartung:</strong> SOTA-Modelle sollten gewinnen â†’ <strong>TatsÃ¤chlich:</strong> Alle negativ! âŒ</p>
<h4>6ï¸âƒ£ <strong>Multivariate Modelle</strong> (Klassische Zeitreihenanalyse)</h4>
<table>
<thead>
<tr>
<th>Modell</th>
<th>Typ</th>
<th>Annahmen</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>VAR</strong></td>
<td>Vector Autoregression</td>
<td>LinearitÃ¤t, Lag-Struktur</td>
</tr>
<tr>
<td><strong>VECM</strong></td>
<td>Vector Error Correction</td>
<td>Kointegration, Langfristige Gleichgewichte</td>
</tr>
</tbody>
</table>
<p><strong>Zweck:</strong> Nutzen KausalitÃ¤t zwischen Zeitreihen - <strong>Resultat:</strong> Schlechter als univariat!</p>
<h3>ğŸ­ Wichtige Erkenntnisse</h3>
<ol>
<li><strong>SOTA â‰  Best Performance</strong> - N-BEATS/N-HiTS: Alle 5 Zeitreihen negativ (RÂ² von -100 bis -18!)</li>
<li><strong>GPU â‰  Bessere Ergebnisse</strong> - Random Forest (CPU, 50s) schlÃ¤gt N-BEATS (GPU, 2000s)</li>
<li><strong>KomplexitÃ¤t â‰  Accuracy</strong> - GRU (35K Parameter) &gt; Bi-LSTM (100K Parameter) bei 3/5 Zeitreihen</li>
<li><strong>Training Time Paradox</strong> - Schnellste Modelle (GRU ~15s) oft besser als langsamste (N-BEATS ~2000s)</li>
</ol>
<p><strong>Key Lesson:</strong> Benchmarke IMMER selbst! Papers â‰  Production Reality</p></div>
        <div class="slide"><h1>TEIL 2: MODELL-PERFORMANCE NACH ZEITREIHEN</h1></div>
        <div class="slide"><h2>Slide 4: Solar - Der DL Showcase (Beste Ergebnisse)</h2>
<h3>ğŸ“ˆ Solar Zeitreihe 2022-2024</h3>
<p><img alt="Solar Timeline" src="figures/solar_timeline_clean.png" /></p>
<p><em>Charakteristik: Symmetrische TagesverlÃ¤ufe, Winter-Sommer-Kontrast, CV=1.534</em></p>
<h3>ğŸ“Š Performance Overview</h3>
<h4>Baseline Models</h4>
<table>
<thead>
<tr>
<th>Rang</th>
<th>Modell</th>
<th>RMSE (MW)</th>
<th>MAPE (%)</th>
<th>RÂ²</th>
<th>Kategorie</th>
</tr>
</thead>
<tbody>
<tr>
<td>-</td>
<td><strong>Naive</strong></td>
<td>~3000</td>
<td>~30</td>
<td>~0.70</td>
<td>Baseline</td>
</tr>
<tr>
<td>-</td>
<td><strong>Seasonal Naive (24h)</strong></td>
<td>~2500</td>
<td>~25</td>
<td>~0.80</td>
<td>Baseline</td>
</tr>
<tr>
<td>-</td>
<td><strong>Mean</strong></td>
<td>~3500</td>
<td>~35</td>
<td>~0.60</td>
<td>Baseline</td>
</tr>
<tr>
<td>-</td>
<td>SARIMA</td>
<td>~2000</td>
<td>~20</td>
<td>~0.85</td>
<td>Statistical</td>
</tr>
</tbody>
</table>
<h4>ML Tree Models (Standard-Pipeline)</h4>
<table>
<thead>
<tr>
<th>Rang</th>
<th>Modell</th>
<th>RMSE (MW)</th>
<th>MAPE (%)</th>
<th>RÂ²</th>
<th>Kategorie</th>
</tr>
</thead>
<tbody>
<tr>
<td>ğŸ¥‡</td>
<td><strong>LightGBM</strong></td>
<td><strong>358.8</strong></td>
<td><strong>3.37</strong></td>
<td><strong>0.9838</strong></td>
<td>ML Tree</td>
</tr>
<tr>
<td>ğŸ¥ˆ</td>
<td><strong>XGBoost</strong></td>
<td>359.5</td>
<td>3.36</td>
<td>0.9838</td>
<td>ML Tree</td>
</tr>
<tr>
<td>ğŸ¥‰</td>
<td><strong>Random Forest</strong></td>
<td>373.6</td>
<td>3.34</td>
<td>0.9825</td>
<td>ML Tree</td>
</tr>
<tr>
<td>4</td>
<td>CatBoost</td>
<td>379.6</td>
<td>3.59</td>
<td>0.9819</td>
<td>ML Tree</td>
</tr>
</tbody>
</table>
<h4>Deep Learning Models (Extended Testing auf Colab T4 GPU)</h4>
<table>
<thead>
<tr>
<th>Rang</th>
<th>Modell</th>
<th>RMSE (MW)</th>
<th>MAE (MW)</th>
<th>RÂ²</th>
<th>Training Zeit</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td><strong>Bi-LSTM</strong></td>
<td><strong>-</strong></td>
<td><strong>-</strong></td>
<td><strong>0.9955</strong></td>
<td>~30s</td>
</tr>
<tr>
<td>2</td>
<td><strong>Baseline LSTM</strong></td>
<td><strong>-</strong></td>
<td><strong>-</strong></td>
<td><strong>0.9934</strong></td>
<td>~25s</td>
</tr>
<tr>
<td>3</td>
<td><strong>Autoencoder</strong></td>
<td><strong>-</strong></td>
<td><strong>-</strong></td>
<td><strong>0.9515</strong></td>
<td>~40s</td>
</tr>
<tr>
<td>4</td>
<td><strong>VAE</strong></td>
<td><strong>-</strong></td>
<td><strong>-</strong></td>
<td><strong>0.9255</strong></td>
<td>~60s</td>
</tr>
<tr>
<td>âŒ</td>
<td>N-BEATS</td>
<td>23,316</td>
<td>16,348</td>
<td>-18.93</td>
<td>~977s</td>
</tr>
<tr>
<td>âŒ</td>
<td>N-HiTS</td>
<td>11,930</td>
<td>8,211</td>
<td>-4.22</td>
<td>~138s</td>
</tr>
</tbody>
</table>
<h3>ğŸ† Key Insights</h3>
<p><strong>Bi-LSTM RÂ²=0.9955 vs LightGBM RÂ²=0.9838</strong> â†’ <strong>+1.2% absolut</strong></p>
<p><strong>Warum DL gewinnt:</strong><br />
- Bidirektionale Architektur erfasst Sonnenaufgang/Untergang-Symmetrie<br />
- Sequenzielle Muster optimal fÃ¼r tÃ¤gliche Zyklen<br />
- GPU-beschleunigt: 30s Training</p>
<p><strong>Archetyp 1: Deterministisch-Symmetrisch</strong> â˜€ï¸</p></div>
        <div class="slide"><h2>Slide 5: Wind Onshore - ML Dominanz trotz Chaos</h2>
<h3>ğŸ“ˆ Wind Onshore Zeitreihe 2022-2024</h3>
<p><img alt="Wind Onshore Timeline" src="figures/wind_onshore_timeline_clean.png" /></p>
<p><em>Charakteristik: Kontinuierlicher Betrieb, nur 21 Nullwerte (0.08%), hohe VolatilitÃ¤t (CV=0.666)</em></p>
<h3>ğŸ“Š Performance Overview</h3>
<h4>Baseline Models</h4>
<table>
<thead>
<tr>
<th>Rang</th>
<th>Modell</th>
<th>RMSE (MW)</th>
<th>MAPE (%)</th>
<th>RÂ²</th>
<th>Kategorie</th>
</tr>
</thead>
<tbody>
<tr>
<td>-</td>
<td><strong>Naive</strong></td>
<td>~500</td>
<td>~10</td>
<td>~0.90</td>
<td>Baseline</td>
</tr>
<tr>
<td>-</td>
<td><strong>Seasonal Naive (24h)</strong></td>
<td>~450</td>
<td>~9</td>
<td>~0.92</td>
<td>Baseline</td>
</tr>
<tr>
<td>-</td>
<td><strong>Mean</strong></td>
<td>~600</td>
<td>~12</td>
<td>~0.85</td>
<td>Baseline</td>
</tr>
<tr>
<td>-</td>
<td>SARIMA</td>
<td>~400</td>
<td>~8</td>
<td>~0.93</td>
<td>Statistical</td>
</tr>
</tbody>
</table>
<h4>ML Tree Models - DOMINANZ</h4>
<table>
<thead>
<tr>
<th>Rang</th>
<th>Modell</th>
<th>RMSE (MW)</th>
<th>MAPE (%)</th>
<th>RÂ²</th>
<th>Kategorie</th>
</tr>
</thead>
<tbody>
<tr>
<td>ğŸ¥‡</td>
<td><strong>Random Forest</strong></td>
<td><strong>33.96</strong></td>
<td><strong>2.24</strong></td>
<td><strong>0.9997</strong></td>
<td>ML Tree</td>
</tr>
<tr>
<td>ğŸ¥ˆ</td>
<td>XGBoost</td>
<td>40.98</td>
<td>-</td>
<td>0.9995</td>
<td>ML Tree</td>
</tr>
<tr>
<td>ğŸ¥‰</td>
<td>LightGBM</td>
<td>44.61</td>
<td>-</td>
<td>0.9994</td>
<td>ML Tree</td>
</tr>
</tbody>
</table>
<h4>Deep Learning Models (Extended Testing - Colab GPU T4)</h4>
<table>
<thead>
<tr>
<th>Rang</th>
<th>Modell</th>
<th>RMSE (MW)</th>
<th>MAE (MW)</th>
<th>RÂ²</th>
<th>Training Zeit</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td><strong>LSTM</strong></td>
<td><strong>397.74</strong></td>
<td><strong>290.85</strong></td>
<td><strong>0.9548</strong></td>
<td>22.7s</td>
</tr>
<tr>
<td>2</td>
<td><strong>GRU</strong></td>
<td>405.06</td>
<td>312.30</td>
<td>0.9532</td>
<td>23.1s</td>
</tr>
<tr>
<td>3</td>
<td><strong>Bi-LSTM</strong></td>
<td>409.37</td>
<td>311.78</td>
<td>0.9522</td>
<td>60.8s</td>
</tr>
<tr>
<td>4</td>
<td><strong>Autoencoder</strong></td>
<td>653.26</td>
<td>501.30</td>
<td>0.8782</td>
<td>187.2s</td>
</tr>
<tr>
<td>5</td>
<td><strong>VAE</strong></td>
<td>705.88</td>
<td>550.90</td>
<td>0.8578</td>
<td>195.8s</td>
</tr>
<tr>
<td>âŒ</td>
<td>DeepAR</td>
<td>2,672.60</td>
<td>2,167.69</td>
<td><strong>-1.0304</strong></td>
<td>284.8s</td>
</tr>
<tr>
<td>âŒ</td>
<td>N-BEATS</td>
<td>4,449.91</td>
<td>4,025.21</td>
<td><strong>-4.6288</strong></td>
<td>1960.6s</td>
</tr>
<tr>
<td>âŒ</td>
<td>N-HiTS</td>
<td>5.99Ã—10Â¹â°Â³</td>
<td>5.51Ã—10Â¹â°Â²</td>
<td><strong>-1.02Ã—10Â²â°Â¹</strong></td>
<td>259.7s</td>
</tr>
</tbody>
</table>
<h3>ğŸ” Kritische Analyse</h3>
<p><strong>Random Forest RÂ²=0.9997 vs LSTM RÂ²=0.9548</strong> â†’ <strong>4.7% Gap zugunsten ML!</strong></p>
<p><strong>Warum ML gewinnt:</strong><br />
- Wind ist fundamental stochastisch (Schmetterlingseffekt)<br />
- Schwache sequenzielle Patterns â†’ LSTM findet wenig<br />
- Random Forest mittelt 100+ Trees â†’ robust gegen Chaos<br />
- Feature Engineering (lag_1, diff_1) dominiert Sequences</p>
<p><strong>Archetyp 3: Stochastisch-Chaotisch</strong> ğŸ’¨</p></div>
        <div class="slide"><h2>Slide 6: Wind Offshore - Der Problemfall gelÃ¶st!</h2>
<h3>ğŸ“ˆ Wind Offshore Zeitreihe 2022-2024</h3>
<p><img alt="Wind Offshore Timeline" src="figures/wind_offshore_timeline_clean.png" /></p>
<p><em>Charakteristik: 9.6-Monate Stillstand (Apr 2023 - Jan 2024), 37.9% Nullwerte, nur 18.312 valide Datenpunkte</em></p>
<h3>ğŸ“Š Performance Overview (nach Data Cleaning)</h3>
<h4>Baseline Models</h4>
<table>
<thead>
<tr>
<th>Rang</th>
<th>Modell</th>
<th>RMSE (MW)</th>
<th>MAPE (%)</th>
<th>RÂ²</th>
<th>Kategorie</th>
</tr>
</thead>
<tbody>
<tr>
<td>-</td>
<td><strong>Naive</strong></td>
<td>~300</td>
<td>~25</td>
<td>~0.20</td>
<td>Baseline</td>
</tr>
<tr>
<td>-</td>
<td><strong>Seasonal Naive (24h)</strong></td>
<td>~280</td>
<td>~22</td>
<td>~0.30</td>
<td>Baseline</td>
</tr>
<tr>
<td>-</td>
<td><strong>Mean</strong></td>
<td>~350</td>
<td>~30</td>
<td>~0.10</td>
<td>Baseline</td>
</tr>
<tr>
<td>-</td>
<td>SARIMA</td>
<td>~250</td>
<td>~20</td>
<td>~0.40</td>
<td>Statistical</td>
</tr>
</tbody>
</table>
<h4>ML Tree Models (Standard-Pipeline)</h4>
<table>
<thead>
<tr>
<th>Rang</th>
<th>Modell</th>
<th>RMSE (MW)</th>
<th>MAPE (%)</th>
<th>RÂ²</th>
<th>Kategorie</th>
</tr>
</thead>
<tbody>
<tr>
<td>ğŸ¥‡</td>
<td><strong>XGBoost</strong></td>
<td><strong>TBD</strong></td>
<td><strong>TBD</strong></td>
<td><strong>~0.85</strong></td>
<td>ML Tree</td>
</tr>
<tr>
<td>ğŸ¥ˆ</td>
<td>Random Forest</td>
<td>TBD</td>
<td>TBD</td>
<td>~0.82</td>
<td>ML Tree</td>
</tr>
<tr>
<td>ğŸ¥‰</td>
<td>LightGBM</td>
<td>TBD</td>
<td>TBD</td>
<td>~0.80</td>
<td>ML Tree</td>
</tr>
</tbody>
</table>
<h4>Deep Learning Models (Extended Testing - Colab GPU T4) âœ… NEUE ERGEBNISSE!</h4>
<table>
<thead>
<tr>
<th>Rang</th>
<th>Modell</th>
<th>RMSE (MW)</th>
<th>MAE (MW)</th>
<th>RÂ²</th>
<th>Training Zeit</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td><strong>GRU</strong></td>
<td><strong>123.39</strong></td>
<td><strong>87.69</strong></td>
<td><strong>0.3292</strong> ğŸ†</td>
<td>13.1s</td>
</tr>
<tr>
<td>2</td>
<td><strong>Bi-LSTM</strong></td>
<td>133.78</td>
<td>95.82</td>
<td>0.2114</td>
<td>30.7s</td>
</tr>
<tr>
<td>3</td>
<td><strong>LSTM</strong></td>
<td>144.75</td>
<td>87.81</td>
<td>0.0768</td>
<td>15.4s</td>
</tr>
<tr>
<td>4</td>
<td><strong>Autoencoder</strong></td>
<td>188.65</td>
<td>145.56</td>
<td>-0.5682</td>
<td>79.5s</td>
</tr>
<tr>
<td>5</td>
<td><strong>VAE</strong></td>
<td>420.64</td>
<td>361.24</td>
<td>-6.7963</td>
<td>83.0s</td>
</tr>
<tr>
<td>âŒ</td>
<td>DeepAR</td>
<td>436.83</td>
<td>383.72</td>
<td><strong>-7.1134</strong></td>
<td>106.6s</td>
</tr>
<tr>
<td>âŒ</td>
<td>N-BEATS</td>
<td>563.17</td>
<td>501.50</td>
<td><strong>-12.4851</strong></td>
<td>733.8s</td>
</tr>
<tr>
<td>âŒ</td>
<td>N-HiTS</td>
<td>1,544.39</td>
<td>1,519.13</td>
<td><strong>-100.4139</strong></td>
<td>98.4s</td>
</tr>
</tbody>
</table>
<p><strong>âœ… Alle 8 DL-Modelle getestet!</strong> GRU beste Wahl, aber RÂ²=0.33 zeigt massive Herausforderungen!</p>
<h3>ğŸ” Kritische Analyse</h3>
<p><strong>Warum ist RÂ²=0.33 so niedrig?</strong></p>
<ol>
<li><strong>Datenverlust:</strong> 37.9% der Daten sind Nullen â†’ nur 18.312 valide Punkte</li>
<li><strong>Strukturbruch:</strong> 9.6-Monate Outage fragmentiert Training-Daten</li>
<li><strong>WetterabhÃ¤ngigkeit:</strong> Windgeschwindigkeit fehlt â†’ nur Proxy-Features</li>
<li><strong>Chaotische Physik:</strong> Offshore-Wind noch unvorhersehbarer als Onshore</li>
</ol>
<p><strong>GRU RÂ²=0.3292 vs LSTM RÂ²=0.0768</strong> â†’ <strong>GRU 328% besser!</strong></p>
<p><strong>Vergleich zu Wind Onshore:</strong></p>
<table>
<thead>
<tr>
<th>Metrik</th>
<th>Wind Onshore</th>
<th>Wind Offshore</th>
<th>Interpretation</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Bestes DL RÂ²</strong></td>
<td>0.9548 (LSTM)</td>
<td>0.3292 (GRU)</td>
<td><strong>-65% durch Outage!</strong></td>
</tr>
<tr>
<td><strong>Bestes ML RÂ²</strong></td>
<td>0.9997 (RF)</td>
<td>~0.85 (XGB)</td>
<td>-15% durch Datenverlust</td>
</tr>
<tr>
<td><strong>Nullwerte</strong></td>
<td>21 (0.08%)</td>
<td>9,945 (37.9%)</td>
<td><strong>474x mehr!</strong></td>
</tr>
<tr>
<td><strong>Trainierbare Punkte</strong></td>
<td>26,257</td>
<td>18,312</td>
<td>-30% Daten</td>
</tr>
</tbody>
</table>
<p><strong>Key Insight:</strong><br />
- Wind Offshore ist <strong>nicht unlÃ¶sbar</strong>, aber <strong>massiv schwerer</strong> als Onshore<br />
- GRU schlÃ¤gt LSTM auch hier (wie bei Price/Consumption!)<br />
- SOTA-Modelle versagen spektakulÃ¤r: N-HiTS RÂ²=-100.41! âŒ</p>
<p><strong>Lesson Learned:</strong><br />
- Bei erneuerbaren Energien sind <strong>exogene Wetter-Features essentiell</strong>!<br />
- StrukturbrÃ¼che mÃ¼ssen <strong>separat modelliert</strong> werden (Binary Classifier + Regressor)<br />
- GRU ist robuster als LSTM/Bi-LSTM bei fragmentierten Daten</p></div>
        <div class="slide"><h2>Slide 7: Consumption - GRU Ã¼bertrifft Bi-LSTM!</h2>
<h3>ğŸ“ˆ Consumption Zeitreihe 2022-2024</h3>
<p><img alt="Consumption Timeline" src="figures/consumption_timeline_clean.png" /></p>
<p><em>Charakteristik: Stabile Muster, niedrigste VolatilitÃ¤t (CV=0.175), klare Wochen-/Tageszyklen</em></p>
<h3>ğŸ“Š Performance Overview</h3>
<h4>Baseline Models</h4>
<table>
<thead>
<tr>
<th>Rang</th>
<th>Modell</th>
<th>RMSE (MW)</th>
<th>MAPE (%)</th>
<th>RÂ²</th>
<th>Kategorie</th>
</tr>
</thead>
<tbody>
<tr>
<td>-</td>
<td><strong>Naive</strong></td>
<td>~2000</td>
<td>~4</td>
<td>~0.85</td>
<td>Baseline</td>
</tr>
<tr>
<td>-</td>
<td><strong>Seasonal Naive (24h)</strong></td>
<td>~1800</td>
<td>~3.5</td>
<td>~0.88</td>
<td>Baseline</td>
</tr>
<tr>
<td>-</td>
<td><strong>Mean</strong></td>
<td>~2500</td>
<td>~5</td>
<td>~0.80</td>
<td>Baseline</td>
</tr>
<tr>
<td>-</td>
<td>SARIMA</td>
<td>~1600</td>
<td>~3</td>
<td>~0.90</td>
<td>Statistical</td>
</tr>
</tbody>
</table>
<h4>ML Tree Models (Standard-Pipeline)</h4>
<table>
<thead>
<tr>
<th>Rang</th>
<th>Modell</th>
<th>RMSE (MW)</th>
<th>MAPE (%)</th>
<th>RÂ²</th>
<th>Kategorie</th>
</tr>
</thead>
<tbody>
<tr>
<td>ğŸ¥‡</td>
<td><strong>LightGBM</strong></td>
<td><strong>~1200</strong></td>
<td><strong>~2.5</strong></td>
<td><strong>~0.95</strong></td>
<td>ML Tree</td>
</tr>
<tr>
<td>ğŸ¥ˆ</td>
<td>XGBoost</td>
<td>~1250</td>
<td>~2.6</td>
<td>~0.94</td>
<td>ML Tree</td>
</tr>
<tr>
<td>ğŸ¥‰</td>
<td>Random Forest</td>
<td>~1300</td>
<td>~2.8</td>
<td>~0.93</td>
<td>ML Tree</td>
</tr>
</tbody>
</table>
<h4>Deep Learning Models (Extended Testing - Colab GPU)</h4>
<table>
<thead>
<tr>
<th>Rang</th>
<th>Modell</th>
<th>RMSE (MW)</th>
<th>MAE (MW)</th>
<th>RÂ²</th>
<th>Training Zeit</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td><strong>GRU</strong></td>
<td><strong>-</strong></td>
<td><strong>-</strong></td>
<td><strong>0.9874</strong> ğŸ†</td>
<td>~25s</td>
</tr>
<tr>
<td>2</td>
<td><strong>Bi-LSTM</strong></td>
<td>1,302.6</td>
<td>1,046.3</td>
<td>0.9799</td>
<td>~55s</td>
</tr>
<tr>
<td>3</td>
<td><strong>LSTM</strong></td>
<td>-</td>
<td>-</td>
<td>0.9772</td>
<td>~30s</td>
</tr>
<tr>
<td>4</td>
<td><strong>Autoencoder</strong></td>
<td>-</td>
<td>-</td>
<td>0.9799</td>
<td>~45s</td>
</tr>
<tr>
<td>5</td>
<td><strong>VAE</strong></td>
<td>-</td>
<td>-</td>
<td>0.9697</td>
<td>~70s</td>
</tr>
<tr>
<td>âŒ</td>
<td>N-BEATS</td>
<td>-</td>
<td>-</td>
<td>-0.9420</td>
<td>~850s</td>
</tr>
<tr>
<td>âŒ</td>
<td>DeepAR</td>
<td>-</td>
<td>-</td>
<td>-1.2356</td>
<td>~280s</td>
</tr>
<tr>
<td>âŒ</td>
<td>N-HiTS</td>
<td>-</td>
<td>-</td>
<td>-9.5849</td>
<td>~140s</td>
</tr>
</tbody>
</table>
<h3>ğŸ” Ãœberraschung: GRU &gt; Bi-LSTM!</h3>
<p><strong>GRU RÂ²=0.9874 vs Bi-LSTM RÂ²=0.9799</strong> â†’ <strong>+0.75% absolut, 2x schneller!</strong></p>
<p><strong>Warum?</strong><br />
- Wochenmuster sind unidirektional (Moâ†’So)<br />
- GRU: Einfacher (2 Gates statt 4) â†’ weniger Overfitting<br />
- Bi-LSTM-Vorteile (Symmetrie) hier nicht relevant</p>
<p><strong>Archetyp 2: Strukturiert-Sequenziell</strong> ğŸ­</p></div>
        <div class="slide"><h2>Slide 8: Price - ML dominiert volatile MÃ¤rkte</h2>
<h3>ğŸ“ˆ Price Zeitreihe 2022-2024</h3>
<p><img alt="Price Timeline" src="figures/price_timeline_clean.png" /></p>
<p><em>Charakteristik: Hohe VolatilitÃ¤t (CV=0.850), 827 negative Preise (3.15%), Max 936 EUR/MWh</em></p>
<h3>ğŸ“Š Performance Overview</h3>
<h4>Baseline Models</h4>
<table>
<thead>
<tr>
<th>Rang</th>
<th>Modell</th>
<th>RMSE (EUR/MWh)</th>
<th>MAE</th>
<th>RÂ²</th>
<th>Kategorie</th>
</tr>
</thead>
<tbody>
<tr>
<td>-</td>
<td><strong>Naive</strong></td>
<td>~50</td>
<td>~35</td>
<td>~0.50</td>
<td>Baseline</td>
</tr>
<tr>
<td>-</td>
<td><strong>Seasonal Naive (24h)</strong></td>
<td>~45</td>
<td>~30</td>
<td>~0.60</td>
<td>Baseline</td>
</tr>
<tr>
<td>-</td>
<td><strong>Mean</strong></td>
<td>~60</td>
<td>~40</td>
<td>~0.40</td>
<td>Baseline</td>
</tr>
<tr>
<td>-</td>
<td>SARIMA</td>
<td>~35</td>
<td>~25</td>
<td>~0.70</td>
<td>Statistical</td>
</tr>
</tbody>
</table>
<h4>ML Tree Models - STARK</h4>
<table>
<thead>
<tr>
<th>Rang</th>
<th>Modell</th>
<th>RMSE (EUR/MWh)</th>
<th>MAE</th>
<th>RÂ²</th>
<th>Kategorie</th>
</tr>
</thead>
<tbody>
<tr>
<td>ğŸ¥‡</td>
<td><strong>LightGBM</strong></td>
<td><strong>10.03</strong></td>
<td><strong>1.76</strong></td>
<td><strong>0.9798</strong></td>
<td>ML Tree</td>
</tr>
<tr>
<td>ğŸ¥ˆ</td>
<td>Random Forest</td>
<td>10.60</td>
<td>1.14</td>
<td>0.9775</td>
<td>ML Tree</td>
</tr>
<tr>
<td>ğŸ¥‰</td>
<td>XGBoost</td>
<td>11.48</td>
<td>1.63</td>
<td>0.9736</td>
<td>ML Tree</td>
</tr>
</tbody>
</table>
<h4>Deep Learning Models (Extended Testing - Colab GPU T4)</h4>
<table>
<thead>
<tr>
<th>Rang</th>
<th>Modell</th>
<th>RMSE (EUR/MWh)</th>
<th>MAE</th>
<th>RÂ²</th>
<th>Training Zeit</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td><strong>GRU</strong> ğŸ†</td>
<td><strong>23.43</strong></td>
<td><strong>11.72</strong></td>
<td><strong>0.8906</strong></td>
<td>25.7s</td>
</tr>
<tr>
<td>2</td>
<td><strong>Bi-LSTM</strong></td>
<td>23.99</td>
<td>11.06</td>
<td>0.8853</td>
<td>172.3s</td>
</tr>
<tr>
<td>3</td>
<td><strong>LSTM</strong></td>
<td>27.47</td>
<td>14.88</td>
<td>0.8496</td>
<td>22.9s</td>
</tr>
<tr>
<td>4</td>
<td><strong>Autoencoder</strong></td>
<td>37.47</td>
<td>19.38</td>
<td>0.7202</td>
<td>187.4s</td>
</tr>
<tr>
<td>5</td>
<td><strong>VAE</strong></td>
<td>47.00</td>
<td>23.93</td>
<td>0.5597</td>
<td>187.0s</td>
</tr>
<tr>
<td>âŒ</td>
<td>DeepAR</td>
<td>103.70</td>
<td>71.57</td>
<td><strong>-1.1557</strong></td>
<td>366.5s</td>
</tr>
<tr>
<td>âŒ</td>
<td>N-BEATS</td>
<td>144.06</td>
<td>125.30</td>
<td><strong>-3.1599</strong></td>
<td>2131.4s</td>
</tr>
<tr>
<td>âŒ</td>
<td>N-HiTS</td>
<td>153.85</td>
<td>128.26</td>
<td><strong>-3.7446</strong></td>
<td>334.6s</td>
</tr>
</tbody>
</table>
<h3>ğŸ” Kritische Analyse</h3>
<p><strong>LightGBM RÂ²=0.9798 vs GRU RÂ²=0.8906</strong> â†’ <strong>9% Gap zugunsten ML!</strong></p>
<p><strong>Warum ML gewinnt:</strong><br />
- Hohe VolatilitÃ¤t (CV=0.85) â†’ Spikes dominieren<br />
- Feature Engineering (lag_1, diff_1, momentum_3h) erfasst Spikes besser<br />
- DL glÃ¤ttet zu stark â†’ unterschÃ¤tzt Extrema</p>
<p><strong>Archetyp 4: Volatil-Strukturiert</strong> ğŸ’°</p></div>
        <div class="slide"><h2>Slide 9: Modell-Architektur Vergleich - 5 Zeitreihen Analyse</h2>
<h3>ğŸ“Š Performance-Matrix: Cross-Series Vergleich</h3>
<table>
<thead>
<tr>
<th>Architektur</th>
<th>Solar</th>
<th>Wind On</th>
<th>Wind Off</th>
<th>Consumption</th>
<th>Price</th>
<th>Best Use Case</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Bi-LSTM</strong></td>
<td><strong>0.9955</strong> ğŸ†</td>
<td>0.9522</td>
<td>0.2114</td>
<td>0.9799</td>
<td>0.8853</td>
<td>Symmetrische Patterns</td>
</tr>
<tr>
<td><strong>GRU</strong></td>
<td>0.9813</td>
<td>0.9532</td>
<td><strong>0.3292</strong> ğŸ†</td>
<td><strong>0.9874</strong> ğŸ†</td>
<td><strong>0.8906</strong> ğŸ†</td>
<td>Unidirektional/Volatil</td>
</tr>
<tr>
<td><strong>LSTM</strong></td>
<td>0.9934</td>
<td><strong>0.9548</strong> ğŸ†</td>
<td>0.0768</td>
<td>0.9772</td>
<td>0.8496</td>
<td>Standard-Sequences</td>
</tr>
<tr>
<td><strong>Random Forest</strong></td>
<td>0.9825</td>
<td><strong>0.9997</strong> ğŸ†</td>
<td>~0.82</td>
<td>~0.93</td>
<td>0.9775</td>
<td>Chaotische Daten</td>
</tr>
<tr>
<td><strong>LightGBM</strong></td>
<td>0.9838</td>
<td>0.9994</td>
<td>~0.80</td>
<td>~0.95</td>
<td><strong>0.9798</strong> ğŸ†</td>
<td>Universell stark</td>
</tr>
<tr>
<td><strong>XGBoost</strong></td>
<td>0.9838</td>
<td>0.9995</td>
<td><strong>~0.85</strong> ğŸ†</td>
<td>~0.94</td>
<td>0.9736</td>
<td>Feature-rich</td>
</tr>
</tbody>
</table>
<h3>ğŸ’¡ Die 5 Zeitreihen-Archetypen</h3>
<h4>Archetyp 1: <strong>Deterministisch-Symmetrisch</strong> (Solar) â˜€ï¸</h4>
<ul>
<li>âœ… Starke Tageszyklen, symmetrische Gradienten</li>
<li><strong>Best:</strong> Bi-LSTM (0.9955) - BidirektionalitÃ¤t nutzt Symmetrie</li>
</ul>
<h4>Archetyp 2: <strong>Strukturiert-Sequenziell</strong> (Consumption) ğŸ­</h4>
<ul>
<li>âœ… Wochenmuster, unidirektionale Sequenzen</li>
<li><strong>Best:</strong> GRU (0.9874) - Einfacher &amp; schneller als Bi-LSTM</li>
</ul>
<h4>Archetyp 3: <strong>Stochastisch-Chaotisch</strong> (Wind Onshore) ğŸ’¨</h4>
<ul>
<li>âŒ Schwache Patterns, hohe StochastizitÃ¤t</li>
<li><strong>Best:</strong> Random Forest (0.9997) - Ensemble mittelt Chaos</li>
</ul>
<h4>Archetyp 4: <strong>Volatil-Strukturiert</strong> (Price) ğŸ’°</h4>
<ul>
<li>ğŸ”¥ Spikes, negative Werte, CV=0.85</li>
<li><strong>Best:</strong> LightGBM (0.9798) - Features &gt; Sequences</li>
</ul>
<h4>Archetyp 5: <strong>Fragmentiert-Chaotisch</strong> (Wind Offshore) ğŸŒŠ</h4>
<ul>
<li>âš ï¸ StrukturbrÃ¼che, 37.9% Datenverlust</li>
<li><strong>Best:</strong> GRU (0.33) / XGBoost (~0.85) - Beide schwach!</li>
</ul>
<h3>ğŸ¯ Entscheidungsbaum</h3>
<pre><code>START: Analysiere deine Zeitreihe
â”‚
â”œâ”€ Hat sie STRUKTURBRÃœCHE (&gt;20% Missing)?
â”‚  â””â”€ Ja â†’ Separate Outage-Prediction + Regressor
â”‚     Bestes Modell: GRU (robuster als LSTM)
â”‚
â”œâ”€ Ist sie SYMMETRISCH (auf/ab gleich)?
â”‚  â””â”€ Ja (z.B. Solar) â†’ Bi-LSTM (0.9955)
â”‚
â”œâ”€ Ist sie UNIDIREKTIONAL sequenziell?
â”‚  â””â”€ Ja (z.B. Consumption) â†’ GRU (0.9874, 2x schneller als Bi-LSTM)
â”‚
â”œâ”€ Ist sie VOLATIL (CV &gt; 0.7)?
â”‚  â””â”€ Ja (z.B. Price) â†’ LightGBM (0.9798, DL versagt!)
â”‚
â”œâ”€ Ist sie CHAOTISCH (ACF&lt;0.3)?
â”‚  â””â”€ Ja (z.B. Wind) â†’ Random Forest (0.9997, DL -4.7%)
â”‚
â””â”€ NIEMALS N-BEATS/N-HiTS nutzen!
   â†’ Bei uns IMMER negativ (-100 bis -18)
</code></pre></div>
        <div class="slide"><h2>Slide 10: Energiemarkt-Dynamik - Was treibt was?</h2>
<h3>ğŸ’¡ Die Ã¶konomische Perspektive: Granger Causality zeigt Marktmechanismen</h3>
<p><strong>Alle 12 Kombinationen signifikant (p &lt; 0.0001)</strong> - Was bedeutet das wirtschaftlich?</p>
<h3>ğŸŒ <strong>Solar â†’ Price (F=847.3, stÃ¤rkster Effekt!)</strong></h3>
<p><strong>Merit Order Effekt in Aktion:</strong><br />
- Sonniger Tag â†’ 40.000 MW Solar ins Netz<br />
- Solar hat Grenzkosten ~0 EUR/MWh â†’ verdrÃ¤ngt teure Gaskraftwerke<br />
- <strong>Preis fÃ¤llt von 150 auf 50 EUR/MWh</strong></p>
<p><strong>Real-World Impact:</strong><br />
- An sonnigen Sommertagen: Negative Preise mÃ¶glich (827 FÃ¤lle!)<br />
- <strong>Aber:</strong> Prognose schwierig, weil non-linear (Schwellenwert-Effekt)</p>
<h3>âš¡ <strong>Price â†’ Consumption (F=234.5)</strong></h3>
<p><strong>Demand Response - Die Marktreaktion:</strong><br />
- Hoher Preis (&gt;200 EUR/MWh) â†’ Industrie schaltet ab<br />
- Niedriger Preis (&lt;50 EUR/MWh) â†’ ZusÃ¤tzliche Nachfrage</p>
<p><strong>Beispiel Aluminium-Schmelze:</strong><br />
- Flexibler Stromverbrauch 500 MW<br />
- Bei Price &gt; 180 EUR/MWh: Produktion runter â†’ <strong>Consumption sinkt</strong><br />
- Bei Price &lt; 60 EUR/MWh: Produktion hoch â†’ <strong>Consumption steigt</strong></p>
<p><strong>Korrelation:</strong> -0.23 (negativ!) â†’ Hoher Preis drÃ¼ckt Nachfrage</p>
<h3>ğŸ­ <strong>Solar â†‘ â†’ Consumption â†‘ (F=156.2)</strong></h3>
<p><strong>Warum steigt Konsum bei hoher Solar-Einspeisung?</strong></p>
<p><strong>Hypothese 1: Preissignal</strong><br />
- Solar â†‘ â†’ Preis â†“ â†’ Consumption â†‘ (Ã¼ber Price als Mediator)<br />
- <strong>Indirekte KausalitÃ¤t:</strong> Solar â†’ Price â†’ Consumption</p>
<p><strong>Hypothese 2: Tageszeit-Effekt</strong><br />
- Solar peak = 12-14 Uhr<br />
- Industrielle Spitze = 10-16 Uhr<br />
- <strong>Scheinkorrelation:</strong> Beide folgen Tagesrhythmus</p>
<p><strong>Hypothese 3: Smart Grid Response</strong><br />
- Intelligente Verbraucher (WÃ¤rmepumpen, E-Autos)<br />
- Laden automatisch bei hoher Renewable-Einspeisung<br />
- <strong>Reale KausalitÃ¤t:</strong> Solar-Forecast â†’ Consumption-Planung</p>
<p><strong>Test mit VAR:</strong> Solar â†’ Consumption ist signifikant (auch nach Kontrolle fÃ¼r Tageszeit)<br />
â†’ <strong>Hybride ErklÃ¤rung:</strong> Preissignal + Tageszeit + Smart Response</p>
<h3>ğŸ’¨ <strong>Wind â†” Price (Bidirektional, F=298.7)</strong></h3>
<p><strong>Komplexe Wechselwirkung:</strong></p>
<p><strong>Wind â†’ Price:</strong><br />
- Windreiche Nacht â†’ 20.000 MW Offshore â†’ Ãœberangebot<br />
- <strong>Preis kann negativ werden</strong> (-500 EUR/MWh Maximum)</p>
<p><strong>Price â†’ Wind (???):</strong> <br />
- <strong>Scheinbar paradox:</strong> Wie kann Preis Wind beeinflussen?<br />
- <strong>ErklÃ¤rung:</strong> Curtailment (Abregelung)<br />
  - Bei Preis &lt; -50 EUR/MWh: Windparks werden abgeschaltet<br />
  - <strong>Gemessene Wind-Einspeisung sinkt</strong>, obwohl Wind physisch stark ist<br />
  - â†’ Ã–konomische Entscheidung, nicht meteorologisch!</p>
<p><strong>Lesson:</strong> Granger-KausalitÃ¤t â‰  physikalische KausalitÃ¤t!</p>
<h3>ğŸ”— <strong>Kointegration: Langfristige Gleichgewichte</strong></h3>
<p><strong>4 Kointegrationsvektoren gefunden</strong> â†’ Was bedeutet das?</p>
<p><strong>Vereinfachtes Beispiel:</strong></p>
<pre><code>Langfristiger Zusammenhang:
Price = 100 + 0.5 * Consumption - 2 * Solar - 1.5 * Wind

Interpretation:
- 1000 MW mehr Consumption â†’ +0.5 EUR/MWh
- 1000 MW mehr Solar â†’ -2 EUR/MWh (Merit Order!)
- 1000 MW mehr Wind â†’ -1.5 EUR/MWh
</code></pre>
<p><strong>Was sagt uns das?</strong><br />
- Kurzfristig: Preise schwanken wild (Spikes, VolatilitÃ¤t)<br />
- Langfristig: Es gibt Gleichgewichte (Regression to Mean)<br />
- <strong>Praktisch:</strong> FÃ¼r Day-Ahead-Forecasts (24h) â†’ Kointegration hilft wenig</p>
<h3>ğŸ“Š VAR-Modell: Kann man KausalitÃ¤t nutzen?</h3>
<p><strong>ErnÃ¼chternde Ergebnisse:</strong></p>
<table>
<thead>
<tr>
<th>Zeitreihe</th>
<th>Univariat (Best)</th>
<th>VAR (Multivariat)</th>
<th>Delta</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Price</strong></td>
<td>0.9798 (LightGBM)</td>
<td>0.15</td>
<td><strong>-98%!</strong> âŒ</td>
</tr>
<tr>
<td><strong>Solar</strong></td>
<td>0.9955 (Bi-LSTM)</td>
<td>0.63</td>
<td>-53%</td>
</tr>
<tr>
<td><strong>Consumption</strong></td>
<td>0.9874 (GRU)</td>
<td>0.59</td>
<td>-67%</td>
</tr>
</tbody>
</table>
<p><strong>Warum hilft KausalitÃ¤t nicht beim Forecasting?</strong></p>
<p><strong>1. VAR ist linear, MÃ¤rkte sind nicht-linear</strong><br />
- Merit Order: Stufen-Funktion, keine Gerade<br />
- Curtailment: Schwellenwert-Effekt bei negativen Preisen<br />
- VAR erfasst das nicht!</p>
<p><strong>2. Lag 24 zu lang fÃ¼r kurzfristige Dynamik</strong><br />
- Price-Spikes entstehen in Minuten<br />
- VAR mit 24h-Lag ist zu trÃ¤ge<br />
- Braucht kÃ¼rzere Lags (1-3h), aber dann fehlt SaisonalitÃ¤t</p>
<p><strong>3. Fehlende exogene Faktoren</strong><br />
- Wetter (dominant fÃ¼r Solar/Wind!)<br />
- Marktevents (z.B. KraftwerksausfÃ¤lle)<br />
- Policy (z.B. CO2-Preis-Ã„nderungen)</p>
<p><strong>Kritischer Insight:</strong><br />
- <strong>Granger-KausalitÃ¤t ist DESKRIPTIV</strong> (zeigt ZusammenhÃ¤nge)<br />
- <strong>Aber nicht PRÃ„DIKTIV</strong> (hilft nicht beim Forecasting)<br />
- Univariate Modelle mit guten Features (lag_1, diff_1, hour) schlagen VAR</p>
<h3>ğŸ¯ Praktische Implikationen fÃ¼r Energy Trading</h3>
<p><strong>Was haben wir gelernt?</strong></p>
<p><strong>1. Merit Order funktioniert!</strong><br />
- Solar/Wind hoch â†’ Price runter (F=847.3)<br />
- FÃ¼r Trader: Monitor Solar-Forecast fÃ¼r Price-Prognose</p>
<p><strong>2. Demand Response ist real</strong><br />
- Price hoch â†’ Consumption runter (F=234.5)<br />
- FÃ¼r Grid Operators: Preissignale steuern Nachfrage</p>
<p><strong>3. Curtailment ist Ã¶konomisch, nicht physisch</strong><br />
- Price negativ â†’ Wind "sinkt" (Abregelung)<br />
- FÃ¼r Policy: Speicher-Incentives reduzieren Curtailment</p>
<p><strong>4. VAR ist nicht die LÃ¶sung</strong><br />
- Non-Linearity, fehlende Exogene<br />
- <strong>Besser:</strong> Univariate ML/DL + exogene Features<br />
- <strong>Alternativ:</strong> ML-basierte Multivariate (XGBoost mit Cross-Series-Lags)</p>
<p><strong>5. Kointegration zeigt langfristige Trends</strong><br />
- FÃ¼r strategische Planung (Investitionen)<br />
- Nicht fÃ¼r operatives Forecasting (Day-Ahead)</p>
<p><strong>Key Takeaway:</strong><br />
KausalitÃ¤t verstehen â†’ bessere Features bauen â†’ bessere univariate Modelle!<br />
Nicht: KausalitÃ¤t â†’ VAR â†’ schlechte Forecasts</p></div>
        <div class="slide"><h1>TEIL 3: KRITISCHE DISKUSSION &amp; LESSONS LEARNED</h1></div>
        <div class="slide"><h2>Slide 11: Lessons Learned fÃ¼r Advanced Time Series</h2>
<h3>ğŸ“ Was haben wir aus 5 Zeitreihen gelernt?</h3>
<h4>1. <strong>Data Quality beats Fancy Models</strong></h4>
<ul>
<li>Wind Offshore: RÂ² von -36.4 auf ~0.85 nur durch Data Cleaning</li>
<li>9.6-Monate Stillstand â†’ 37.9% Datenverlust</li>
<li>â†’ <strong>Invest more in EDA than Model Tuning!</strong></li>
</ul>
<h4>2. <strong>Deep Learning ist NICHT universell - 5 Archetypen validiert!</strong> ğŸ­</h4>
<ul>
<li><strong>Solar (Archetyp 1):</strong> Bi-LSTM 0.9955 &gt; LightGBM 0.9838 (+1.2%) âœ…</li>
<li><strong>Consumption (Archetyp 2):</strong> GRU 0.9874 &gt; LightGBM 0.95 (+3.7%) âœ…âœ…</li>
<li><strong>Wind Onshore (Archetyp 3):</strong> LSTM 0.9548 &lt;&lt; RF 0.9997 (-4.7%) âŒ</li>
<li><strong>Price (Archetyp 4):</strong> GRU 0.8906 &lt;&lt; LightGBM 0.9798 (-9%) âŒâŒ</li>
<li><strong>Wind Offshore (Archetyp 5):</strong> GRU 0.33 &lt;&lt; XGBoost ~0.85 (-61%) âŒâŒâŒ</li>
<li>â†’ <strong>Pattern erkannt: Je schwÃ¤cher ML, desto mehr hilft DL!</strong></li>
</ul>
<h4>3. <strong>GRU ist der unterschÃ¤tzte Champion!</strong> ğŸ†• ğŸ†</h4>
<ul>
<li><strong>Consumption:</strong> GRU 0.9874 &gt; Bi-LSTM 0.9799 (+0.75%, 2x schneller)</li>
<li><strong>Price:</strong> GRU 0.8906 &gt; Bi-LSTM 0.8853 (+0.53%, 7x schneller)</li>
<li><strong>Wind Offshore:</strong> GRU 0.33 &gt; LSTM 0.08 (+328%!)</li>
<li>Einfacher (2 Gates), schneller, robuster bei VolatilitÃ¤t</li>
<li>â†’ <strong>Probiere GRU BEVOR du zu Bi-LSTM greifst!</strong></li>
</ul>
<h4>4. <strong>Random Forest: Der stille Gewinner bei Chaos</strong></h4>
<ul>
<li>Wind Onshore: RÂ²=0.9997 (besser als JEDES DL-Modell)</li>
<li>Robust gegen StochastizitÃ¤t, kein GPU nÃ¶tig</li>
<li>â†’ <strong>Bei ACF &lt; 0.3: RF als First Choice!</strong></li>
</ul>
<h4>5. <strong>"State-of-the-Art" versagt konsistent bei Energy Data</strong> âŒ</h4>
<ul>
<li><strong>N-BEATS:</strong> -18.93 (Solar), -0.94 (Cons), -4.63 (Wind On), -3.16 (Price), <strong>-12.49 (Wind Off)</strong></li>
<li><strong>N-HiTS:</strong> -4.22 (Solar), -9.58 (Cons), -1.02Ã—10Â²â°Â¹ (Wind On), -3.74 (Price), <strong>-100.41 (Wind Off)</strong></li>
<li><strong>DeepAR:</strong> -1.24 (Cons), -1.03 (Wind On), -1.16 (Price), <strong>-7.11 (Wind Off)</strong></li>
<li><strong>5/5 Zeitreihen:</strong> Alle SOTA-Modelle negativ!</li>
<li>â†’ <strong>SOTA â‰  Production-Ready! Immer selbst benchmarken!</strong></li>
</ul>
<h4>6. <strong>BidirektionalitÃ¤t hilft nur bei Symmetrie</strong></h4>
<ul>
<li>Solar (symmetrisch): Bi-LSTM &gt; GRU (+1.4%)</li>
<li>Consumption (sequenziell): GRU &gt; Bi-LSTM (+0.75%)</li>
<li>Wind Offshore (fragmentiert): GRU &gt; Bi-LSTM (+55%!)</li>
<li>â†’ <strong>Pattern-Typ bestimmt Architektur-Wahl!</strong></li>
</ul>
<h4>7. <strong>DL-ROI korreliert negativ mit ML-Performance</strong></h4>
<ul>
<li>Consumption: ML schwach (0.95) â†’ DL Vorteil groÃŸ (+3.7%)</li>
<li>Solar: ML stark (0.9838) â†’ DL Vorteil klein (+1.2%)</li>
<li>Price: ML perfekt (0.9798) â†’ DL versagt (-9%)</li>
<li>â†’ <strong>Wenn ML schon gut ist, bringt DL wenig!</strong></li>
</ul>
<h4>8. <strong>StrukturbrÃ¼che brauchen separate Behandlung</strong></h4>
<ul>
<li>Wind Offshore: Outage-Periode zerstÃ¶rt Training</li>
<li>LÃ¶sung: Binary Classifier ("lÃ¤uft?") + Regressor ("wie viel?")</li>
<li>â†’ <strong>Domain Knowledge &gt; Algorithmen!</strong></li>
</ul>
<h4>9. <strong>Training Zeit â‰  Performance</strong></h4>
<ul>
<li>N-BEATS: 733s â†’ RÂ²=-12.49 âŒ</li>
<li>GRU: 13s â†’ RÂ²=0.33 âœ… (56x schneller!)</li>
<li>â†’ <strong>Schnell iterieren &gt; langsames "Perfect Model"!</strong></li>
</ul>
<h4>10. <strong>Exogene Features sind kritisch bei Renewables</strong></h4>
<ul>
<li>Wind Offshore RÂ²=0.33 ohne Windgeschwindigkeit</li>
<li>Erwartung: RÂ²~0.90+ mit Weather-APIs</li>
<li>â†’ <strong>Investiere in Data Sourcing!</strong></li>
</ul>
<h3>ğŸ”® NÃ¤chste Schritte</h3>
<ol>
<li>âœ… <strong>Alle 5 Zeitreihen getestet</strong> - DL-Archetypen validiert!</li>
<li>ğŸ¯ <strong>GRU-First Strategy</strong> - GRU als Default fÃ¼r neue Zeitreihen</li>
<li>ğŸ”„ <strong>Ensemble:</strong> GRU + LightGBM (temporal + features)</li>
<li>ğŸ“Š <strong>ACF-Based Routing:</strong> Automatische Modellwahl</li>
<li>ğŸŒ <strong>Exogene Features:</strong> Wetter-APIs integrieren (Wind, Solar-Irradiance)</li>
<li>ğŸ­ <strong>Production:</strong> Binary Classifier + Regressor fÃ¼r Wind Offshore</li>
<li>ğŸ”§ <strong>SOTA-Debug:</strong> Kann man N-BEATS/N-HiTS retten? (evtl. nicht lohnend)</li>
</ol>
<h3>ğŸ’¡ Open Questions fÃ¼r Diskussion</h3>
<ol>
<li><strong>Warum ist GRU so viel besser als Bi-LSTM bei fragmentierten Daten?</strong></li>
<li>Wind Offshore: +328%! (0.33 vs 0.08)</li>
<li>
<p>Einfachheit = Robustheit?</p>
</li>
<li>
<p><strong>Warum versagen SOTA-Modelle SO konsistent?</strong></p>
</li>
<li>5/5 Zeitreihen negativ</li>
<li>Univariate Optimierung vs Feature-Rich Energy Data?</li>
<li>
<p>Fundamental falsch fÃ¼r Energy?</p>
</li>
<li>
<p><strong>Kann man Wind Offshore auf 0.85+ bringen?</strong></p>
</li>
<li>Exogene Features (Windgeschwindigkeit, Richtung)?</li>
<li>Separate Outage-Prediction?</li>
<li>
<p>Hybrid-Modell (Binary + Regressor)?</p>
</li>
<li>
<p><strong>GRU + LightGBM Ensemble = 0.99+?</strong></p>
</li>
<li>GRU lernt temporal (0.9874)</li>
<li>LightGBM lernt features (0.95)</li>
<li>
<p>Unterschiedliche Fehler â†’ Kombination?</p>
</li>
<li>
<p><strong>Transfer Learning zwischen Archetypen?</strong></p>
</li>
<li>Solar-Bi-LSTM â†’ andere PV? âœ…</li>
<li>Consumption-GRU â†’ andere LÃ¤nder? âœ…</li>
<li>Zwischen Archetypen? âŒ (zu unterschiedlich)</li>
</ol></div>
        <div class="slide"><h2>ğŸ“š Referenzen &amp; Quellen</h2>
<ol>
<li><strong>Daten:</strong> SMARD.de, ENTSO-E Transparency Platform, EPEX Spot</li>
<li><strong>Frameworks:</strong> scikit-learn, XGBoost, LightGBM, TensorFlow/Keras</li>
<li><strong>Literatur:</strong></li>
<li>Hyndman &amp; Athanasopoulos (2021): "Forecasting: Principles and Practice"</li>
<li>Hochreiter &amp; Schmidhuber (1997): "Long Short-Term Memory"</li>
<li>Ke et al. (2017): "LightGBM"</li>
</ol></div>
        <div class="slide"><h1>ğŸ¤ DANKE FÃœR IHRE AUFMERKSAMKEIT!</h1>
<p><strong>Fragen? Diskussion?</strong></p>
<p><strong>Key Takeaway:</strong> 5 Zeitreihen â†’ 5 Archetypen â†’ Keine UniversallÃ¶sung!<br />
<strong>Praktischer Rat:</strong> Teste GRU, LightGBM, Random Forest in dieser Reihenfolge.<br />
<strong>Wichtigste Lektion:</strong> Data Quality &gt; Model Complexity (Wind Offshore +36.4 RÂ² durch Cleaning!)</p></div>
    </div>
    
    <div class="controls">
        <button onclick="prevSlide()" title="Vorherige Folie (â†)">â—€ ZurÃ¼ck</button>
        <button onclick="nextSlide()" title="NÃ¤chste Folie (â†’)">Weiter â–¶</button>
    </div>
    
    <div class="slide-number">
        <span id="current">1</span> / <span id="total">18</span>
    </div>
    
    <script>
        let currentSlide = 0;
        const slides = document.querySelectorAll('.slide');
        const totalSlides = slides.length;
        
        document.getElementById('total').textContent = totalSlides;
        updateProgress();
        
        function showSlide(n) {
            slides[currentSlide].classList.remove('active');
            currentSlide = (n + totalSlides) % totalSlides;
            slides[currentSlide].classList.add('active');
            document.getElementById('current').textContent = currentSlide + 1;
            updateProgress();
        }
        
        function updateProgress() {
            const progress = ((currentSlide + 1) / totalSlides) * 100;
            document.getElementById('progress').style.width = progress + '%';
        }
        
        function nextSlide() { showSlide(currentSlide + 1); }
        function prevSlide() { showSlide(currentSlide - 1); }
        
        // Keyboard navigation
        document.addEventListener('keydown', (e) => {
            if (e.key === 'ArrowRight' || e.key === ' ' || e.key === 'Enter') {
                e.preventDefault();
                nextSlide();
            }
            if (e.key === 'ArrowLeft' || e.key === 'Backspace') {
                e.preventDefault();
                prevSlide();
            }
            if (e.key === 'Home') {
                e.preventDefault();
                showSlide(0);
            }
            if (e.key === 'End') {
                e.preventDefault();
                showSlide(totalSlides - 1);
            }
        });
        
        // Touch/Swipe support
        let touchStartX = 0;
        let touchEndX = 0;
        
        document.addEventListener('touchstart', e => {
            touchStartX = e.changedTouches[0].screenX;
        });
        
        document.addEventListener('touchend', e => {
            touchEndX = e.changedTouches[0].screenX;
            handleSwipe();
        });
        
        function handleSwipe() {
            if (touchEndX < touchStartX - 50) nextSlide();
            if (touchEndX > touchStartX + 50) prevSlide();
        }
        
        // Show first slide
        showSlide(0);
    </script>
</body>
</html>