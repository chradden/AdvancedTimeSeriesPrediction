<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Advanced Time Series Forecasting - Presentation</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            overflow: hidden;
        }
        .slide {
            display: none;
            width: 100vw;
            height: 100vh;
            padding: 50px 80px;
            background: white;
            overflow-y: auto;
        }
        .slide.active { display: block; }
        
        /* Typography */
        .slide h1 { 
            color: #667eea; 
            font-size: 2.5em; 
            margin-bottom: 25px;
            border-bottom: 4px solid #764ba2;
            padding-bottom: 15px;
            font-weight: 700;
        }
        .slide h2 { 
            color: #764ba2; 
            font-size: 2em; 
            margin: 25px 0 15px;
            font-weight: 600;
        }
        .slide h3 { 
            color: #667eea; 
            font-size: 1.5em; 
            margin: 20px 0 10px;
            font-weight: 600;
        }
        .slide h4 {
            color: #555;
            font-size: 1.2em;
            margin: 15px 0 10px;
            font-weight: 600;
        }
        
        /* Tables */
        .slide table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            box-shadow: 0 4px 10px rgba(0,0,0,0.1);
            font-size: 0.9em;
        }
        .slide th, .slide td {
            padding: 12px 15px;
            text-align: left;
            border-bottom: 1px solid #e0e0e0;
        }
        .slide th {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            font-weight: 600;
            text-transform: uppercase;
            font-size: 0.85em;
            letter-spacing: 0.5px;
        }
        .slide tbody tr:hover { 
            background: #f8f9fa;
            transition: background 0.2s;
        }
        .slide tbody tr:nth-child(even) {
            background: #f9f9f9;
        }
        
        /* Code */
        .slide code {
            background: #f4f4f4;
            padding: 3px 8px;
            border-radius: 4px;
            font-family: 'SF Mono', Monaco, 'Courier New', monospace;
            font-size: 0.9em;
            color: #d63384;
        }
        .slide pre {
            background: #2d2d2d;
            color: #f8f8f2;
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            margin: 20px 0;
            font-size: 0.85em;
            line-height: 1.5;
            box-shadow: 0 4px 10px rgba(0,0,0,0.2);
        }
        .slide pre code {
            background: none;
            color: inherit;
            padding: 0;
        }
        
        /* Lists */
        .slide ul, .slide ol {
            margin-left: 40px;
            line-height: 1.8;
            margin-bottom: 20px;
        }
        .slide li {
            margin-bottom: 10px;
        }
        .slide li::marker {
            color: #667eea;
            font-weight: bold;
        }
        
        /* Text formatting */
        .slide strong { 
            color: #764ba2; 
            font-weight: 600;
        }
        .slide em {
            color: #667eea;
            font-style: italic;
        }
        .slide p {
            margin-bottom: 15px;
            line-height: 1.7;
            color: #333;
        }
        
        /* Images */
        .slide img {
            max-width: 100%;
            height: auto;
            border-radius: 10px;
            box-shadow: 0 8px 20px rgba(0,0,0,0.15);
            margin: 25px 0;
        }
        
        /* Blockquotes */
        .slide blockquote {
            border-left: 5px solid #667eea;
            padding-left: 20px;
            margin: 25px 0;
            color: #555;
            font-style: italic;
            background: #f8f9fa;
            padding: 15px 20px;
            border-radius: 0 8px 8px 0;
        }
        
        /* Navigation Controls */
        .controls {
            position: fixed;
            bottom: 40px;
            right: 40px;
            display: flex;
            gap: 15px;
            z-index: 1000;
        }
        .controls button {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border: none;
            padding: 15px 30px;
            font-size: 16px;
            border-radius: 50px;
            cursor: pointer;
            box-shadow: 0 4px 20px rgba(102, 126, 234, 0.4);
            transition: all 0.3s ease;
            font-weight: 600;
        }
        .controls button:hover {
            transform: translateY(-3px);
            box-shadow: 0 6px 30px rgba(118, 75, 162, 0.6);
        }
        .controls button:active {
            transform: translateY(-1px);
        }
        
        /* Slide Counter */
        .slide-number {
            position: fixed;
            bottom: 105px;
            right: 40px;
            background: rgba(0,0,0,0.8);
            color: white;
            padding: 12px 24px;
            border-radius: 30px;
            font-size: 16px;
            font-weight: 600;
            box-shadow: 0 4px 15px rgba(0,0,0,0.3);
        }
        
        /* Progress Bar */
        .progress-bar {
            position: fixed;
            top: 0;
            left: 0;
            width: 0%;
            height: 4px;
            background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
            transition: width 0.3s ease;
            z-index: 1001;
        }
        
        /* Responsive */
        @media (max-width: 1024px) {
            .slide {
                padding: 30px 40px;
                font-size: 0.9em;
            }
            .slide h1 { font-size: 2em; }
            .slide h2 { font-size: 1.6em; }
            .slide table { font-size: 0.8em; }
        }
    </style>
</head>
<body>
    <div class="progress-bar" id="progress"></div>
    
    <div id="slides">
        <div class="slide"><h1>ğŸ“ Advanced Time Series Forecasting for energy Markets</h1>
<h2>A Critical Comparison of ML, DL, and Statisical Methods</h2>
<p><strong>Duration:</strong> 20 Minutes<br />
<strong>Audience:</strong> Advanced Time Series Analysis Course<br />
<strong>Date:</strong> February 2026</p></div>
        <div class="slide"><h2>ğŸ“‹ Agenda (20 Min)</h2>
<ol>
<li><strong>Data Basis &amp; Preprocessing</strong> (4 Min) - Slides 1-3</li>
<li><strong>Model Performance by Time Series</strong> (10 Min) - Slides 4-8</li>
<li><strong>Critical Discussion &amp; Lessons Learned</strong> (5 Min) - Slides 9-10</li>
<li><strong>Q&amp;A</strong> (1 Min)</li>
</ol></div>
        <div class="slide"><h1>PART 1: DATA BASIS &amp; PREPROCESSING</h1></div>
        <div class="slide"><h2>Slide 1: Data Basis - German energy Markets 2022-2024</h2>
<h3>ğŸ“Š Five Time Series, Hourly Resolution</h3>
<table>
<thead>
<tr>
<th>Time Series</th>
<th>Data Points</th>
<th>Period</th>
<th>Source</th>
<th>Unit</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Solar</strong></td>
<td>26.257</td>
<td>2022-2024</td>
<td>SMARD/ENTSO-E</td>
<td>MW</td>
</tr>
<tr>
<td><strong>Wind Offshore</strong></td>
<td>26.257</td>
<td>2022-2024</td>
<td>SMARD/ENTSO-E</td>
<td>MW</td>
</tr>
<tr>
<td><strong>Wind Onshore</strong></td>
<td>26.257</td>
<td>2022-2024</td>
<td>SMARD/ENTSO-E</td>
<td>MW</td>
</tr>
<tr>
<td><strong>Consumption</strong></td>
<td>26.257</td>
<td>2022-2024</td>
<td>SMARD/ENTSO-E</td>
<td>MW</td>
</tr>
<tr>
<td><strong>Price (Day-ahead)</strong></td>
<td>26.257</td>
<td>2022-2024</td>
<td>EPEX Spot</td>
<td>EUR/MWh</td>
</tr>
</tbody>
</table>
<h3>ğŸ“ˆ Time Series Overview</h3>
<p><img alt="All Time Seriesn" src="figures/all_timeseries_overview.png" /></p>
<h3>ğŸ¯ Challenges</h3>
<ul>
<li><strong>High Volatility:</strong> CV from 0.31 (Solar) to 0.85 (Price)</li>
<li><strong>Seasonality:</strong> Multiple Patterns (tÃ¤glich, wÃ¶chentlich, jÃ¤hrlich)</li>
<li><strong>Structural Breaks:</strong> Wind Offshore Outage (Apr 2023 - Feb 2024, 9.8 Months!)</li>
<li><strong>Negative Prices:</strong> 827 Cases (3.15%) - Oversupply-Situationen</li>
<li><strong>missing Data:</strong> Wind Onshore had data gaps</li>
<li><strong>Nicht-Stationarity:</strong> All Time Seriesn nicht-stationÃ¤r (KPSS Test p&lt;0.01)</li>
</ul></div>
        <div class="slide"><h2>Slide 2: Preprocessing Pipeline - From Raw Data to 31 Features</h2>
<h3>ğŸ”§ Critical Processing Steps</h3>
<h4>1. <strong>Data Cleaning</strong></h4>
<pre><code class="language-python"># missing Data Detection
missing_rate = df.isna().sum() / len(df)

# Interpolation for individual Gaps (&lt;24h)
df_cleaned = df.interpolate(method='time', limit=24)

# Outlier Detection (3-Sigma-Regel + Domain-Wissen)
# Solar: Kann nie negative sein
# Wind: MaximalkapazitÃ¤t checken
</code></pre>
<h4>2. <strong>Feature Engineering</strong> (31 Features pro Time Series)</h4>
<p><strong>Categories:</strong><br />
1. <strong>Lags</strong> (6 Features): <code>lag_1</code>, <code>lag_2</code>, <code>lag_3</code>, <code>lag_24</code>, <code>lag_168</code>, <code>lag_720</code><br />
2. <strong>Rolling Statisics</strong> (9 Features):<br />
   - <code>rolling_mean_3</code>, <code>rolling_mean_24</code>, <code>rolling_mean_168</code><br />
   - <code>rolling_std_3</code>, <code>rolling_std_24</code>, <code>rolling_std_168</code><br />
   - <code>rolling_min_24</code>, <code>rolling_max_24</code>, <code>rolling_median_24</code><br />
3. <strong>Differences</strong> (4 Features): <code>diff_1</code>, <code>diff_24</code>, <code>diff_168</code>, <code>diff_720</code><br />
4. <strong>Temporal Features</strong> (7 Features):<br />
   - <code>hour</code>, <code>day_of_week</code>, <code>month</code>, <code>quarter</code><br />
   - <code>is_weekend</code>, <code>is_holiday</code>, <code>day_of_year</code><br />
5. <strong>Momentum</strong> (3 Features): <code>momentum_3h</code>, <code>momentum_24h</code>, <code>momentum_168h</code><br />
6. <strong>Volatility</strong> (2 Features): <code>volatility_24h</code>, <code>volatility_168h</code></p>
<p><strong>Why so many?</strong><br />
- ML-models (XGBoost, LightGBM) benefit massively from Features<br />
- Feature Importance shows: Top 3 Features = 60-80% of performance!<br />
- LSTM uses only raw data, but Feature-Augmentation hilft here too</p>
<h4>3. <strong>Train/Val/Test Split</strong></h4>
<pre><code class="language-python"># Temporal Split (KEINE Random-Shuffle bei Time Seriesn!)
train: 2022-01-01 to 2023-06-30  (60%)
val:   2023-07-01 to 2023-12-31  (20%)
test:  2024-01-01 to 2024-12-31  (20%)
</code></pre>
<p><strong>Important:</strong> Walk-Forward Validation for Production-Deployment!</p></div>
        <div class="slide"><h2>Slide 3: Model Portfolio - 15 Models in Benchmark</h2>
<h3>ğŸ¯ Testd Model Architectures</h3>
<p>We tested <strong>15 different models</strong> via <strong>5 Time Seriesn</strong> tested (= 75 experiments!)</p>
<h3>ğŸ“Š Model Categories</h3>
<h4>1ï¸âƒ£ <strong>Statisical Baseline Models</strong> (Univariatee, Simple)</h4>
<table>
<thead>
<tr>
<th>Model</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Naive</strong></td>
<td>Last Value</td>
<td>Last observation carried forward</td>
</tr>
<tr>
<td><strong>Seasonal Naive</strong></td>
<td>Seasonal Last Value</td>
<td>Last seasonal cycle (24h) is repeated</td>
</tr>
<tr>
<td><strong>Mean</strong></td>
<td>Hisorical Average</td>
<td>Mean of training data</td>
</tr>
<tr>
<td><strong>SARIMA</strong></td>
<td>Seasonal ARIMA</td>
<td>Stationarity, Linearity, Seasonality</td>
</tr>
</tbody>
</table>
<p><strong>Purpose:</strong> Simplest baselines - Show how much complexity brings</p>
<h4>2ï¸âƒ£ <strong>Machine Learning Tree Models</strong> (Standard Python Pipeline)</h4>
<table>
<thead>
<tr>
<th>Model</th>
<th>Type</th>
<th>Training Environment</th>
<th>Strengths</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>XGBoost</strong></td>
<td>Gradient Boosting</td>
<td>Local (CPU)</td>
<td>Feature-rich, robust</td>
</tr>
<tr>
<td><strong>LightGBM</strong></td>
<td>Gradient Boosting</td>
<td>Local (CPU)</td>
<td>Fast, memory-efficient</td>
</tr>
<tr>
<td><strong>Random Forest</strong></td>
<td>Ensemble</td>
<td>Local (CPU)</td>
<td>Chaos-resisant, no hyperparameters</td>
</tr>
<tr>
<td><strong>CatBoost</strong></td>
<td>Gradient Boosting</td>
<td>Local (CPU)</td>
<td>Categorical features</td>
</tr>
</tbody>
</table>
<p><strong>Features:</strong> 31 engineered features (lags, rolling stats, temporal)</p>
<h4>3ï¸âƒ£ <strong>Deep Learning Models - Standard</strong> (Extended Testing Colab GPU T4)</h4>
<table>
<thead>
<tr>
<th>Model</th>
<th>Architecture</th>
<th>Parameters</th>
<th>Training time</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>LSTM</strong></td>
<td>Recurrent</td>
<td>~50K</td>
<td>20-30s</td>
<td>Sequences</td>
</tr>
<tr>
<td><strong>GRU</strong></td>
<td>Recurrent (simplified)</td>
<td>~35K</td>
<td>15-25s</td>
<td>Unidirectional, faster</td>
</tr>
<tr>
<td><strong>Bi-LSTM</strong></td>
<td>Bidirectional</td>
<td>~100K</td>
<td>30-60s</td>
<td>Symmetric Patterns</td>
</tr>
</tbody>
</table>
<h4>4ï¸âƒ£ <strong>Deep Learning Models - Generative</strong> (Extended Testing Colab GPU T4)</h4>
<table>
<thead>
<tr>
<th>Model</th>
<th>Type</th>
<th>Parameters</th>
<th>Training time</th>
<th>Complexity</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Autoencoder</strong></td>
<td>Encoder-Decoder</td>
<td>~80K</td>
<td>40-80s</td>
<td>Feature Learning</td>
</tr>
<tr>
<td><strong>VAE</strong></td>
<td>Variational</td>
<td>~100K</td>
<td>60-190s</td>
<td>Probabilisic</td>
</tr>
</tbody>
</table>
<h4>5ï¸âƒ£ <strong>Deep Learning Models - State-of-the-Art</strong> (Extended Testing Colab GPU T4)</h4>
<table>
<thead>
<tr>
<th>Model</th>
<th>Paper</th>
<th>Parameters</th>
<th>Training time</th>
<th>Specialization</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>N-BEATS</strong></td>
<td>2020 (Oreshkin et al.)</td>
<td>~200K</td>
<td>700-2000s</td>
<td>Univariatee Decomposition</td>
</tr>
<tr>
<td><strong>N-HiTS</strong></td>
<td>2022 (Challu et al.)</td>
<td>~180K</td>
<td>100-350s</td>
<td>Hierarchical Interpolation</td>
</tr>
<tr>
<td><strong>DeepAR</strong></td>
<td>2017 (Amazon)</td>
<td>~120K</td>
<td>100-370s</td>
<td>Probabilisic Forecasting</td>
</tr>
</tbody>
</table>
<p><strong>Expectation:</strong> SOTA models should win â†’ <strong>Actually:</strong> All negativee! âŒ</p>
<h4>6ï¸âƒ£ <strong>Multivariatee Models</strong> (Klassische Time Seriesnanalyse)</h4>
<table>
<thead>
<tr>
<th>Model</th>
<th>Type</th>
<th>Assumptions</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>VAR</strong></td>
<td>Vector Autoregression</td>
<td>Linearity, Lag structure</td>
</tr>
<tr>
<td><strong>VECM</strong></td>
<td>Vector Error Correction</td>
<td>Cointegration, Long-term equilibria</td>
</tr>
</tbody>
</table>
<p><strong>Purpose:</strong> Nutzen KausalitÃ¤t zwischen Time Seriesn - <strong>Result:</strong> Worse than univariate!</p>
<h3>ğŸ­ Key Insights</h3>
<ol>
<li><strong>SOTA â‰  Best Performance</strong> - N-BEATS/N-HiTS: All 5 Time Seriesn negative (RÂ² from -100 to -18!)</li>
<li><strong>GPU â‰  Better Results</strong> - Random Forest (CPU, 50s) beats N-BEATS (GPU, 2000s)</li>
<li><strong>Complexity â‰  Accuracy</strong> - GRU (35K Parameters) &gt; Bi-LSTM (100K Parameters) bei 3/5 Time Seriesn</li>
<li><strong>Training Time Paradox</strong> - Fastest models (GRU ~15s) often better than slowest (N-BEATS ~2000s)</li>
</ol>
<p><strong>Key Lesson:</strong> ALWAYS benchmark yourself! Papers â‰  Production Reality</p></div>
        <div class="slide"><h1>PART 2: MODEL PERFORMANCE BY TIME SERIES</h1></div>
        <div class="slide"><h2>Slide 4: Solar - The DL Showcase (Best Results)</h2>
<h3>ğŸ“ˆ Solar Time Series 2022-2024</h3>
<p><img alt="Solar Timeline" src="figures/solar_timeline_clean.png" /></p>
<p><em>Characterisics: Symmetrische DayssverlÃ¤ufe, Winter-summer contrast, CV=1.534</em></p>
<h3>ğŸ“Š Performance Overview</h3>
<h4>Baseline Models</h4>
<table>
<thead>
<tr>
<th>Rank</th>
<th>Model</th>
<th>RMSE (MW)</th>
<th>MAPE (%)</th>
<th>RÂ²</th>
<th>Category</th>
</tr>
</thead>
<tbody>
<tr>
<td>-</td>
<td><strong>Naive</strong></td>
<td>~3000</td>
<td>~30</td>
<td>~0.70</td>
<td>Baseline</td>
</tr>
<tr>
<td>-</td>
<td><strong>Seasonal Naive (24h)</strong></td>
<td>~2500</td>
<td>~25</td>
<td>~0.80</td>
<td>Baseline</td>
</tr>
<tr>
<td>-</td>
<td><strong>Mean</strong></td>
<td>~3500</td>
<td>~35</td>
<td>~0.60</td>
<td>Baseline</td>
</tr>
<tr>
<td>-</td>
<td>SARIMA</td>
<td>~2000</td>
<td>~20</td>
<td>~0.85</td>
<td>Statisical</td>
</tr>
</tbody>
</table>
<h4>ML Tree Models (Standard Pipeline)</h4>
<table>
<thead>
<tr>
<th>Rank</th>
<th>Model</th>
<th>RMSE (MW)</th>
<th>MAPE (%)</th>
<th>RÂ²</th>
<th>Category</th>
</tr>
</thead>
<tbody>
<tr>
<td>ğŸ¥‡</td>
<td><strong>LightGBM</strong></td>
<td><strong>358.8</strong></td>
<td><strong>3.37</strong></td>
<td><strong>0.9838</strong></td>
<td>ML Tree</td>
</tr>
<tr>
<td>ğŸ¥ˆ</td>
<td><strong>XGBoost</strong></td>
<td>359.5</td>
<td>3.36</td>
<td>0.9838</td>
<td>ML Tree</td>
</tr>
<tr>
<td>ğŸ¥‰</td>
<td><strong>Random Forest</strong></td>
<td>373.6</td>
<td>3.34</td>
<td>0.9825</td>
<td>ML Tree</td>
</tr>
<tr>
<td>4</td>
<td>CatBoost</td>
<td>379.6</td>
<td>3.59</td>
<td>0.9819</td>
<td>ML Tree</td>
</tr>
</tbody>
</table>
<h4>Deep Learning Models (Extended Testing to Colab T4 GPU)</h4>
<table>
<thead>
<tr>
<th>Rank</th>
<th>Model</th>
<th>RMSE (MW)</th>
<th>MAE (MW)</th>
<th>RÂ²</th>
<th>Training time</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td><strong>Bi-LSTM</strong></td>
<td><strong>-</strong></td>
<td><strong>-</strong></td>
<td><strong>0.9955</strong></td>
<td>~30s</td>
</tr>
<tr>
<td>2</td>
<td><strong>Baseline LSTM</strong></td>
<td><strong>-</strong></td>
<td><strong>-</strong></td>
<td><strong>0.9934</strong></td>
<td>~25s</td>
</tr>
<tr>
<td>3</td>
<td><strong>Autoencoder</strong></td>
<td><strong>-</strong></td>
<td><strong>-</strong></td>
<td><strong>0.9515</strong></td>
<td>~40s</td>
</tr>
<tr>
<td>4</td>
<td><strong>VAE</strong></td>
<td><strong>-</strong></td>
<td><strong>-</strong></td>
<td><strong>0.9255</strong></td>
<td>~60s</td>
</tr>
<tr>
<td>âŒ</td>
<td>N-BEATS</td>
<td>23,316</td>
<td>16,348</td>
<td>-18.93</td>
<td>~977s</td>
</tr>
<tr>
<td>âŒ</td>
<td>N-HiTS</td>
<td>11,930</td>
<td>8,211</td>
<td>-4.22</td>
<td>~138s</td>
</tr>
</tbody>
</table>
<h3>ğŸ† Key Insights</h3>
<p><strong>Bi-LSTM RÂ²=0.9955 vs LightGBM RÂ²=0.9838</strong> â†’ <strong>+1.2% absolute</strong></p>
<p><strong>Why DL wins:</strong><br />
- Bidirectionale Architecture erfasst Sonnentogang/Untergang-symmetry<br />
- Sequenzielle Muster optimal for tÃ¤gliche Zyklen<br />
- GPU-accelerated: 30s Training</p>
<p><strong>Archetype 1: Determinisic-Symmetric</strong> â˜€ï¸</p></div>
        <div class="slide"><h2>Slide 5: Wind Onshore - ML Dominance Despite Chaos</h2>
<h3>ğŸ“ˆ Wind Onshore Time Series 2022-2024</h3>
<p><img alt="Wind Onshore Timeline" src="figures/wind_onshore_timeline_clean.png" /></p>
<p><em>Characterisics: Continuous operation, only 21 zero values (0.08%), hohe Volatility (CV=0.666)</em></p>
<h3>ğŸ“Š Performance Overview</h3>
<h4>Baseline Models</h4>
<table>
<thead>
<tr>
<th>Rank</th>
<th>Model</th>
<th>RMSE (MW)</th>
<th>MAPE (%)</th>
<th>RÂ²</th>
<th>Category</th>
</tr>
</thead>
<tbody>
<tr>
<td>-</td>
<td><strong>Naive</strong></td>
<td>~500</td>
<td>~10</td>
<td>~0.90</td>
<td>Baseline</td>
</tr>
<tr>
<td>-</td>
<td><strong>Seasonal Naive (24h)</strong></td>
<td>~450</td>
<td>~9</td>
<td>~0.92</td>
<td>Baseline</td>
</tr>
<tr>
<td>-</td>
<td><strong>Mean</strong></td>
<td>~600</td>
<td>~12</td>
<td>~0.85</td>
<td>Baseline</td>
</tr>
<tr>
<td>-</td>
<td>SARIMA</td>
<td>~400</td>
<td>~8</td>
<td>~0.93</td>
<td>Statisical</td>
</tr>
</tbody>
</table>
<h4>ML Tree Models - DOMINANCE</h4>
<table>
<thead>
<tr>
<th>Rank</th>
<th>Model</th>
<th>RMSE (MW)</th>
<th>MAPE (%)</th>
<th>RÂ²</th>
<th>Category</th>
</tr>
</thead>
<tbody>
<tr>
<td>ğŸ¥‡</td>
<td><strong>Random Forest</strong></td>
<td><strong>33.96</strong></td>
<td><strong>2.24</strong></td>
<td><strong>0.9997</strong></td>
<td>ML Tree</td>
</tr>
<tr>
<td>ğŸ¥ˆ</td>
<td>XGBoost</td>
<td>40.98</td>
<td>-</td>
<td>0.9995</td>
<td>ML Tree</td>
</tr>
<tr>
<td>ğŸ¥‰</td>
<td>LightGBM</td>
<td>44.61</td>
<td>-</td>
<td>0.9994</td>
<td>ML Tree</td>
</tr>
</tbody>
</table>
<h4>Deep Learning Models (Extended Testing - Colab GPU T4)</h4>
<table>
<thead>
<tr>
<th>Rank</th>
<th>Model</th>
<th>RMSE (MW)</th>
<th>MAE (MW)</th>
<th>RÂ²</th>
<th>Training time</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td><strong>LSTM</strong></td>
<td><strong>397.74</strong></td>
<td><strong>290.85</strong></td>
<td><strong>0.9548</strong></td>
<td>22.7s</td>
</tr>
<tr>
<td>2</td>
<td><strong>GRU</strong></td>
<td>405.06</td>
<td>312.30</td>
<td>0.9532</td>
<td>23.1s</td>
</tr>
<tr>
<td>3</td>
<td><strong>Bi-LSTM</strong></td>
<td>409.37</td>
<td>311.78</td>
<td>0.9522</td>
<td>60.8s</td>
</tr>
<tr>
<td>4</td>
<td><strong>Autoencoder</strong></td>
<td>653.26</td>
<td>501.30</td>
<td>0.8782</td>
<td>187.2s</td>
</tr>
<tr>
<td>5</td>
<td><strong>VAE</strong></td>
<td>705.88</td>
<td>550.90</td>
<td>0.8578</td>
<td>195.8s</td>
</tr>
<tr>
<td>âŒ</td>
<td>DeepAR</td>
<td>2,672.60</td>
<td>2,167.69</td>
<td><strong>-1.0304</strong></td>
<td>284.8s</td>
</tr>
<tr>
<td>âŒ</td>
<td>N-BEATS</td>
<td>4,449.91</td>
<td>4,025.21</td>
<td><strong>-4.6288</strong></td>
<td>1960.6s</td>
</tr>
<tr>
<td>âŒ</td>
<td>N-HiTS</td>
<td>5.99Ã—10Â¹â°Â³</td>
<td>5.51Ã—10Â¹â°Â²</td>
<td><strong>-1.02Ã—10Â²â°Â¹</strong></td>
<td>259.7s</td>
</tr>
</tbody>
</table>
<h3>ğŸ” Critical Analysis</h3>
<p><strong>Random Forest RÂ²=0.9997 vs LSTM RÂ²=0.9548</strong> â†’ <strong>4.7% gap in favor of ML!</strong></p>
<p><strong>Why ML wins:</strong><br />
- Wind is fundamentally stochastic (Butterfly effect)<br />
- Weak sequential patterns â†’ LSTM finds little<br />
- Random Forest averages 100+ Trees â†’ robust against chaos<br />
- Feature Engineering (lag_1, diff_1) dominates sequences</p>
<p><strong>Archetype 3: Stochastic-Chaotic</strong> ğŸ’¨</p></div>
        <div class="slide"><h2>Slide 6: Wind Offshore - The Problem Case Solved!</h2>
<h3>ğŸ“ˆ Wind Offshore Time Series 2022-2024</h3>
<p><img alt="Wind Offshore Timeline" src="figures/wind_offshore_timeline_clean.png" /></p>
<p><em>Characterisics: 9.6-Months Outage (Apr 2023 - Yesn 2024), 37.9% zero values, only 18.312 valide Data Points</em></p>
<h3>ğŸ“Š Performance Overview (after data cleaning)</h3>
<h4>Baseline Models</h4>
<table>
<thead>
<tr>
<th>Rank</th>
<th>Model</th>
<th>RMSE (MW)</th>
<th>MAPE (%)</th>
<th>RÂ²</th>
<th>Category</th>
</tr>
</thead>
<tbody>
<tr>
<td>-</td>
<td><strong>Naive</strong></td>
<td>~300</td>
<td>~25</td>
<td>~0.20</td>
<td>Baseline</td>
</tr>
<tr>
<td>-</td>
<td><strong>Seasonal Naive (24h)</strong></td>
<td>~280</td>
<td>~22</td>
<td>~0.30</td>
<td>Baseline</td>
</tr>
<tr>
<td>-</td>
<td><strong>Mean</strong></td>
<td>~350</td>
<td>~30</td>
<td>~0.10</td>
<td>Baseline</td>
</tr>
<tr>
<td>-</td>
<td>SARIMA</td>
<td>~250</td>
<td>~20</td>
<td>~0.40</td>
<td>Statisical</td>
</tr>
</tbody>
</table>
<h4>ML Tree Models (Standard Pipeline)</h4>
<table>
<thead>
<tr>
<th>Rank</th>
<th>Model</th>
<th>RMSE (MW)</th>
<th>MAPE (%)</th>
<th>RÂ²</th>
<th>Category</th>
</tr>
</thead>
<tbody>
<tr>
<td>ğŸ¥‡</td>
<td><strong>XGBoost</strong></td>
<td><strong>TBD</strong></td>
<td><strong>TBD</strong></td>
<td><strong>~0.85</strong></td>
<td>ML Tree</td>
</tr>
<tr>
<td>ğŸ¥ˆ</td>
<td>Random Forest</td>
<td>TBD</td>
<td>TBD</td>
<td>~0.82</td>
<td>ML Tree</td>
</tr>
<tr>
<td>ğŸ¥‰</td>
<td>LightGBM</td>
<td>TBD</td>
<td>TBD</td>
<td>~0.80</td>
<td>ML Tree</td>
</tr>
</tbody>
</table>
<h4>Deep Learning Models (Extended Testing - Colab GPU T4) âœ… NEW RESULTS!</h4>
<table>
<thead>
<tr>
<th>Rank</th>
<th>Model</th>
<th>RMSE (MW)</th>
<th>MAE (MW)</th>
<th>RÂ²</th>
<th>Training time</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td><strong>GRU</strong></td>
<td><strong>123.39</strong></td>
<td><strong>87.69</strong></td>
<td><strong>0.3292</strong> ğŸ†</td>
<td>13.1s</td>
</tr>
<tr>
<td>2</td>
<td><strong>Bi-LSTM</strong></td>
<td>133.78</td>
<td>95.82</td>
<td>0.2114</td>
<td>30.7s</td>
</tr>
<tr>
<td>3</td>
<td><strong>LSTM</strong></td>
<td>144.75</td>
<td>87.81</td>
<td>0.0768</td>
<td>15.4s</td>
</tr>
<tr>
<td>4</td>
<td><strong>Autoencoder</strong></td>
<td>188.65</td>
<td>145.56</td>
<td>-0.5682</td>
<td>79.5s</td>
</tr>
<tr>
<td>5</td>
<td><strong>VAE</strong></td>
<td>420.64</td>
<td>361.24</td>
<td>-6.7963</td>
<td>83.0s</td>
</tr>
<tr>
<td>âŒ</td>
<td>DeepAR</td>
<td>436.83</td>
<td>383.72</td>
<td><strong>-7.1134</strong></td>
<td>106.6s</td>
</tr>
<tr>
<td>âŒ</td>
<td>N-BEATS</td>
<td>563.17</td>
<td>501.50</td>
<td><strong>-12.4851</strong></td>
<td>733.8s</td>
</tr>
<tr>
<td>âŒ</td>
<td>N-HiTS</td>
<td>1,544.39</td>
<td>1,519.13</td>
<td><strong>-100.4139</strong></td>
<td>98.4s</td>
</tr>
</tbody>
</table>
<p><strong>âœ… All 8 DL-models tested!</strong> GRU best choice, but RÂ²=0.33 shows massive Challenges!</p>
<h3>ğŸ” Critical Analysis</h3>
<p><strong>Why is RÂ²=0.33 so low?</strong></p>
<ol>
<li><strong>Data Loss:</strong> 37.9% der data sind Nullen â†’ only 18.312 valide points</li>
<li><strong>Structural Break:</strong> 9.6-Months Outage fragments training data</li>
<li><strong>Weather Dependency:</strong> Wind speed missing â†’ only Proxy-Features</li>
<li><strong>Chaotic Physics:</strong> Offshore wind even more unpredictable than onshore</li>
</ol>
<p><strong>GRU RÂ²=0.3292 vs LSTM RÂ²=0.0768</strong> â†’ <strong>GRU 328% better!</strong></p>
<p><strong>Comparison to Wind Onshore:</strong></p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Wind Onshore</th>
<th>Wind Offshore</th>
<th>Interpretation</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Best DL RÂ²</strong></td>
<td>0.9548 (LSTM)</td>
<td>0.3292 (GRU)</td>
<td><strong>-65% due to outage!</strong></td>
</tr>
<tr>
<td><strong>Best ML RÂ²</strong></td>
<td>0.9997 (RF)</td>
<td>~0.85 (XGB)</td>
<td>-15% due to data loss</td>
</tr>
<tr>
<td><strong>zero values</strong></td>
<td>21 (0.08%)</td>
<td>9,945 (37.9%)</td>
<td><strong>474x more!</strong></td>
</tr>
<tr>
<td><strong>Trainierbare points</strong></td>
<td>26,257</td>
<td>18,312</td>
<td>-30% data</td>
</tr>
</tbody>
</table>
<p><strong>Key Insight:</strong><br />
- Wind Offshore is <strong>not unsolvable</strong>, but <strong>massively harder</strong> than Onshore<br />
- GRU beats LSTM here too (as with Price/Consumption!)<br />
- SOTA models fail spectacularly: N-HiTS RÂ²=-100.41! âŒ</p>
<p><strong>Lesson Learned:</strong><br />
- Bei ernewrbaren Energien sind <strong>exogenous weather features are essential</strong>!<br />
- Structural breaks must be <strong>modeled separately</strong> be (Binary Classifier + Regressor)<br />
- GRU is robuster than LSTM/Bi-LSTM bei fragmenteden data</p></div>
        <div class="slide"><h2>Slide 7: Consumption - GRU viatrifft Bi-LSTM!</h2>
<h3>ğŸ“ˆ Consumption Time Series 2022-2024</h3>
<p><img alt="Consumption Timeline" src="figures/consumption_timeline_clean.png" /></p>
<p><em>Characterisics: Stable patterns, niedrigste Volatility (CV=0.175), klare Wochen-/Daysszyklen</em></p>
<h3>ğŸ“Š Performance Overview</h3>
<h4>Baseline Models</h4>
<table>
<thead>
<tr>
<th>Rank</th>
<th>Model</th>
<th>RMSE (MW)</th>
<th>MAPE (%)</th>
<th>RÂ²</th>
<th>Category</th>
</tr>
</thead>
<tbody>
<tr>
<td>-</td>
<td><strong>Naive</strong></td>
<td>~2000</td>
<td>~4</td>
<td>~0.85</td>
<td>Baseline</td>
</tr>
<tr>
<td>-</td>
<td><strong>Seasonal Naive (24h)</strong></td>
<td>~1800</td>
<td>~3.5</td>
<td>~0.88</td>
<td>Baseline</td>
</tr>
<tr>
<td>-</td>
<td><strong>Mean</strong></td>
<td>~2500</td>
<td>~5</td>
<td>~0.80</td>
<td>Baseline</td>
</tr>
<tr>
<td>-</td>
<td>SARIMA</td>
<td>~1600</td>
<td>~3</td>
<td>~0.90</td>
<td>Statisical</td>
</tr>
</tbody>
</table>
<h4>ML Tree Models (Standard Pipeline)</h4>
<table>
<thead>
<tr>
<th>Rank</th>
<th>Model</th>
<th>RMSE (MW)</th>
<th>MAPE (%)</th>
<th>RÂ²</th>
<th>Category</th>
</tr>
</thead>
<tbody>
<tr>
<td>ğŸ¥‡</td>
<td><strong>LightGBM</strong></td>
<td><strong>~1200</strong></td>
<td><strong>~2.5</strong></td>
<td><strong>~0.95</strong></td>
<td>ML Tree</td>
</tr>
<tr>
<td>ğŸ¥ˆ</td>
<td>XGBoost</td>
<td>~1250</td>
<td>~2.6</td>
<td>~0.94</td>
<td>ML Tree</td>
</tr>
<tr>
<td>ğŸ¥‰</td>
<td>Random Forest</td>
<td>~1300</td>
<td>~2.8</td>
<td>~0.93</td>
<td>ML Tree</td>
</tr>
</tbody>
</table>
<h4>Deep Learning Models (Extended Testing - Colab GPU)</h4>
<table>
<thead>
<tr>
<th>Rank</th>
<th>Model</th>
<th>RMSE (MW)</th>
<th>MAE (MW)</th>
<th>RÂ²</th>
<th>Training time</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td><strong>GRU</strong></td>
<td><strong>-</strong></td>
<td><strong>-</strong></td>
<td><strong>0.9874</strong> ğŸ†</td>
<td>~25s</td>
</tr>
<tr>
<td>2</td>
<td><strong>Bi-LSTM</strong></td>
<td>1,302.6</td>
<td>1,046.3</td>
<td>0.9799</td>
<td>~55s</td>
</tr>
<tr>
<td>3</td>
<td><strong>LSTM</strong></td>
<td>-</td>
<td>-</td>
<td>0.9772</td>
<td>~30s</td>
</tr>
<tr>
<td>4</td>
<td><strong>Autoencoder</strong></td>
<td>-</td>
<td>-</td>
<td>0.9799</td>
<td>~45s</td>
</tr>
<tr>
<td>5</td>
<td><strong>VAE</strong></td>
<td>-</td>
<td>-</td>
<td>0.9697</td>
<td>~70s</td>
</tr>
<tr>
<td>âŒ</td>
<td>N-BEATS</td>
<td>-</td>
<td>-</td>
<td>-0.9420</td>
<td>~850s</td>
</tr>
<tr>
<td>âŒ</td>
<td>DeepAR</td>
<td>-</td>
<td>-</td>
<td>-1.2356</td>
<td>~280s</td>
</tr>
<tr>
<td>âŒ</td>
<td>N-HiTS</td>
<td>-</td>
<td>-</td>
<td>-9.5849</td>
<td>~140s</td>
</tr>
</tbody>
</table>
<h3>ğŸ” Surprise: GRU &gt; Bi-LSTM!</h3>
<p><strong>GRU RÂ²=0.9874 vs Bi-LSTM RÂ²=0.9799</strong> â†’ <strong>+0.75% absolute, 2x faster!</strong></p>
<p><strong>Why?</strong><br />
- Weekly patterns are unidirectional (Moâ†’So)<br />
- GRU: Simpler (2 gates instead of 4) â†’ less overfitting<br />
- Bi-LSTM-advantages (symmetry) not relevant here</p>
<p><strong>Archetype 2: Structured-Sequential</strong> ğŸ­</p></div>
        <div class="slide"><h2>Slide 8: Price - ML Dominates Volatile Markets</h2>
<h3>ğŸ“ˆ Price Time Series 2022-2024</h3>
<p><img alt="Price Timeline" src="figures/price_timeline_clean.png" /></p>
<p><em>Characterisics: Hohe Volatility (CV=0.850), 827 negativee prices (3.15%), Max 936 EUR/MWh</em></p>
<h3>ğŸ“Š Performance Overview</h3>
<h4>Baseline Models</h4>
<table>
<thead>
<tr>
<th>Rank</th>
<th>Model</th>
<th>RMSE (EUR/MWh)</th>
<th>MAE</th>
<th>RÂ²</th>
<th>Category</th>
</tr>
</thead>
<tbody>
<tr>
<td>-</td>
<td><strong>Naive</strong></td>
<td>~50</td>
<td>~35</td>
<td>~0.50</td>
<td>Baseline</td>
</tr>
<tr>
<td>-</td>
<td><strong>Seasonal Naive (24h)</strong></td>
<td>~45</td>
<td>~30</td>
<td>~0.60</td>
<td>Baseline</td>
</tr>
<tr>
<td>-</td>
<td><strong>Mean</strong></td>
<td>~60</td>
<td>~40</td>
<td>~0.40</td>
<td>Baseline</td>
</tr>
<tr>
<td>-</td>
<td>SARIMA</td>
<td>~35</td>
<td>~25</td>
<td>~0.70</td>
<td>Statisical</td>
</tr>
</tbody>
</table>
<h4>ML Tree Models - STRONG</h4>
<table>
<thead>
<tr>
<th>Rank</th>
<th>Model</th>
<th>RMSE (EUR/MWh)</th>
<th>MAE</th>
<th>RÂ²</th>
<th>Category</th>
</tr>
</thead>
<tbody>
<tr>
<td>ğŸ¥‡</td>
<td><strong>LightGBM</strong></td>
<td><strong>10.03</strong></td>
<td><strong>1.76</strong></td>
<td><strong>0.9798</strong></td>
<td>ML Tree</td>
</tr>
<tr>
<td>ğŸ¥ˆ</td>
<td>Random Forest</td>
<td>10.60</td>
<td>1.14</td>
<td>0.9775</td>
<td>ML Tree</td>
</tr>
<tr>
<td>ğŸ¥‰</td>
<td>XGBoost</td>
<td>11.48</td>
<td>1.63</td>
<td>0.9736</td>
<td>ML Tree</td>
</tr>
</tbody>
</table>
<h4>Deep Learning Models (Extended Testing - Colab GPU T4)</h4>
<table>
<thead>
<tr>
<th>Rank</th>
<th>Model</th>
<th>RMSE (EUR/MWh)</th>
<th>MAE</th>
<th>RÂ²</th>
<th>Training time</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td><strong>GRU</strong> ğŸ†</td>
<td><strong>23.43</strong></td>
<td><strong>11.72</strong></td>
<td><strong>0.8906</strong></td>
<td>25.7s</td>
</tr>
<tr>
<td>2</td>
<td><strong>Bi-LSTM</strong></td>
<td>23.99</td>
<td>11.06</td>
<td>0.8853</td>
<td>172.3s</td>
</tr>
<tr>
<td>3</td>
<td><strong>LSTM</strong></td>
<td>27.47</td>
<td>14.88</td>
<td>0.8496</td>
<td>22.9s</td>
</tr>
<tr>
<td>4</td>
<td><strong>Autoencoder</strong></td>
<td>37.47</td>
<td>19.38</td>
<td>0.7202</td>
<td>187.4s</td>
</tr>
<tr>
<td>5</td>
<td><strong>VAE</strong></td>
<td>47.00</td>
<td>23.93</td>
<td>0.5597</td>
<td>187.0s</td>
</tr>
<tr>
<td>âŒ</td>
<td>DeepAR</td>
<td>103.70</td>
<td>71.57</td>
<td><strong>-1.1557</strong></td>
<td>366.5s</td>
</tr>
<tr>
<td>âŒ</td>
<td>N-BEATS</td>
<td>144.06</td>
<td>125.30</td>
<td><strong>-3.1599</strong></td>
<td>2131.4s</td>
</tr>
<tr>
<td>âŒ</td>
<td>N-HiTS</td>
<td>153.85</td>
<td>128.26</td>
<td><strong>-3.7446</strong></td>
<td>334.6s</td>
</tr>
</tbody>
</table>
<h3>ğŸ” Critical Analysis</h3>
<p><strong>LightGBM RÂ²=0.9798 vs GRU RÂ²=0.8906</strong> â†’ <strong>9% gap in favor of ML!</strong></p>
<p><strong>Why ML wins:</strong><br />
- Hohe Volatility (CV=0.85) â†’ spikes dominate<br />
- Feature Engineering (lag_1, diff_1, momentum_3h) erfasst Spikes better<br />
- DL smooths too much â†’ underestimates extrema</p>
<p><strong>Archetype 4: Volatile-Structured</strong> ğŸ’°</p></div>
        <div class="slide"><h2>Slide 9: Model-Architecture Vergleich - 5 Time Seriesn Analyse</h2>
<h3>ğŸ“Š Performance Matrix: Cross-Series Comparison</h3>
<table>
<thead>
<tr>
<th>Architecture</th>
<th>Solar</th>
<th>Wind On</th>
<th>Wind Off</th>
<th>Consumption</th>
<th>Price</th>
<th>Best Use Case</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Bi-LSTM</strong></td>
<td><strong>0.9955</strong> ğŸ†</td>
<td>0.9522</td>
<td>0.2114</td>
<td>0.9799</td>
<td>0.8853</td>
<td>Symmetric Patterns</td>
</tr>
<tr>
<td><strong>GRU</strong></td>
<td>0.9813</td>
<td>0.9532</td>
<td><strong>0.3292</strong> ğŸ†</td>
<td><strong>0.9874</strong> ğŸ†</td>
<td><strong>0.8906</strong> ğŸ†</td>
<td>Unidirectional/Volatil</td>
</tr>
<tr>
<td><strong>LSTM</strong></td>
<td>0.9934</td>
<td><strong>0.9548</strong> ğŸ†</td>
<td>0.0768</td>
<td>0.9772</td>
<td>0.8496</td>
<td>Standard Sequences</td>
</tr>
<tr>
<td><strong>Random Forest</strong></td>
<td>0.9825</td>
<td><strong>0.9997</strong> ğŸ†</td>
<td>~0.82</td>
<td>~0.93</td>
<td>0.9775</td>
<td>Chaotische data</td>
</tr>
<tr>
<td><strong>LightGBM</strong></td>
<td>0.9838</td>
<td>0.9994</td>
<td>~0.80</td>
<td>~0.95</td>
<td><strong>0.9798</strong> ğŸ†</td>
<td>Universally strong</td>
</tr>
<tr>
<td><strong>XGBoost</strong></td>
<td>0.9838</td>
<td>0.9995</td>
<td><strong>~0.85</strong> ğŸ†</td>
<td>~0.94</td>
<td>0.9736</td>
<td>Feature-rich</td>
</tr>
</tbody>
</table>
<h3>ğŸ’¡ Die 5 Time Seriesn-Archetypeen</h3>
<h4>Archetype 1: <strong>Determinisic-Symmetric</strong> (Solar) â˜€ï¸</h4>
<ul>
<li>âœ… Starke Daysszyklen, symmetric gradients</li>
<li><strong>Best:</strong> Bi-LSTM (0.9955) - BidirectionalitÃ¤t nutzt symmetry</li>
</ul>
<h4>Archetype 2: <strong>Structured-Sequential</strong> (Consumption) ğŸ­</h4>
<ul>
<li>âœ… Weekly patterns, unidirektionale Sequences</li>
<li><strong>Best:</strong> GRU (0.9874) - Simpler &amp; faster than Bi-LSTM</li>
</ul>
<h4>Archetype 3: <strong>Stochastic-Chaotic</strong> (Wind Onshore) ğŸ’¨</h4>
<ul>
<li>âŒ Weak patterns, high stochasticity</li>
<li><strong>Best:</strong> Random Forest (0.9997) - Ensemble averages Chaos</li>
</ul>
<h4>Archetype 4: <strong>Volatile-Structured</strong> (Price) ğŸ’°</h4>
<ul>
<li>ğŸ”¥ Spikes, negativee values, CV=0.85</li>
<li><strong>Best:</strong> LightGBM (0.9798) - Features &gt; Sequences</li>
</ul>
<h4>Archetype 5: <strong>Fragmented-Chaotic</strong> (Wind Offshore) ğŸŒŠ</h4>
<ul>
<li>âš ï¸ Structural breaks, 37.9% dataverlust</li>
<li><strong>Best:</strong> GRU (0.33) / XGBoost (~0.85) - Both weak!</li>
</ul>
<h3>ğŸ¯ Decision Tree</h3>
<pre><code>START: Analysiere deine Time Series
â”‚
â”œâ”€ Does it have STRUCTURAL BREAKS (&gt;20% missing)?
â”‚  â””â”€ Yes â†’ Separate Outage Prediction + Regressor
â”‚     Bestes Model: GRU (robuster than LSTM)
â”‚
â”œâ”€ Is it SYMMETRIC (up/down equal)?
â”‚  â””â”€ Yes (e.g. Solar) â†’ Bi-LSTM (0.9955)
â”‚
â”œâ”€ Is it UNIDIRECTIONAL sequential?
â”‚  â””â”€ Yes (e.g. Consumption) â†’ GRU (0.9874, 2x faster than Bi-LSTM)
â”‚
â”œâ”€ Is it VOLATILE (CV &gt; 0.7)?
â”‚  â””â”€ Yes (e.g. Price) â†’ LightGBM (0.9798, DL fails!)
â”‚
â”œâ”€ Is it CHAOTIC (ACF&lt;0.3)?
â”‚  â””â”€ Yes (e.g. Wind) â†’ Random Forest (0.9997, DL -4.7%)
â”‚
â””â”€ NEVER N-BEATS/N-HiTS use!
   â†’ Always negativee for us (-100 to -18)
</code></pre></div>
        <div class="slide"><h2>Slide 10: Energiemarkt-dynamics - What drives what?</h2>
<h3>ğŸ’¡ The economic perspective: Granger Causality shows Marktmechanismen</h3>
<p><strong>All 12 Kombinationen significant (p &lt; 0.0001)</strong> - What does this mean economically?</p>
<h3>ğŸŒ <strong>Solar â†’ Price (F=847.3, strongest effect!)</strong></h3>
<p><strong>Merit Order Effect in Action:</strong><br />
- Sunny day â†’ 40.000 MW Solar into the grid<br />
- Solar has marginal costs ~0 EUR/MWh â†’ displaces expensive gas plants<br />
- <strong>Price drops from 150 to 50 EUR/MWh</strong></p>
<p><strong>Real-World Impact:</strong><br />
- On sunny summer days: Negative Preise possible (827 Cases!)<br />
- <strong>But:</strong> forecasting difficult, because non-linear (threshold effect)</p>
<h3>âš¡ <strong>Price â†’ Consumption (F=234.5)</strong></h3>
<p><strong>Demand Response - The market reaction:</strong><br />
- High price (&gt;200 EUR/MWh) â†’ Industry shuts down<br />
- Low price (&lt;50 EUR/MWh) â†’ Additional demand</p>
<p><strong>Example Aluminum smelter:</strong><br />
- Flexible power consumption 500 MW<br />
- Bei Price &gt; 180 EUR/MWh: Production down â†’ <strong>Consumption decreases</strong><br />
- Bei Price &lt; 60 EUR/MWh: Production up â†’ <strong>Consumption increases</strong></p>
<p><strong>Correlation:</strong> -0.23 (negative!) â†’ High price suppresses demand</p>
<h3>ğŸ­ <strong>Solar â†‘ â†’ Consumption â†‘ (F=156.2)</strong></h3>
<p><strong>Warum increases Konsum bei hoher Solar-feed-in?</strong></p>
<p><strong>Hypothesis 1: Price signal</strong><br />
- Solar â†‘ â†’ Preis â†“ â†’ Consumption â†‘ (via Price than Mediator)<br />
- <strong>Indirect causality:</strong> Solar â†’ Price â†’ Consumption</p>
<p><strong>Hypothesis 2: Time of day-Effekt</strong><br />
- Solar peak = 12-14 o'clock<br />
- Industrial peak = 10-16 o'clock<br />
- <strong>Spurious correlation:</strong> Beide folgen Dayssrhythmus</p>
<p><strong>Hypothesis 3: Smart Grid Response</strong><br />
- Smart consumers (Heat pumps, E-cars)<br />
- Charge automatically with high renewable feed-in<br />
- <strong>Real causality:</strong> Solar forecast â†’ Consumption planning</p>
<p><strong>Test with VAR:</strong> Solar â†’ Consumption is significant (even after controlling for Time of day)<br />
â†’ <strong>Hybrid explanation:</strong> Price signal + Time of day + Smart response</p>
<h3>ğŸ’¨ <strong>Wind â†” Price (Bidirectional, F=298.7)</strong></h3>
<p><strong>Complex interaction:</strong></p>
<p><strong>Wind â†’ Price:</strong><br />
- Windy night â†’ 20.000 MW Offshore â†’ Oversupply<br />
- <strong>Preis kann negative be</strong> (-500 EUR/MWh Maximum)</p>
<p><strong>Price â†’ Wind (???):</strong> <br />
- <strong>Seemingly paradoxical:</strong> How can price influence wind?<br />
- <strong>Explanation:</strong> Curtailment (Curtailment)<br />
  - Bei Preis &lt; -50 EUR/MWh: Windparks be abgeschaltet<br />
  - <strong>Gemessene Wind-Einspeisung decreases</strong>, obwohl Wind physisch strong is<br />
  - â†’ Economic decision, not meteorological!</p>
<p><strong>Lesson:</strong> Granger causality â‰  physical causality!</p>
<h3>ğŸ”— <strong>Cointegration: Long-term equilibria</strong></h3>
<p><strong>4 Cointegrationsvektoren gefunden</strong> â†’ What does this mean?</p>
<p><strong>Vereinfachtes Example:</strong></p>
<pre><code>Langfrisiger Zusammenhang:
Price = 100 + 0.5 * Consumption - 2 * Solar - 1.5 * Wind

Interpretation:
- 1000 MW more Consumption â†’ +0.5 EUR/MWh
- 1000 MW more Solar â†’ -2 EUR/MWh (Merit Order!)
- 1000 MW more Wind â†’ -1.5 EUR/MWh
</code></pre>
<p><strong>What does this tell us?</strong><br />
- Kurzfrisig: Prices fluctuate wildly (Spikes, Volatility)<br />
- Langfrisig: Equilibria exist (Regression to mean)<br />
- <strong>Practically:</strong> For day-ahead forecasts (24h) â†’ Cointegration hilft wenig</p>
<h3>ğŸ“Š VAR-Model: Can we bring KausalitÃ¤t use?</h3>
<p><strong>Sobering results:</strong></p>
<table>
<thead>
<tr>
<th>Time Series</th>
<th>Univariate (Best)</th>
<th>VAR (Multivariate)</th>
<th>Delta</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Price</strong></td>
<td>0.9798 (LightGBM)</td>
<td>0.15</td>
<td><strong>-98%!</strong> âŒ</td>
</tr>
<tr>
<td><strong>Solar</strong></td>
<td>0.9955 (Bi-LSTM)</td>
<td>0.63</td>
<td>-53%</td>
</tr>
<tr>
<td><strong>Consumption</strong></td>
<td>0.9874 (GRU)</td>
<td>0.59</td>
<td>-67%</td>
</tr>
</tbody>
</table>
<p><strong>Why doesn't causality help with forecasting?</strong></p>
<p><strong>1. VAR is linear, markets are non-linear</strong><br />
- Merit Order: Step function, not a line<br />
- Curtailment: threshold effect bei negativeen Preisen<br />
- VAR doesn't capture this!</p>
<p><strong>2. Lag 24 too long for short-term dynamics</strong><br />
- Price-Spikes entstehen in Minutes<br />
- VAR mit 24h-Lag is zu trÃ¤ge<br />
- Needs shorter lags (1-3h), but dann fehlt Seasonality</p>
<p><strong>3. Missing exogenous factors</strong><br />
- Weather (dominant for Solar/Wind!)<br />
- Market events (e.g. Power plant outages)<br />
- Policy (e.g. CO2 price changes)</p>
<p><strong>Critical insight:</strong><br />
- <strong>Granger causality is DESKRIPTIV</strong> (shows ZusammenhÃ¤nge)<br />
- <strong>But not PREDICTIVE</strong> (doesn't help with forecasting)<br />
- Univariatee models with good Features (lag_1, diff_1, hour) beat VAR</p>
<h3>ğŸ¯ Praktische Implikationen for energy Trading</h3>
<p><strong>What did we learn?</strong></p>
<p><strong>1. Merit Order works!</strong><br />
- Solar/Wind high â†’ Price down (F=847.3)<br />
- For traders: Monitor Solar forecast for price prediction</p>
<p><strong>2. Demand Response is real</strong><br />
- Price high â†’ Consumption down (F=234.5)<br />
- For grid operators: Price signale steuern Nachfrage</p>
<p><strong>3. Curtailment is Ã¶konomisch, not physical</strong><br />
- Price negative â†’ Wind "decreases" (Curtailment)<br />
- For policy: Storage incentives reduce curtailment</p>
<p><strong>4. VAR is nicht die LÃ¶sung</strong><br />
- Non-linearity, missing exogenous<br />
- <strong>Better:</strong> Univariatee ML/DL + exogenous features<br />
- <strong>Alternatively:</strong> ML-based multivariatee (XGBoost with cross-series lags)</p>
<p><strong>5. Cointegration shows langfrisige Trends</strong><br />
- For strategic planning (Investments)<br />
- Nicht for operatives Forecasting (Day-ahead)</p>
<p><strong>Key Takeaway:</strong><br />
Understand causality â†’ bettere Features build â†’ bettere univariate models!<br />
Not: KausalitÃ¤t â†’ VAR â†’ poor forecasts</p></div>
        <div class="slide"><h1>PART 3: CRITICAL DISCUSSION &amp; LESSONS LEARNED</h1></div>
        <div class="slide"><h2>Slide 11: Lessons Learned for Advanced Time Series</h2>
<h3>ğŸ“ Was haben wir aus 5 Time Seriesn gelernt?</h3>
<h4>1. <strong>Data Quality beats Fancy Models</strong></h4>
<ul>
<li>Wind Offshore: RÂ² from -36.4 to ~0.85 only durch Data Cleaning</li>
<li>9.6-Months Outage â†’ 37.9% dataverlust</li>
<li>â†’ <strong>Invest more in EDA than Model Tuning!</strong></li>
</ul>
<h4>2. <strong>Deep Learning is NICHT universell - 5 Archetypeen validiert!</strong> ğŸ­</h4>
<ul>
<li><strong>Solar (Archetype 1):</strong> Bi-LSTM 0.9955 &gt; LightGBM 0.9838 (+1.2%) âœ…</li>
<li><strong>Consumption (Archetype 2):</strong> GRU 0.9874 &gt; LightGBM 0.95 (+3.7%) âœ…âœ…</li>
<li><strong>Wind Onshore (Archetype 3):</strong> LSTM 0.9548 &lt;&lt; RF 0.9997 (-4.7%) âŒ</li>
<li><strong>Price (Archetype 4):</strong> GRU 0.8906 &lt;&lt; LightGBM 0.9798 (-9%) âŒâŒ</li>
<li><strong>Wind Offshore (Archetype 5):</strong> GRU 0.33 &lt;&lt; XGBoost ~0.85 (-61%) âŒâŒâŒ</li>
<li>â†’ <strong>Pattern recognized: The weaker ML, desto more hilft DL!</strong></li>
</ul>
<h4>3. <strong>GRU is der unterschÃ¤tzte Champion!</strong> ğŸ†• ğŸ†</h4>
<ul>
<li><strong>Consumption:</strong> GRU 0.9874 &gt; Bi-LSTM 0.9799 (+0.75%, 2x faster)</li>
<li><strong>Price:</strong> GRU 0.8906 &gt; Bi-LSTM 0.8853 (+0.53%, 7x faster)</li>
<li><strong>Wind Offshore:</strong> GRU 0.33 &gt; LSTM 0.08 (+328%!)</li>
<li>Simpler (2 Gates), faster, robuster bei Volatility</li>
<li>â†’ <strong>Probiere GRU BEVOR du zu Bi-LSTM greifst!</strong></li>
</ul>
<h4>4. <strong>Random Forest: The silent winner with chaos</strong></h4>
<ul>
<li>Wind Onshore: RÂ²=0.9997 (better than JEDES DL-Model)</li>
<li>Robust against stochasticity, no GPU needed</li>
<li>â†’ <strong>Bei ACF &lt; 0.3: RF than First Choice!</strong></li>
</ul>
<h4>5. <strong>"State-of-the-Art" versagt konsisent bei energy data</strong> âŒ</h4>
<ul>
<li><strong>N-BEATS:</strong> -18.93 (Solar), -0.94 (Cons), -4.63 (Wind On), -3.16 (Price), <strong>-12.49 (Wind Off)</strong></li>
<li><strong>N-HiTS:</strong> -4.22 (Solar), -9.58 (Cons), -1.02Ã—10Â²â°Â¹ (Wind On), -3.74 (Price), <strong>-100.41 (Wind Off)</strong></li>
<li><strong>DeepAR:</strong> -1.24 (Cons), -1.03 (Wind On), -1.16 (Price), <strong>-7.11 (Wind Off)</strong></li>
<li><strong>5/5 Time Seriesn:</strong> All SOTA models negative!</li>
<li>â†’ <strong>SOTA â‰  Production-ready! Always benchmark yourself!</strong></li>
</ul>
<h4>6. <strong>BidirectionalitÃ¤t hilft only bei symmetry</strong></h4>
<ul>
<li>Solar (symmetric): Bi-LSTM &gt; GRU (+1.4%)</li>
<li>Consumption (sequential): GRU &gt; Bi-LSTM (+0.75%)</li>
<li>Wind Offshore (fragmented): GRU &gt; Bi-LSTM (+55%!)</li>
<li>â†’ <strong>Pattern-Type bestimmt Architecture-Wahl!</strong></li>
</ul>
<h4>7. <strong>DL-ROI korreliert negative mit ML-Performance</strong></h4>
<ul>
<li>Consumption: ML weak (0.95) â†’ DL advantage large (+3.7%)</li>
<li>Solar: ML strong (0.9838) â†’ DL advantage small (+1.2%)</li>
<li>Price: ML perfect (0.9798) â†’ DL fails (-9%)</li>
<li>â†’ <strong>Wenn ML schon gut is, DL brings little!</strong></li>
</ul>
<h4>8. <strong>Structural breaks brauchen separate Behandlung</strong></h4>
<ul>
<li>Wind Offshore: Outage period destroys training</li>
<li>Solution: Binary Classifier ("running?") + Regressor ("how much?")</li>
<li>â†’ <strong>Domain knowledge &gt; algorithms!</strong></li>
</ul>
<h4>9. <strong>Training time â‰  Performance</strong></h4>
<ul>
<li>N-BEATS: 733s â†’ RÂ²=-12.49 âŒ</li>
<li>GRU: 13s â†’ RÂ²=0.33 âœ… (56x faster!)</li>
<li>â†’ <strong>Fast iterieren &gt; slow "perfect model"!</strong></li>
</ul>
<h4>10. <strong>Exogenous features are critical for renewables</strong></h4>
<ul>
<li>Wind Offshore RÂ²=0.33 without wind speed</li>
<li>Expectation: RÂ²~0.90+ with weather APIs</li>
<li>â†’ <strong>Invest in data sourcing!</strong></li>
</ul>
<h3>ğŸ”® Next Steps</h3>
<ol>
<li>âœ… <strong>All 5 Time Seriesn tested</strong> - DL-Archetypeen validiert!</li>
<li>ğŸ¯ <strong>GRU-First Strategy</strong> - GRU than default for new Time Seriesn</li>
<li>ğŸ”„ <strong>Ensemble:</strong> GRU + LightGBM (temporal + features)</li>
<li>ğŸ“Š <strong>ACF-Based Routing:</strong> Automatische Modelwahl</li>
<li>ğŸŒ <strong>Exogenous Features:</strong> Weather-APIs integrieren (Wind, Solar irradiance)</li>
<li>ğŸ­ <strong>Production:</strong> Binary Classifier + Regressor for Wind Offshore</li>
<li>ğŸ”§ <strong>SOTA-Debug:</strong> Can we bring N-BEATS/N-HiTS rescue? (possibly not worthwhile)</li>
</ol>
<h3>ğŸ’¡ Open Questions for Diskussion</h3>
<ol>
<li><strong>Why is GRU so viel better than Bi-LSTM bei fragmenteden data?</strong></li>
<li>Wind Offshore: +328%! (0.33 vs 0.08)</li>
<li>
<p>Simplicity = robustness?</p>
</li>
<li>
<p><strong>Why do SOTA models SO konsisent?</strong></p>
</li>
<li>5/5 Time Seriesn negative</li>
<li>Univariatee optimization vs Feature-Rich energy data?</li>
<li>
<p>Fundamental fthanch for energy?</p>
</li>
<li>
<p><strong>Can we bring Wind Offshore to 0.85+ ?</strong></p>
</li>
<li>Exogene Features (Wind speed, Direction)?</li>
<li>Separate Outage Prediction?</li>
<li>
<p>Hybrid-Model (Binary + Regressor)?</p>
</li>
<li>
<p><strong>GRU + LightGBM Ensemble = 0.99+?</strong></p>
</li>
<li>GRU learns temporal (0.9874)</li>
<li>LightGBM learns features (0.95)</li>
<li>
<p>Different errors â†’ Combination?</p>
</li>
<li>
<p><strong>Transfer Learning zwischen Archetypeen?</strong></p>
</li>
<li>Solar-Bi-LSTM â†’ other PV? âœ…</li>
<li>Consumption-GRU â†’ other countries? âœ…</li>
<li>Zwischen Archetypeen? âŒ (too different)</li>
</ol></div>
        <div class="slide"><h2>ğŸ“š Referenzen &amp; Sourcen</h2>
<ol>
<li><strong>data:</strong> SMARD.de, ENTSO-E Transparency Platform, EPEX Spot</li>
<li><strong>Frameworks:</strong> scikit-learn, XGBoost, LightGBM, TensorFlow/Keras</li>
<li><strong>Literature:</strong></li>
<li>Hyndman &amp; Athanasopoulos (2021): "Forecasting: Principles and Practice"</li>
<li>Hochreiter &amp; Schmidhuber (1997): "Long Short-Term Memory"</li>
<li>Ke et al. (2017): "LightGBM"</li>
</ol></div>
        <div class="slide"><h1>ğŸ¤ THANK YOU FOR YOUR ATTENTION!</h1>
<p><strong>Questions? Discussion?</strong></p>
<p><strong>Key Takeaway:</strong> 5 Time Seriesn â†’ 5 Archetypeen â†’ No universal solution!<br />
<strong>Practical advice:</strong> Test GRU, LightGBM, Random Forest in this order.<br />
<strong>Most important lesson:</strong> Data Quality &gt; Model Complexity (Wind Offshore +36.4 RÂ² through cleaning!)</p></div>
    </div>
    
    <div class="controls">
        <button onclick="prevSlide()" title="Previous Slide (â†)">â—€ Back</button>
        <button onclick="nextSlide()" title="Next Slide (â†’)">Next â–¶</button>
    </div>
    
    <div class="slide-number">
        <span id="current">1</span> / <span id="total">18</span>
    </div>
    
    <script>
        let currentSlide = 0;
        const slides = document.querySelectorAll('.slide');
        const totalSlides = slides.length;
        
        document.getElementById('total').textContent = totalSlides;
        updateProgress();
        
        function showSlide(n) {
            slides[currentSlide].classList.remove('active');
            currentSlide = (n + totalSlides) % totalSlides;
            slides[currentSlide].classList.add('active');
            document.getElementById('current').textContent = currentSlide + 1;
            updateProgress();
        }
        
        function updateProgress() {
            const progress = ((currentSlide + 1) / totalSlides) * 100;
            document.getElementById('progress').style.width = progress + '%';
        }
        
        function nextSlide() { showSlide(currentSlide + 1); }
        function prevSlide() { showSlide(currentSlide - 1); }
        
        // Keyboard navigation
        document.addEventListener('keydown', (e) => {
            if (e.key === 'ArrowRight' || e.key === ' ' || e.key === 'Enter') {
                e.preventDefault();
                nextSlide();
            }
            if (e.key === 'ArrowLeft' || e.key === 'Backspace') {
                e.preventDefault();
                prevSlide();
            }
            if (e.key === 'Home') {
                e.preventDefault();
                showSlide(0);
            }
            if (e.key === 'End') {
                e.preventDefault();
                showSlide(totalSlides - 1);
            }
        });
        
        // Touch/Swipe support
        let touchStartX = 0;
        let touchEndX = 0;
        
        document.addEventListener('touchstart', e => {
            touchStartX = e.changedTouches[0].screenX;
        });
        
        document.addEventListener('touchend', e => {
            touchEndX = e.changedTouches[0].screenX;
            handleSwipe();
        });
        
        function handleSwipe() {
            if (touchEndX < touchStartX - 50) nextSlide();
            if (touchEndX > touchStartX + 50) prevSlide();
        }
        
        // Show first slide
        showSlide(0);
    </script>
</body>
</html>