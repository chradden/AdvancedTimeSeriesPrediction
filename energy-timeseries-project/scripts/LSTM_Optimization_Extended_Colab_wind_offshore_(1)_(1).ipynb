{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3bee9d27",
      "metadata": {
        "id": "3bee9d27"
      },
      "source": [
        "# üöÄ Advanced Deep Learning Models - Extended Edition\n",
        "\n",
        "**Komplette Suite moderner DL-Architekturen mit GPU-Beschleunigung**\n",
        "\n",
        "## üìã Modelle in diesem Notebook:\n",
        "\n",
        "### Basis DL-Modelle:\n",
        "1. **LSTM** - Standard Long Short-Term Memory\n",
        "2. **Bi-LSTM** - Bidirectional LSTM\n",
        "3. **GRU** - Gated Recurrent Unit (schneller als LSTM)\n",
        "\n",
        "### Generative Modelle:\n",
        "4. **Autoencoder** - Dimensionsreduktion + Forecasting\n",
        "5. **VAE** - Variational Autoencoder (mit Unsicherheitssch√§tzung)\n",
        "6. **TimeGAN** - Generative Adversarial Network f√ºr Zeitreihen\n",
        "\n",
        "### Advanced/Transformer:\n",
        "7. **N-BEATS** - Neural Basis Expansion\n",
        "8. **N-HiTS** - Hierarchical Interpolation\n",
        "9. **DeepAR** - Amazon's probabilistisches Modell\n",
        "10. **TFT** - Temporal Fusion Transformer (State-of-the-Art)\n",
        "\n",
        "### Hinweise zur Rechenzeit (GPU T4):\n",
        "- ‚úÖ **Schnell** (<5 Min): LSTM, GRU, Bi-LSTM, Autoencoder, VAE\n",
        "- ‚ö†Ô∏è **Mittel** (5-15 Min): N-BEATS, N-HiTS, DeepAR\n",
        "- üî• **Langsam** (15-45 Min): TFT, TimeGAN\n",
        "\n",
        "**Setup:** Runtime ‚Üí Change runtime type ‚Üí GPU (T4 empfohlen, A100 f√ºr TFT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7a4be948",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a4be948",
        "outputId": "46b3d348-2a94-46e8-ac1f-812e054b7f61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.19.0\n",
            "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "\n",
            "üöÄ GPU should show above!\n"
          ]
        }
      ],
      "source": [
        "# Check GPU\n",
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
        "print(\"\\nüöÄ GPU should show above!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3607b12e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3607b12e",
        "outputId": "7517da40-9ac0-405a-c487-74393e9e42ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AdvancedTimeSeriesPrediction'...\n",
            "remote: Enumerating objects: 815, done.\u001b[K\n",
            "remote: Counting objects: 100% (133/133), done.\u001b[K\n",
            "remote: Compressing objects: 100% (90/90), done.\u001b[K\n",
            "remote: Total 815 (delta 61), reused 102 (delta 38), pack-reused 682 (from 1)\u001b[K\n",
            "Receiving objects: 100% (815/815), 73.14 MiB | 31.30 MiB/s, done.\n",
            "Resolving deltas: 100% (306/306), done.\n",
            "/content/AdvancedTimeSeriesPrediction/energy-timeseries-project\n"
          ]
        }
      ],
      "source": [
        "# Clone Repository\n",
        "!git clone https://github.com/chradden/AdvancedTimeSeriesPrediction.git\n",
        "%cd AdvancedTimeSeriesPrediction/energy-timeseries-project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "80df3cc9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80df3cc9",
        "outputId": "49898401-cf32-47a2-f50d-ecdfc840175b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Installing packages (this may take 2-3 minutes)...\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m857.3/857.3 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: darts 0.40.0 does not provide the extra 'torch'\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m204.7/204.7 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m825.4/825.4 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h‚úÖ Installation complete!\n"
          ]
        }
      ],
      "source": [
        "# Install ALL Dependencies\n",
        "print(\"üì¶ Installing packages (this may take 2-3 minutes)...\")\n",
        "!pip install -q pandas numpy matplotlib seaborn scikit-learn\n",
        "!pip install -q tensorflow keras pytorch-lightning\n",
        "!pip install -q 'darts[torch]'  # N-BEATS, N-HiTS, TFT, DeepAR\n",
        "!pip install -q gluonts  # DeepAR alternative\n",
        "print(\"‚úÖ Installation complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "813e5d25",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "813e5d25",
        "outputId": "e8a4f666-2fdd-4bde-d0f6-1d5664d389bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ GPU configured: 1 device(s)\n",
            "\n",
            "üìä All imports successful!\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set seeds\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# GPU Config\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(f\"‚úÖ GPU configured: {len(gpus)} device(s)\")\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No GPU found - training will be slow!\")\n",
        "\n",
        "print(\"\\nüìä All imports successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f508784",
      "metadata": {
        "id": "1f508784"
      },
      "source": [
        "## ‚öôÔ∏è Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ea77c9d6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea77c9d6",
        "outputId": "1264c4e6-6bbd-4898-a72c-9862dac01f67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Zeitreihe: WIND_OFFSHORE\n",
            "‚ö†Ô∏è  OUTAGE FILTERING AKTIV f√ºr Wind Offshore!\n",
            "   Threshold: < 10 MW werden entfernt\n",
            "\n",
            "üéØ Aktivierte Modelle:\n",
            "   ‚úÖ Basic DL (LSTM, GRU, Bi-LSTM)\n",
            "   ‚úÖ Generative (Autoencoder, VAE)\n",
            "   ‚úÖ Advanced (N-BEATS, N-HiTS)\n",
            "   ‚úÖ DeepAR (probabilistisch)\n",
            "\n",
            "‚úÖ Konfiguration abgeschlossen!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "SERIES_NAME = 'wind_offshore'  # √Ñndern: 'solar', 'wind_offshore', 'wind_onshore', 'price', 'consumption'\n",
        "\n",
        "# ‚ö†Ô∏è CRITICAL FOR WIND OFFSHORE: Stillstand Detection\n",
        "# Wind Offshore hatte 9.8 Monate Stillstand (Apr 2023 - Feb 2024)\n",
        "# Wenn SERIES_NAME = 'wind_offshore' ‚Üí automatisch Stillstand filtern\n",
        "FILTER_OUTAGE = (SERIES_NAME == 'wind_offshore')\n",
        "OUTAGE_THRESHOLD = 10  # MW - Werte unter 10 MW als Stillstand klassifizieren\n",
        "\n",
        "# Model Selection (setze auf False um Modelle zu √ºberspringen)\n",
        "RUN_BASIC = True          # LSTM, GRU, Bi-LSTM (~5 min)\n",
        "RUN_GENERATIVE = True     # Autoencoder, VAE (~5 min)\n",
        "RUN_GAN = False           # TimeGAN (~30 min, experimentell)\n",
        "RUN_ADVANCED = True       # N-BEATS, N-HiTS (~10 min)\n",
        "RUN_PROBABILISTIC = True  # DeepAR (~10 min)\n",
        "RUN_TFT = False           # Temporal Fusion Transformer (~30-45 min)\n",
        "\n",
        "print(f\"üìä Zeitreihe: {SERIES_NAME.upper()}\")\n",
        "if FILTER_OUTAGE:\n",
        "    print(f\"‚ö†Ô∏è  OUTAGE FILTERING AKTIV f√ºr Wind Offshore!\")\n",
        "    print(f\"   Threshold: < {OUTAGE_THRESHOLD} MW werden entfernt\")\n",
        "print(f\"\\nüéØ Aktivierte Modelle:\")\n",
        "if RUN_BASIC: print(\"   ‚úÖ Basic DL (LSTM, GRU, Bi-LSTM)\")\n",
        "if RUN_GENERATIVE: print(\"   ‚úÖ Generative (Autoencoder, VAE)\")\n",
        "if RUN_GAN: print(\"   ‚úÖ TimeGAN (experimentell, ~30 min)\")\n",
        "if RUN_ADVANCED: print(\"   ‚úÖ Advanced (N-BEATS, N-HiTS)\")\n",
        "if RUN_PROBABILISTIC: print(\"   ‚úÖ DeepAR (probabilistisch)\")\n",
        "if RUN_TFT: print(\"   ‚úÖ TFT (State-of-the-Art, ~30-45 min)\")\n",
        "\n",
        "print(f\"\\n‚úÖ Konfiguration abgeschlossen!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "587212fb",
      "metadata": {
        "id": "587212fb"
      },
      "source": [
        "## üìÇ Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "fb4e8e25",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb4e8e25",
        "outputId": "8c6d3e53-4c2a-4088-fcc4-4bcb429f2c2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Data loaded for: WIND_OFFSHORE\n",
            "   Train: 7744 | Val: 1659 | Test: 1660\n",
            "\n",
            "‚ö†Ô∏è  CHECKING FOR OUTAGE DATA (< 10 MW)...\n",
            "   ‚ÑπÔ∏è  INFO: No outage data found (< 10 MW)\n",
            "   ‚ÑπÔ∏è  Die Daten wurden bereits in der Preprocessing-Pipeline gefiltert!\n",
            "   ‚ÑπÔ∏è  Original hatte 9.8 Monate Stillstand (Apr 2023 - Feb 2024)\n",
            "   ‚úÖ Nutze bereits bereinigte Daten f√ºr Training\n",
            "\n",
            "üìä Final dataset sizes:\n",
            "   Train: 7744 | Val: 1659 | Test: 1660\n",
            "   Value column: value\n",
            "   Features: 31\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "train_df = pd.read_csv(f'data/processed/{SERIES_NAME}_train.csv')\n",
        "val_df = pd.read_csv(f'data/processed/{SERIES_NAME}_val.csv')\n",
        "test_df = pd.read_csv(f'data/processed/{SERIES_NAME}_test.csv')\n",
        "\n",
        "# Determine value column\n",
        "value_col = [c for c in train_df.columns if c in ['solar', 'price', 'value',\n",
        "                                                     'wind_offshore', 'wind_onshore', 'consumption']][0]\n",
        "feature_cols = [c for c in train_df.columns if c not in ['timestamp', value_col]]\n",
        "\n",
        "print(f\"üìÇ Data loaded for: {SERIES_NAME.upper()}\")\n",
        "print(f\"   Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}\")\n",
        "\n",
        "# ‚ö†Ô∏è WIND OFFSHORE OUTAGE FILTERING\n",
        "if FILTER_OUTAGE:\n",
        "    print(f\"\\n‚ö†Ô∏è  CHECKING FOR OUTAGE DATA (< {OUTAGE_THRESHOLD} MW)...\")\n",
        "\n",
        "    # Check how many values are below threshold\n",
        "    train_outage = (train_df[value_col] < OUTAGE_THRESHOLD).sum()\n",
        "    val_outage = (val_df[value_col] < OUTAGE_THRESHOLD).sum()\n",
        "    test_outage = (test_df[value_col] < OUTAGE_THRESHOLD).sum()\n",
        "    total_outage = train_outage + val_outage + test_outage\n",
        "\n",
        "    if total_outage == 0:\n",
        "        print(f\"   ‚ÑπÔ∏è  INFO: No outage data found (< {OUTAGE_THRESHOLD} MW)\")\n",
        "        print(f\"   ‚ÑπÔ∏è  Die Daten wurden bereits in der Preprocessing-Pipeline gefiltert!\")\n",
        "        print(f\"   ‚ÑπÔ∏è  Original hatte 9.8 Monate Stillstand (Apr 2023 - Feb 2024)\")\n",
        "        print(f\"   ‚úÖ Nutze bereits bereinigte Daten f√ºr Training\")\n",
        "    else:\n",
        "        print(f\"   ‚ö†Ô∏è  OUTAGE DATA FOUND - FILTERING...\")\n",
        "\n",
        "        # Filter each set\n",
        "        train_before = len(train_df)\n",
        "        val_before = len(val_df)\n",
        "        test_before = len(test_df)\n",
        "\n",
        "        train_df = train_df[train_df[value_col] >= OUTAGE_THRESHOLD].reset_index(drop=True)\n",
        "        val_df = val_df[val_df[value_col] >= OUTAGE_THRESHOLD].reset_index(drop=True)\n",
        "        test_df = test_df[test_df[value_col] >= OUTAGE_THRESHOLD].reset_index(drop=True)\n",
        "\n",
        "        train_pct_removed = 100*(train_before - len(train_df))/train_before\n",
        "        val_pct_removed = 100*(val_before - len(val_df))/val_before\n",
        "        test_pct_removed = 100*(test_before - len(test_df))/test_before\n",
        "\n",
        "        print(f\"   Train: {train_before} ‚Üí {len(train_df)} (-{train_before - len(train_df)}, {train_pct_removed:.1f}%)\")\n",
        "        print(f\"   Val:   {val_before} ‚Üí {len(val_df)} (-{val_before - len(val_df)}, {val_pct_removed:.1f}%)\")\n",
        "        print(f\"   Test:  {test_before} ‚Üí {len(test_df)} (-{test_before - len(test_df)}, {test_pct_removed:.1f}%)\")\n",
        "\n",
        "        # Warning for excessive data loss\n",
        "        if train_pct_removed > 30 or val_pct_removed > 30 or test_pct_removed > 30:\n",
        "            print(f\"   ‚ö†Ô∏è  WARNING: >30% data removed! Results may be less reliable.\")\n",
        "        else:\n",
        "            print(f\"   ‚úÖ Outage periods removed!\")\n",
        "\n",
        "print(f\"\\nüìä Final dataset sizes:\")\n",
        "print(f\"   Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}\")\n",
        "print(f\"   Value column: {value_col}\")\n",
        "print(f\"   Features: {len(feature_cols)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2eec36f1",
      "metadata": {
        "id": "2eec36f1"
      },
      "source": [
        "## üîß Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a0a03164",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0a03164",
        "outputId": "658c51e6-3434-48d1-a61e-ae142c26532b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Data prepared:\n",
            "   X_train_seq: (7720, 24, 31)\n",
            "   y_test_seq: (1636,)\n"
          ]
        }
      ],
      "source": [
        "def create_sequences(data, target, seq_length):\n",
        "    \"\"\"Create sequences for RNN models\"\"\"\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i + seq_length])\n",
        "        y.append(target[i + seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Scale data\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "X_train = scaler_X.fit_transform(train_df[feature_cols])\n",
        "y_train = scaler_y.fit_transform(train_df[[value_col]])\n",
        "\n",
        "X_val = scaler_X.transform(val_df[feature_cols])\n",
        "y_val = scaler_y.transform(val_df[[value_col]])\n",
        "\n",
        "X_test = scaler_X.transform(test_df[feature_cols])\n",
        "y_test_orig = test_df[value_col].values\n",
        "\n",
        "# Create sequences\n",
        "seq_length = 24\n",
        "X_train_seq, y_train_seq = create_sequences(X_train, y_train.flatten(), seq_length)\n",
        "X_val_seq, y_val_seq = create_sequences(X_val, y_val.flatten(), seq_length)\n",
        "X_test_seq, _ = create_sequences(X_test, np.zeros(len(X_test)), seq_length)\n",
        "y_test_seq = y_test_orig[seq_length:]\n",
        "\n",
        "print(f\"‚úÖ Data prepared:\")\n",
        "print(f\"   X_train_seq: {X_train_seq.shape}\")\n",
        "print(f\"   y_test_seq: {y_test_seq.shape}\")\n",
        "\n",
        "# Storage for results\n",
        "all_results = []"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b30aa764",
      "metadata": {
        "id": "b30aa764"
      },
      "source": [
        "---\n",
        "# üîµ BASIC MODELS\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38f42898",
      "metadata": {
        "id": "38f42898"
      },
      "source": [
        "## üß™ Model 1: LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3168efd2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3168efd2",
        "outputId": "fac9f700-889e-4bfa-8ea5-0406c0977a5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "üß™ MODEL 1: LSTM\n",
            "================================================================================\n",
            "\n",
            "üìä LSTM RESULTS:\n",
            "   R¬≤ = 0.0768\n",
            "   RMSE = 144.75\n",
            "   MAE = 87.81\n",
            "   Time = 15.4s\n"
          ]
        }
      ],
      "source": [
        "if RUN_BASIC:\n",
        "    print(\"=\"*80)\n",
        "    print(\"üß™ MODEL 1: LSTM\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Build\n",
        "    model_lstm = keras.Sequential([\n",
        "        layers.LSTM(64, activation='relu', return_sequences=False,\n",
        "                   input_shape=(seq_length, len(feature_cols))),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    model_lstm.compile(optimizer=keras.optimizers.Adam(0.001), loss='mse')\n",
        "\n",
        "    # Train\n",
        "    start = time.time()\n",
        "    history = model_lstm.fit(\n",
        "        X_train_seq, y_train_seq,\n",
        "        validation_data=(X_val_seq, y_val_seq),\n",
        "        epochs=100, batch_size=64,\n",
        "        callbacks=[\n",
        "            EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
        "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
        "        ],\n",
        "        verbose=0\n",
        "    )\n",
        "    train_time = time.time() - start\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred_scaled = model_lstm.predict(X_test_seq, verbose=0)\n",
        "    y_pred = scaler_y.inverse_transform(y_pred_scaled).flatten()\n",
        "\n",
        "    r2 = r2_score(y_test_seq, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test_seq, y_pred))\n",
        "    mae = mean_absolute_error(y_test_seq, y_pred)\n",
        "\n",
        "    print(f\"\\nüìä LSTM RESULTS:\")\n",
        "    print(f\"   R¬≤ = {r2:.4f}\")\n",
        "    print(f\"   RMSE = {rmse:.2f}\")\n",
        "    print(f\"   MAE = {mae:.2f}\")\n",
        "    print(f\"   Time = {train_time:.1f}s\")\n",
        "\n",
        "    all_results.append({\n",
        "        'Model': 'LSTM',\n",
        "        'R¬≤': r2,\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'Time (s)': train_time\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07b77a30",
      "metadata": {
        "id": "07b77a30"
      },
      "source": [
        "## üß™ Model 2: GRU (‚≠ê NEU!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d7f85ea4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7f85ea4",
        "outputId": "e28130ff-904b-4d0d-cab4-146be0c8d945"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "üß™ MODEL 2: GRU (Gated Recurrent Unit)\n",
            "================================================================================\n",
            "\n",
            "üìä GRU RESULTS:\n",
            "   R¬≤ = 0.3292\n",
            "   RMSE = 123.39\n",
            "   MAE = 87.69\n",
            "   Time = 13.1s\n",
            "\n",
            "üí° GRU vs LSTM: 14.9% faster!\n"
          ]
        }
      ],
      "source": [
        "if RUN_BASIC:\n",
        "    print(\"=\"*80)\n",
        "    print(\"üß™ MODEL 2: GRU (Gated Recurrent Unit)\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Build GRU (schneller als LSTM!)\n",
        "    model_gru = keras.Sequential([\n",
        "        layers.GRU(64, activation='relu', return_sequences=False,\n",
        "                  input_shape=(seq_length, len(feature_cols))),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    model_gru.compile(optimizer=keras.optimizers.Adam(0.001), loss='mse')\n",
        "\n",
        "    # Train\n",
        "    start = time.time()\n",
        "    history = model_gru.fit(\n",
        "        X_train_seq, y_train_seq,\n",
        "        validation_data=(X_val_seq, y_val_seq),\n",
        "        epochs=100, batch_size=64,\n",
        "        callbacks=[\n",
        "            EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
        "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
        "        ],\n",
        "        verbose=0\n",
        "    )\n",
        "    train_time_gru = time.time() - start\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred_gru_scaled = model_gru.predict(X_test_seq, verbose=0)\n",
        "    y_pred_gru = scaler_y.inverse_transform(y_pred_gru_scaled).flatten()\n",
        "\n",
        "    r2_gru = r2_score(y_test_seq, y_pred_gru)\n",
        "    rmse_gru = np.sqrt(mean_squared_error(y_test_seq, y_pred_gru))\n",
        "    mae_gru = mean_absolute_error(y_test_seq, y_pred_gru)\n",
        "\n",
        "    print(f\"\\nüìä GRU RESULTS:\")\n",
        "    print(f\"   R¬≤ = {r2_gru:.4f}\")\n",
        "    print(f\"   RMSE = {rmse_gru:.2f}\")\n",
        "    print(f\"   MAE = {mae_gru:.2f}\")\n",
        "    print(f\"   Time = {train_time_gru:.1f}s\")\n",
        "    print(f\"\\nüí° GRU vs LSTM: {((train_time - train_time_gru) / train_time * 100):.1f}% faster!\")\n",
        "\n",
        "    all_results.append({\n",
        "        'Model': 'GRU',\n",
        "        'R¬≤': r2_gru,\n",
        "        'RMSE': rmse_gru,\n",
        "        'MAE': mae_gru,\n",
        "        'Time (s)': train_time_gru\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53a5145e",
      "metadata": {
        "id": "53a5145e"
      },
      "source": [
        "## üß™ Model 3: Bi-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b3823262",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3823262",
        "outputId": "7cfba82c-eee8-4c1f-9298-4b6ba355b6de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "üß™ MODEL 3: Bi-LSTM (Bidirectional)\n",
            "================================================================================\n",
            "\n",
            "üìä BI-LSTM RESULTS:\n",
            "   R¬≤ = 0.2114\n",
            "   RMSE = 133.78\n",
            "   MAE = 95.82\n",
            "   Time = 30.7s\n"
          ]
        }
      ],
      "source": [
        "if RUN_BASIC:\n",
        "    print(\"=\"*80)\n",
        "    print(\"üß™ MODEL 3: Bi-LSTM (Bidirectional)\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Build\n",
        "    model_bilstm = keras.Sequential([\n",
        "        layers.Bidirectional(layers.LSTM(64, activation='relu', return_sequences=True),\n",
        "                           input_shape=(seq_length, len(feature_cols))),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Bidirectional(layers.LSTM(32, activation='relu')),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(16, activation='relu'),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    model_bilstm.compile(optimizer=keras.optimizers.Adam(0.001), loss='mse')\n",
        "\n",
        "    # Train\n",
        "    start = time.time()\n",
        "    history = model_bilstm.fit(\n",
        "        X_train_seq, y_train_seq,\n",
        "        validation_data=(X_val_seq, y_val_seq),\n",
        "        epochs=100, batch_size=64,\n",
        "        callbacks=[\n",
        "            EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
        "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
        "        ],\n",
        "        verbose=0\n",
        "    )\n",
        "    train_time_bilstm = time.time() - start\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred_bilstm_scaled = model_bilstm.predict(X_test_seq, verbose=0)\n",
        "    y_pred_bilstm = scaler_y.inverse_transform(y_pred_bilstm_scaled).flatten()\n",
        "\n",
        "    r2_bilstm = r2_score(y_test_seq, y_pred_bilstm)\n",
        "    rmse_bilstm = np.sqrt(mean_squared_error(y_test_seq, y_pred_bilstm))\n",
        "    mae_bilstm = mean_absolute_error(y_test_seq, y_pred_bilstm)\n",
        "\n",
        "    print(f\"\\nüìä BI-LSTM RESULTS:\")\n",
        "    print(f\"   R¬≤ = {r2_bilstm:.4f}\")\n",
        "    print(f\"   RMSE = {rmse_bilstm:.2f}\")\n",
        "    print(f\"   MAE = {mae_bilstm:.2f}\")\n",
        "    print(f\"   Time = {train_time_bilstm:.1f}s\")\n",
        "\n",
        "    all_results.append({\n",
        "        'Model': 'Bi-LSTM',\n",
        "        'R¬≤': r2_bilstm,\n",
        "        'RMSE': rmse_bilstm,\n",
        "        'MAE': mae_bilstm,\n",
        "        'Time (s)': train_time_bilstm\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e81bdb7d",
      "metadata": {
        "id": "e81bdb7d"
      },
      "source": [
        "---\n",
        "# üü¢ GENERATIVE MODELS\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76712816",
      "metadata": {
        "id": "76712816"
      },
      "source": [
        "## üß™ Model 4: Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d23f548f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d23f548f",
        "outputId": "5e7c2180-3922-4448-fd3f-7acfe8131734"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "üß™ MODEL 4: Autoencoder\n",
            "================================================================================\n",
            "\n",
            "üìä AUTOENCODER RESULTS:\n",
            "   R¬≤ = -0.5682\n",
            "   RMSE = 188.65\n",
            "   MAE = 145.56\n",
            "   Time = 79.5s\n"
          ]
        }
      ],
      "source": [
        "if RUN_GENERATIVE:\n",
        "    print(\"=\"*80)\n",
        "    print(\"üß™ MODEL 4: Autoencoder\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Build Autoencoder\n",
        "    encoding_dim = 32\n",
        "    input_ae = layers.Input(shape=(seq_length, len(feature_cols)))\n",
        "    encoded = layers.LSTM(64, activation='relu', return_sequences=True)(input_ae)\n",
        "    encoded = layers.LSTM(encoding_dim, activation='relu')(encoded)\n",
        "\n",
        "    decoded = layers.RepeatVector(seq_length)(encoded)\n",
        "    decoded = layers.LSTM(64, activation='relu', return_sequences=True)(decoded)\n",
        "    decoded = layers.TimeDistributed(layers.Dense(len(feature_cols)))(decoded)\n",
        "\n",
        "    autoencoder = keras.Model(input_ae, decoded)\n",
        "    autoencoder.compile(optimizer='adam', loss='mse')\n",
        "    encoder = keras.Model(input_ae, encoded)\n",
        "\n",
        "    # Train Autoencoder\n",
        "    start = time.time()\n",
        "    autoencoder.fit(\n",
        "        X_train_seq, X_train_seq,\n",
        "        validation_data=(X_val_seq, X_val_seq),\n",
        "        epochs=50, batch_size=64,\n",
        "        callbacks=[EarlyStopping(patience=10, restore_best_weights=True)],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Train Forecast Head\n",
        "    encoded_train = encoder.predict(X_train_seq, verbose=0)\n",
        "    encoded_val = encoder.predict(X_val_seq, verbose=0)\n",
        "    encoded_test = encoder.predict(X_test_seq, verbose=0)\n",
        "\n",
        "    forecast_head = keras.Sequential([\n",
        "        layers.Dense(16, activation='relu', input_shape=(encoding_dim,)),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "    forecast_head.compile(optimizer='adam', loss='mse')\n",
        "    forecast_head.fit(\n",
        "        encoded_train, y_train_seq,\n",
        "        validation_data=(encoded_val, y_val_seq),\n",
        "        epochs=50, batch_size=64,\n",
        "        callbacks=[EarlyStopping(patience=10, restore_best_weights=True)],\n",
        "        verbose=0\n",
        "    )\n",
        "    train_time_ae = time.time() - start\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred_ae_scaled = forecast_head.predict(encoded_test, verbose=0)\n",
        "    y_pred_ae = scaler_y.inverse_transform(y_pred_ae_scaled).flatten()\n",
        "\n",
        "    r2_ae = r2_score(y_test_seq, y_pred_ae)\n",
        "    rmse_ae = np.sqrt(mean_squared_error(y_test_seq, y_pred_ae))\n",
        "    mae_ae = mean_absolute_error(y_test_seq, y_pred_ae)\n",
        "\n",
        "    print(f\"\\nüìä AUTOENCODER RESULTS:\")\n",
        "    print(f\"   R¬≤ = {r2_ae:.4f}\")\n",
        "    print(f\"   RMSE = {rmse_ae:.2f}\")\n",
        "    print(f\"   MAE = {mae_ae:.2f}\")\n",
        "    print(f\"   Time = {train_time_ae:.1f}s\")\n",
        "\n",
        "    all_results.append({\n",
        "        'Model': 'Autoencoder',\n",
        "        'R¬≤': r2_ae,\n",
        "        'RMSE': rmse_ae,\n",
        "        'MAE': mae_ae,\n",
        "        'Time (s)': train_time_ae\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64d2ce7a",
      "metadata": {
        "id": "64d2ce7a"
      },
      "source": [
        "## üß™ Model 5: VAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "712a98ce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "712a98ce",
        "outputId": "264fa35c-7eb0-44d9-a42b-b004783da848"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "üß™ MODEL 5: VAE (Variational Autoencoder)\n",
            "================================================================================\n",
            "\n",
            "üìä VAE RESULTS:\n",
            "   R¬≤ = -6.7963\n",
            "   RMSE = 420.64\n",
            "   MAE = 361.24\n",
            "   Time = 83.0s\n"
          ]
        }
      ],
      "source": [
        "if RUN_GENERATIVE:\n",
        "    print(\"=\"*80)\n",
        "    print(\"üß™ MODEL 5: VAE (Variational Autoencoder)\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Build VAE parts as separate functional models\n",
        "    latent_dim = 32\n",
        "\n",
        "    # Encoder Model\n",
        "    input_vae_enc = layers.Input(shape=(seq_length, len(feature_cols)))\n",
        "    x_enc = layers.LSTM(64, activation='relu', return_sequences=True)(input_vae_enc)\n",
        "    x_enc = layers.LSTM(64, activation='relu')(x_enc)\n",
        "\n",
        "    z_mean = layers.Dense(latent_dim)(x_enc)\n",
        "    z_log_var = layers.Dense(latent_dim)(x_enc)\n",
        "\n",
        "    def sampling(args):\n",
        "        z_mean, z_log_var = args\n",
        "        epsilon = tf.random.normal(shape=tf.shape(z_mean))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "    z = layers.Lambda(sampling)([z_mean, z_log_var])\n",
        "\n",
        "    encoder_model = keras.Model(input_vae_enc, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "\n",
        "    # Decoder Model\n",
        "    decoder_input = layers.Input(shape=(latent_dim,))\n",
        "    x_decoded = layers.RepeatVector(seq_length)(decoder_input)\n",
        "    x_decoded = layers.LSTM(64, activation='relu', return_sequences=True)(x_decoded)\n",
        "    x_decoded = layers.TimeDistributed(layers.Dense(len(feature_cols)))(x_decoded)\n",
        "    decoder_model = keras.Model(decoder_input, x_decoded, name=\"decoder\")\n",
        "\n",
        "    # Custom VAE Model subclass\n",
        "    class CustomVAE(keras.Model):\n",
        "        def __init__(self, encoder, decoder, **kwargs):\n",
        "            super().__init__(**kwargs)\n",
        "            self.encoder = encoder\n",
        "            self.decoder = decoder\n",
        "            self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "            self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
        "            self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "        def call(self, inputs):\n",
        "            _, _, z = self.encoder(inputs)\n",
        "            reconstruction = self.decoder(z)\n",
        "            return reconstruction\n",
        "\n",
        "        def train_step(self, data):\n",
        "            # Unpack the data. We use X_train_seq as both input and target for reconstruction.\n",
        "            if isinstance(data, tuple):\n",
        "                inputs, _ = data\n",
        "            else:\n",
        "                inputs = data\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                z_mean, z_log_var, z = self.encoder(inputs)\n",
        "                reconstruction = self.decoder(z)\n",
        "\n",
        "                # Calculate reconstruction loss\n",
        "                reconstruction_loss = keras.ops.mean(keras.ops.sum(keras.losses.mse(inputs, reconstruction), axis=-1))\n",
        "\n",
        "                # Calculate KL divergence loss\n",
        "                kl_loss = -0.5 * keras.ops.mean(1 + z_log_var - keras.ops.square(z_mean) - keras.ops.exp(z_log_var))\n",
        "\n",
        "                total_loss = reconstruction_loss + kl_loss\n",
        "\n",
        "            grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "            self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "\n",
        "            self.total_loss_tracker.update_state(total_loss)\n",
        "            self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "            self.kl_loss_tracker.update_state(kl_loss)\n",
        "\n",
        "            return {\n",
        "                \"loss\": self.total_loss_tracker.result(),\n",
        "                \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "                \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "            }\n",
        "\n",
        "        @property\n",
        "        def metrics(self):\n",
        "            return [\n",
        "                self.total_loss_tracker,\n",
        "                self.reconstruction_loss_tracker,\n",
        "                self.kl_loss_tracker,\n",
        "            ]\n",
        "\n",
        "    # Instantiate the custom VAE model\n",
        "    vae = CustomVAE(encoder_model, decoder_model)\n",
        "    # Provide a dummy loss function to satisfy the compile() check.\n",
        "    # The actual loss computation is handled within the custom train_step.\n",
        "    vae.compile(optimizer='adam', loss=keras.losses.MeanSquaredError())\n",
        "\n",
        "    # The encoder used for generating latent space representations for the forecast head\n",
        "    prediction_encoder_vae = keras.Model(input_vae_enc, z_mean, name=\"prediction_encoder_vae\")\n",
        "\n",
        "    # Train VAE\n",
        "    start = time.time()\n",
        "    vae.fit(\n",
        "        X_train_seq, X_train_seq, # VAE trains on its inputs\n",
        "        validation_data=(X_val_seq, X_val_seq),\n",
        "        epochs=50, batch_size=64,\n",
        "        callbacks=[EarlyStopping(patience=10, restore_best_weights=True)],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Train Forecast Head\n",
        "    encoded_vae_train = prediction_encoder_vae.predict(X_train_seq, verbose=0)\n",
        "    encoded_vae_val = prediction_encoder_vae.predict(X_val_seq, verbose=0)\n",
        "    encoded_vae_test = prediction_encoder_vae.predict(X_test_seq, verbose=0)\n",
        "\n",
        "    forecast_head_vae = keras.Sequential([\n",
        "        layers.Dense(16, activation='relu', input_shape=(latent_dim,)),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "    forecast_head_vae.compile(optimizer='adam', loss='mse')\n",
        "    forecast_head_vae.fit(\n",
        "        encoded_vae_train, y_train_seq,\n",
        "        validation_data=(encoded_vae_val, y_val_seq),\n",
        "        epochs=50, batch_size=64,\n",
        "        callbacks=[EarlyStopping(patience=10, restore_best_weights=True)],\n",
        "        verbose=0\n",
        "    )\n",
        "    train_time_vae = time.time() - start\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred_vae_scaled = forecast_head_vae.predict(encoded_vae_test, verbose=0)\n",
        "    y_pred_vae = scaler_y.inverse_transform(y_pred_vae_scaled).flatten()\n",
        "\n",
        "    r2_vae = r2_score(y_test_seq, y_pred_vae)\n",
        "    rmse_vae = np.sqrt(mean_squared_error(y_test_seq, y_pred_vae))\n",
        "    mae_vae = mean_absolute_error(y_test_seq, y_pred_vae)\n",
        "\n",
        "    print(f\"\\nüìä VAE RESULTS:\")\n",
        "    print(f\"   R¬≤ = {r2_vae:.4f}\")\n",
        "    print(f\"   RMSE = {rmse_vae:.2f}\")\n",
        "    print(f\"   MAE = {mae_vae:.2f}\")\n",
        "    print(f\"   Time = {train_time_vae:.1f}s\")\n",
        "\n",
        "    all_results.append({\n",
        "        'Model': 'VAE',\n",
        "        'R¬≤': r2_vae,\n",
        "        'RMSE': rmse_vae,\n",
        "        'MAE': mae_vae,\n",
        "        'Time (s)': train_time_vae\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f1fa817",
      "metadata": {
        "id": "1f1fa817"
      },
      "source": [
        "---\n",
        "# üî¥ ADVANCED MODELS (Darts Framework)\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "06f5ec5b",
      "metadata": {
        "id": "06f5ec5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fe72933-4901-4eaa-e374-9fb30719a44c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Darts TimeSeries prepared\n"
          ]
        }
      ],
      "source": [
        "# Prepare Darts TimeSeries (needed for N-BEATS, N-HiTS, DeepAR, TFT)\n",
        "if RUN_ADVANCED or RUN_PROBABILISTIC or RUN_TFT:\n",
        "    from darts import TimeSeries\n",
        "    from darts.dataprocessing.transformers import Scaler as DartsScaler\n",
        "\n",
        "    ts_train = TimeSeries.from_values(train_df[value_col].values)\n",
        "    ts_val = TimeSeries.from_values(val_df[value_col].values)\n",
        "    ts_test = TimeSeries.from_values(test_df[value_col].values)\n",
        "\n",
        "    scaler_darts = DartsScaler()\n",
        "    ts_train_scaled = scaler_darts.fit_transform(ts_train)\n",
        "    ts_val_scaled = scaler_darts.transform(ts_val)\n",
        "\n",
        "    print(\"‚úÖ Darts TimeSeries prepared\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d97b09f2",
      "metadata": {
        "id": "d97b09f2"
      },
      "source": [
        "## üß™ Model 6: N-BEATS (‚≠ê NEU!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "49be663f",
      "metadata": {
        "id": "49be663f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce509fa3-1b77-4690-91f6-27ece23c2ca7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:darts.models:The StatsForecast module could not be imported. To enable support for the AutoARIMA, AutoETS and Croston models, please consider installing it.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "üß™ MODEL 6: N-BEATS (Neural Basis Expansion)\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä N-BEATS RESULTS:\n",
            "   R¬≤ = -12.4851\n",
            "   RMSE = 563.17\n",
            "   MAE = 501.50\n",
            "   Time = 733.8s\n"
          ]
        }
      ],
      "source": [
        "if RUN_ADVANCED:\n",
        "    from darts.models import NBEATSModel\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"üß™ MODEL 6: N-BEATS (Neural Basis Expansion)\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    model_nbeats = NBEATSModel(\n",
        "        input_chunk_length=24,\n",
        "        output_chunk_length=1,\n",
        "        n_epochs=100,\n",
        "        batch_size=64,\n",
        "        pl_trainer_kwargs={\n",
        "            \"accelerator\": \"gpu\",\n",
        "            \"devices\": 1,\n",
        "            \"enable_progress_bar\": False\n",
        "        },\n",
        "        force_reset=True,\n",
        "        save_checkpoints=False\n",
        "    )\n",
        "\n",
        "    start = time.time()\n",
        "    model_nbeats.fit(series=ts_train_scaled, val_series=ts_val_scaled, verbose=False)\n",
        "    train_time_nbeats = time.time() - start\n",
        "\n",
        "    # Predict\n",
        "    n_pred = len(ts_test)\n",
        "    pred_nbeats_scaled = model_nbeats.predict(n=n_pred, series=ts_train_scaled)\n",
        "    pred_nbeats = scaler_darts.inverse_transform(pred_nbeats_scaled)\n",
        "\n",
        "    y_pred_nbeats = pred_nbeats.values().flatten()\n",
        "    y_test_nbeats = ts_test.values().flatten()\n",
        "    min_len = min(len(y_pred_nbeats), len(y_test_nbeats))\n",
        "    y_pred_nbeats = y_pred_nbeats[:min_len]\n",
        "    y_test_nbeats = y_test_nbeats[:min_len]\n",
        "\n",
        "    r2_nbeats = r2_score(y_test_nbeats, y_pred_nbeats)\n",
        "    rmse_nbeats = np.sqrt(mean_squared_error(y_test_nbeats, y_pred_nbeats))\n",
        "    mae_nbeats = mean_absolute_error(y_test_nbeats, y_pred_nbeats)\n",
        "\n",
        "    print(f\"\\nüìä N-BEATS RESULTS:\")\n",
        "    print(f\"   R¬≤ = {r2_nbeats:.4f}\")\n",
        "    print(f\"   RMSE = {rmse_nbeats:.2f}\")\n",
        "    print(f\"   MAE = {mae_nbeats:.2f}\")\n",
        "    print(f\"   Time = {train_time_nbeats:.1f}s\")\n",
        "\n",
        "    all_results.append({\n",
        "        'Model': 'N-BEATS',\n",
        "        'R¬≤': r2_nbeats,\n",
        "        'RMSE': rmse_nbeats,\n",
        "        'MAE': mae_nbeats,\n",
        "        'Time (s)': train_time_nbeats\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6ac4199",
      "metadata": {
        "id": "a6ac4199"
      },
      "source": [
        "## üß™ Model 7: N-HiTS (‚≠ê NEU!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "f27c5475",
      "metadata": {
        "id": "f27c5475",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6d2c7b1-4f40-4a21-fa01-001a0e2df598"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "üß™ MODEL 7: N-HiTS (Hierarchical Interpolation)\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä N-HiTS RESULTS:\n",
            "   R¬≤ = -100.4139\n",
            "   RMSE = 1544.39\n",
            "   MAE = 1519.13\n",
            "   Time = 98.4s\n"
          ]
        }
      ],
      "source": [
        "if RUN_ADVANCED:\n",
        "    from darts.models import NHiTSModel\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"üß™ MODEL 7: N-HiTS (Hierarchical Interpolation)\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    model_nhits = NHiTSModel(\n",
        "        input_chunk_length=24,\n",
        "        output_chunk_length=1,\n",
        "        n_epochs=100,\n",
        "        batch_size=64,\n",
        "        pl_trainer_kwargs={\n",
        "            \"accelerator\": \"gpu\",\n",
        "            \"devices\": 1,\n",
        "            \"enable_progress_bar\": False\n",
        "        },\n",
        "        force_reset=True,\n",
        "        save_checkpoints=False\n",
        "    )\n",
        "\n",
        "    start = time.time()\n",
        "    model_nhits.fit(series=ts_train_scaled, val_series=ts_val_scaled, verbose=False)\n",
        "    train_time_nhits = time.time() - start\n",
        "\n",
        "    # Predict\n",
        "    pred_nhits_scaled = model_nhits.predict(n=n_pred, series=ts_train_scaled)\n",
        "    pred_nhits = scaler_darts.inverse_transform(pred_nhits_scaled)\n",
        "\n",
        "    y_pred_nhits = pred_nhits.values().flatten()\n",
        "    y_test_nhits = ts_test.values().flatten()\n",
        "    min_len = min(len(y_pred_nhits), len(y_test_nhits))\n",
        "    y_pred_nhits = y_pred_nhits[:min_len]\n",
        "    y_test_nhits = y_test_nhits[:min_len]\n",
        "\n",
        "    r2_nhits = r2_score(y_test_nhits, y_pred_nhits)\n",
        "    rmse_nhits = np.sqrt(mean_squared_error(y_test_nhits, y_pred_nhits))\n",
        "    mae_nhits = mean_absolute_error(y_test_nhits, y_pred_nhits)\n",
        "\n",
        "    print(f\"\\nüìä N-HiTS RESULTS:\")\n",
        "    print(f\"   R¬≤ = {r2_nhits:.4f}\")\n",
        "    print(f\"   RMSE = {rmse_nhits:.2f}\")\n",
        "    print(f\"   MAE = {mae_nhits:.2f}\")\n",
        "    print(f\"   Time = {train_time_nhits:.1f}s\")\n",
        "\n",
        "    all_results.append({\n",
        "        'Model': 'N-HiTS',\n",
        "        'R¬≤': r2_nhits,\n",
        "        'RMSE': rmse_nhits,\n",
        "        'MAE': mae_nhits,\n",
        "        'Time (s)': train_time_nhits\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fea7afb3",
      "metadata": {
        "id": "fea7afb3"
      },
      "source": [
        "## üß™ Model 8: DeepAR (‚≠ê NEU! - Probabilistisch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "106816d8",
      "metadata": {
        "id": "106816d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "366dff7b-a80f-42e0-f78d-8a2458b4398d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "üß™ MODEL 8: DeepAR (Probabilistic Forecasting)\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä DEEPAR RESULTS:\n",
            "   R¬≤ = -7.1134\n",
            "   RMSE = 436.83\n",
            "   MAE = 383.72\n",
            "   Time = 106.6s\n",
            "\n",
            "üí° DeepAR liefert probabilistische Forecasts (Konfidenzintervalle m√∂glich!)\n"
          ]
        }
      ],
      "source": [
        "if RUN_PROBABILISTIC:\n",
        "    from darts.models import RNNModel\n",
        "    from darts.utils.likelihood_models import GaussianLikelihood # Import GaussianLikelihood\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"üß™ MODEL 8: DeepAR (Probabilistic Forecasting)\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # DeepAR via Darts' RNNModel with probabilistic output\n",
        "    model_deepar = RNNModel(\n",
        "        model='LSTM',\n",
        "        input_chunk_length=24,\n",
        "        training_length=48,\n",
        "        n_epochs=100,\n",
        "        batch_size=64,\n",
        "        hidden_dim=64,\n",
        "        n_rnn_layers=2,\n",
        "        dropout=0.2,\n",
        "        likelihood=GaussianLikelihood(),  # Pass an instance of GaussianLikelihood\n",
        "        pl_trainer_kwargs={\n",
        "            \"accelerator\": \"gpu\",\n",
        "            \"devices\": 1,\n",
        "            \"enable_progress_bar\": False\n",
        "        },\n",
        "        force_reset=True,\n",
        "        save_checkpoints=False\n",
        "    )\n",
        "\n",
        "    start = time.time()\n",
        "    model_deepar.fit(series=ts_train_scaled, val_series=ts_val_scaled, verbose=False)\n",
        "    train_time_deepar = time.time() - start\n",
        "\n",
        "    # Predict (median)\n",
        "    pred_deepar_scaled = model_deepar.predict(n=n_pred, series=ts_train_scaled, num_samples=100)\n",
        "    pred_deepar = scaler_darts.inverse_transform(pred_deepar_scaled)\n",
        "\n",
        "    y_pred_deepar = pred_deepar.values().flatten()\n",
        "    y_test_deepar = ts_test.values().flatten()\n",
        "    min_len = min(len(y_pred_deepar), len(y_test_deepar))\n",
        "    y_pred_deepar = y_pred_deepar[:min_len]\n",
        "    y_test_deepar = y_test_deepar[:min_len]\n",
        "\n",
        "    r2_deepar = r2_score(y_test_deepar, y_pred_deepar)\n",
        "    rmse_deepar = np.sqrt(mean_squared_error(y_test_deepar, y_pred_deepar))\n",
        "    mae_deepar = mean_absolute_error(y_test_deepar, y_pred_deepar)\n",
        "\n",
        "    print(f\"\\nüìä DEEPAR RESULTS:\")\n",
        "    print(f\"   R¬≤ = {r2_deepar:.4f}\")\n",
        "    print(f\"   RMSE = {rmse_deepar:.2f}\")\n",
        "    print(f\"   MAE = {mae_deepar:.2f}\")\n",
        "    print(f\"   Time = {train_time_deepar:.1f}s\")\n",
        "    print(f\"\\nüí° DeepAR liefert probabilistische Forecasts (Konfidenzintervalle m√∂glich!)\")\n",
        "\n",
        "    all_results.append({\n",
        "        'Model': 'DeepAR',\n",
        "        'R¬≤': r2_deepar,\n",
        "        'RMSE': rmse_deepar,\n",
        "        'MAE': mae_deepar,\n",
        "        'Time (s)': train_time_deepar\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96b113e4",
      "metadata": {
        "id": "96b113e4"
      },
      "source": [
        "## üß™ Model 9: TFT (‚≠ê NEU! - State-of-the-Art)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "2769f799",
      "metadata": {
        "id": "2769f799"
      },
      "outputs": [],
      "source": [
        "if RUN_TFT:\n",
        "    from darts.models import TFTModel\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"üß™ MODEL 9: TFT (Temporal Fusion Transformer)\")\n",
        "    print(\"‚ö†Ô∏è WARNING: This can take 30-45 minutes!\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    model_tft = TFTModel(\n",
        "        input_chunk_length=24,\n",
        "        output_chunk_length=1,\n",
        "        hidden_size=64,\n",
        "        lstm_layers=2,\n",
        "        num_attention_heads=4,\n",
        "        dropout=0.1,\n",
        "        batch_size=64,\n",
        "        n_epochs=100,\n",
        "        pl_trainer_kwargs={\n",
        "            \"accelerator\": \"gpu\",\n",
        "            \"devices\": 1,\n",
        "            \"enable_progress_bar\": True\n",
        "        },\n",
        "        force_reset=True,\n",
        "        save_checkpoints=False\n",
        "    )\n",
        "\n",
        "    start = time.time()\n",
        "    model_tft.fit(series=ts_train_scaled, val_series=ts_val_scaled, verbose=False)\n",
        "    train_time_tft = time.time() - start\n",
        "\n",
        "    # Predict\n",
        "    pred_tft_scaled = model_tft.predict(n=n_pred, series=ts_train_scaled)\n",
        "    pred_tft = scaler_darts.inverse_transform(pred_tft_scaled)\n",
        "\n",
        "    y_pred_tft = pred_tft.values().flatten()\n",
        "    y_test_tft = ts_test.values().flatten()\n",
        "    min_len = min(len(y_pred_tft), len(y_test_tft))\n",
        "    y_pred_tft = y_pred_tft[:min_len]\n",
        "    y_test_tft = y_test_tft[:min_len]\n",
        "\n",
        "    r2_tft = r2_score(y_test_tft, y_pred_tft)\n",
        "    rmse_tft = np.sqrt(mean_squared_error(y_test_tft, y_pred_tft))\n",
        "    mae_tft = mean_absolute_error(y_test_tft, y_pred_tft)\n",
        "\n",
        "    print(f\"\\nüìä TFT RESULTS:\")\n",
        "    print(f\"   R¬≤ = {r2_tft:.4f}\")\n",
        "    print(f\"   RMSE = {rmse_tft:.2f}\")\n",
        "    print(f\"   MAE = {mae_tft:.2f}\")\n",
        "    print(f\"   Time = {train_time_tft:.1f}s ({train_time_tft/60:.1f} min)\")\n",
        "    print(f\"\\nüèÜ TFT: State-of-the-Art Transformer-basiert!\")\n",
        "\n",
        "    all_results.append({\n",
        "        'Model': 'TFT',\n",
        "        'R¬≤': r2_tft,\n",
        "        'RMSE': rmse_tft,\n",
        "        'MAE': mae_tft,\n",
        "        'Time (s)': train_time_tft\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e11d4921",
      "metadata": {
        "id": "e11d4921"
      },
      "source": [
        "---\n",
        "# üìä FINAL SUMMARY\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "7428e3a3",
      "metadata": {
        "id": "7428e3a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b8d589f-2673-439a-969c-a36834ef7429"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====================================================================================================\n",
            "üèÜ FINAL RESULTS: WIND_OFFSHORE\n",
            "====================================================================================================\n",
            "      Model          R¬≤        RMSE         MAE   Time (s)\n",
            "        GRU    0.329192  123.385628   87.688595  13.122822\n",
            "    Bi-LSTM    0.211380  133.782456   95.822595  30.699859\n",
            "       LSTM    0.076794  144.748681   87.808592  15.422146\n",
            "Autoencoder   -0.568189  188.653358  145.564323  79.527352\n",
            "        VAE   -6.796339  420.640162  361.237674  82.967615\n",
            "     DeepAR   -7.113428  436.828840  383.723389 106.583884\n",
            "    N-BEATS  -12.485067  563.165023  501.503240 733.767478\n",
            "     N-HiTS -100.413911 1544.393203 1519.133306  98.378062\n",
            "\n",
            "====================================================================================================\n",
            "ü•á BEST MODEL:\n",
            "====================================================================================================\n",
            "   Model: GRU\n",
            "   R¬≤ = 0.3292\n",
            "   RMSE = 123.39\n",
            "   MAE = 87.69\n",
            "   Training Time = 13.1s (0.2 min)\n",
            "\n",
            "üíæ Ergebnisse gespeichert: results/metrics/deep_learning_extended_wind_offshore.csv\n",
            "\n",
            "====================================================================================================\n",
            "üí° KEY INSIGHTS:\n",
            "====================================================================================================\n",
            "   üìå GRU vs LSTM: R¬≤ 0.3292 vs 0.0768\n",
            "      ‚Üí GRU ist 328.67% besser!\n",
            "\n",
            "   üìå Durchschnittliche R¬≤: -15.8449\n",
            "   üìå Schnellstes Modell: GRU (13.1s)\n",
            "   üìå Langsamtes Modell: N-BEATS (733.8s)\n",
            "\n",
            "   ‚ö†Ô∏è Modelle mit negativem R¬≤ (schlecht konfiguriert):\n",
            "      - Autoencoder: R¬≤ = -0.5682\n",
            "      - VAE: R¬≤ = -6.7963\n",
            "      - DeepAR: R¬≤ = -7.1134\n",
            "      - N-BEATS: R¬≤ = -12.4851\n",
            "      - N-HiTS: R¬≤ = -100.4139\n",
            "\n",
            "====================================================================================================\n",
            "‚úÖ EXPERIMENT ABGESCHLOSSEN!\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Create summary DataFrame\n",
        "if all_results:\n",
        "    results_df = pd.DataFrame(all_results)\n",
        "    results_df = results_df.sort_values('R¬≤', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(f\"üèÜ FINAL RESULTS: {SERIES_NAME.upper()}\")\n",
        "    print(\"=\"*100)\n",
        "    print(results_df.to_string(index=False))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"ü•á BEST MODEL:\")\n",
        "    print(\"=\"*100)\n",
        "    best = results_df.iloc[0]\n",
        "    print(f\"   Model: {best['Model']}\")\n",
        "    print(f\"   R¬≤ = {best['R¬≤']:.4f}\")\n",
        "    print(f\"   RMSE = {best['RMSE']:.2f}\")\n",
        "    print(f\"   MAE = {best['MAE']:.2f}\")\n",
        "    print(f\"   Training Time = {best['Time (s)']:.1f}s ({best['Time (s)']/60:.1f} min)\")\n",
        "\n",
        "    # Save results\n",
        "    output_file = f'results/metrics/deep_learning_extended_{SERIES_NAME}.csv'\n",
        "    results_df.to_csv(output_file, index=False)\n",
        "    print(f\"\\nüíæ Ergebnisse gespeichert: {output_file}\")\n",
        "\n",
        "    # Performance insights\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"üí° KEY INSIGHTS:\")\n",
        "    print(\"=\"*100)\n",
        "\n",
        "    if 'GRU' in results_df['Model'].values and 'LSTM' in results_df['Model'].values:\n",
        "        gru_r2 = results_df[results_df['Model'] == 'GRU']['R¬≤'].values[0]\n",
        "        lstm_r2 = results_df[results_df['Model'] == 'LSTM']['R¬≤'].values[0]\n",
        "        print(f\"   üìå GRU vs LSTM: R¬≤ {gru_r2:.4f} vs {lstm_r2:.4f}\")\n",
        "        if gru_r2 > lstm_r2:\n",
        "            print(f\"      ‚Üí GRU ist {((gru_r2 - lstm_r2) / lstm_r2 * 100):.2f}% besser!\")\n",
        "\n",
        "    print(f\"\\n   üìå Durchschnittliche R¬≤: {results_df['R¬≤'].mean():.4f}\")\n",
        "    print(f\"   üìå Schnellstes Modell: {results_df.loc[results_df['Time (s)'].idxmin(), 'Model']} ({results_df['Time (s)'].min():.1f}s)\")\n",
        "    print(f\"   üìå Langsamtes Modell: {results_df.loc[results_df['Time (s)'].idxmax(), 'Model']} ({results_df['Time (s)'].max():.1f}s)\")\n",
        "\n",
        "    negative_r2 = results_df[results_df['R¬≤'] < 0]\n",
        "    if len(negative_r2) > 0:\n",
        "        print(f\"\\n   ‚ö†Ô∏è Modelle mit negativem R¬≤ (schlecht konfiguriert):\")\n",
        "        for _, row in negative_r2.iterrows():\n",
        "            print(f\"      - {row['Model']}: R¬≤ = {row['R¬≤']:.4f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"‚úÖ EXPERIMENT ABGESCHLOSSEN!\")\n",
        "    print(\"=\"*100)\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è Keine Modelle wurden ausgef√ºhrt. Bitte aktiviere mindestens eine Modellkategorie!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3273543",
      "metadata": {
        "id": "d3273543"
      },
      "source": [
        "## üìù Empfehlungen\n",
        "\n",
        "### F√ºr Produktion:\n",
        "1. **H√∂chste Genauigkeit**: Bestes Modell nach R¬≤ w√§hlen\n",
        "2. **Balance Speed/Accuracy**: GRU oder Bi-LSTM\n",
        "3. **Unsicherheitssch√§tzung**: DeepAR oder VAE\n",
        "4. **State-of-the-Art**: TFT (wenn Rechenzeit kein Problem)\n",
        "\n",
        "### F√ºr weitere Experimente:\n",
        "- **TimeGAN**: Aktiviere `RUN_GAN = True` (sehr experimentell)\n",
        "- **Hyperparameter-Tuning**: Optimiere die besten 2-3 Modelle weiter\n",
        "- **Ensemble**: Kombiniere mehrere Top-Modelle"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}