{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3bee9d27",
      "metadata": {
        "id": "3bee9d27"
      },
      "source": [
        "# ðŸš€ Advanced Deep Learning Models - Extended Edition\n",
        "\n",
        "**Komplette Suite moderner DL-Architekturen mit GPU-Beschleunigung**\n",
        "\n",
        "## ðŸ“‹ Modelle in diesem Notebook:\n",
        "\n",
        "### Basis DL-Modelle:\n",
        "1. **LSTM** - Standard Long Short-Term Memory\n",
        "2. **Bi-LSTM** - Bidirectional LSTM\n",
        "3. **GRU** - Gated Recurrent Unit (schneller als LSTM)\n",
        "\n",
        "### Generative Modelle:\n",
        "4. **Autoencoder** - Dimensionsreduktion + Forecasting\n",
        "5. **VAE** - Variational Autoencoder (mit UnsicherheitsschÃ¤tzung)\n",
        "6. **TimeGAN** - Generative Adversarial Network fÃ¼r Zeitreihen\n",
        "\n",
        "### Advanced/Transformer:\n",
        "7. **N-BEATS** - Neural Basis Expansion\n",
        "8. **N-HiTS** - Hierarchical Interpolation\n",
        "9. **DeepAR** - Amazon's probabilistisches Modell\n",
        "10. **TFT** - Temporal Fusion Transformer (State-of-the-Art)\n",
        "\n",
        "### Hinweise zur Rechenzeit (GPU T4):\n",
        "- âœ… **Schnell** (<5 Min): LSTM, GRU, Bi-LSTM, Autoencoder, VAE\n",
        "- âš ï¸ **Mittel** (5-15 Min): N-BEATS, N-HiTS, DeepAR\n",
        "- ðŸ”¥ **Langsam** (15-45 Min): TFT, TimeGAN\n",
        "\n",
        "**Setup:** Runtime â†’ Change runtime type â†’ GPU (T4 empfohlen, A100 fÃ¼r TFT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7a4be948",
      "metadata": {
        "id": "7a4be948",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8da3e763-e636-492f-8f68-e5b1e223b810"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.19.0\n",
            "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "\n",
            "ðŸš€ GPU should show above!\n"
          ]
        }
      ],
      "source": [
        "# Check GPU\n",
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
        "print(\"\\nðŸš€ GPU should show above!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3607b12e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3607b12e",
        "outputId": "a8526994-72ae-4690-8042-9b2ca68ae6d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AdvancedTimeSeriesPrediction'...\n",
            "remote: Enumerating objects: 788, done.\u001b[K\n",
            "remote: Counting objects: 100% (106/106), done.\u001b[K\n",
            "remote: Compressing objects: 100% (75/75), done.\u001b[K\n",
            "remote: Total 788 (delta 38), reused 86 (delta 26), pack-reused 682 (from 1)\u001b[K\n",
            "Receiving objects: 100% (788/788), 73.09 MiB | 15.21 MiB/s, done.\n",
            "Resolving deltas: 100% (283/283), done.\n",
            "/content/AdvancedTimeSeriesPrediction/energy-timeseries-project\n"
          ]
        }
      ],
      "source": [
        "# Clone Repository\n",
        "!git clone https://github.com/chradden/AdvancedTimeSeriesPrediction.git\n",
        "%cd AdvancedTimeSeriesPrediction/energy-timeseries-project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "80df3cc9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80df3cc9",
        "outputId": "d09b4295-c783-4995-96e7-1f3613790716"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“¦ Installing packages (this may take 2-3 minutes)...\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m857.3/857.3 kB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: darts 0.40.0 does not provide the extra 'torch'\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m204.7/204.7 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m825.4/825.4 kB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hâœ… Installation complete!\n"
          ]
        }
      ],
      "source": [
        "# Install ALL Dependencies\n",
        "print(\"ðŸ“¦ Installing packages (this may take 2-3 minutes)...\")\n",
        "!pip install -q pandas numpy matplotlib seaborn scikit-learn\n",
        "!pip install -q tensorflow keras pytorch-lightning\n",
        "!pip install -q 'darts[torch]'  # N-BEATS, N-HiTS, TFT, DeepAR\n",
        "!pip install -q gluonts  # DeepAR alternative\n",
        "print(\"âœ… Installation complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "813e5d25",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "813e5d25",
        "outputId": "bbc49c8f-90e0-4342-eca5-ad6910b57344"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… GPU configured: 1 device(s)\n",
            "\n",
            "ðŸ“Š All imports successful!\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set seeds\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# GPU Config\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(f\"âœ… GPU configured: {len(gpus)} device(s)\")\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "else:\n",
        "    print(\"âš ï¸ No GPU found - training will be slow!\")\n",
        "\n",
        "print(\"\\nðŸ“Š All imports successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f508784",
      "metadata": {
        "id": "1f508784"
      },
      "source": [
        "## âš™ï¸ Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ea77c9d6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea77c9d6",
        "outputId": "74cd0b24-324a-4aa3-c55a-eb4ff32a8f95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“Š Zeitreihe: WIND_ONSHORE\n",
            "\n",
            "ðŸŽ¯ Aktivierte Modelle:\n",
            "   âœ… Basic DL (LSTM, GRU, Bi-LSTM)\n",
            "   âœ… Generative (Autoencoder, VAE)\n",
            "   âœ… Advanced (N-BEATS, N-HiTS)\n",
            "   âœ… DeepAR (probabilistisch)\n",
            "\n",
            "âœ… Konfiguration abgeschlossen!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "SERIES_NAME = 'wind_onshore'  # Ã„ndern: 'solar', 'wind_offshore', 'wind_onshore', 'price', 'consumption'\n",
        "\n",
        "# Model Selection (setze auf False um Modelle zu Ã¼berspringen)\n",
        "RUN_BASIC = True          # LSTM, GRU, Bi-LSTM (~5 min)\n",
        "RUN_GENERATIVE = True     # Autoencoder, VAE (~5 min)\n",
        "RUN_GAN = False           # TimeGAN (~30 min, experimentell)\n",
        "RUN_ADVANCED = True       # N-BEATS, N-HiTS (~10 min)\n",
        "RUN_PROBABILISTIC = True  # DeepAR (~10 min)\n",
        "RUN_TFT = False           # Temporal Fusion Transformer (~30-45 min)\n",
        "\n",
        "print(f\"ðŸ“Š Zeitreihe: {SERIES_NAME.upper()}\")\n",
        "print(f\"\\nðŸŽ¯ Aktivierte Modelle:\")\n",
        "if RUN_BASIC: print(\"   âœ… Basic DL (LSTM, GRU, Bi-LSTM)\")\n",
        "if RUN_GENERATIVE: print(\"   âœ… Generative (Autoencoder, VAE)\")\n",
        "if RUN_GAN: print(\"   âœ… TimeGAN (experimentell, ~30 min)\")\n",
        "if RUN_ADVANCED: print(\"   âœ… Advanced (N-BEATS, N-HiTS)\")\n",
        "if RUN_PROBABILISTIC: print(\"   âœ… DeepAR (probabilistisch)\")\n",
        "if RUN_TFT: print(\"   âœ… TFT (State-of-the-Art, ~30-45 min)\")\n",
        "\n",
        "print(f\"\\nâœ… Konfiguration abgeschlossen!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "587212fb",
      "metadata": {
        "id": "587212fb"
      },
      "source": [
        "## ðŸ“‚ Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "fb4e8e25",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb4e8e25",
        "outputId": "1ceab5ee-da49-4a22-b011-ea1a012b2292"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“‚ Data loaded for: WIND_ONSHORE\n",
            "   Train: 21697 | Val: 2232 | Test: 2208\n",
            "   Value column: wind_power\n",
            "   Features: 27\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "train_df = pd.read_csv(f'data/processed/{SERIES_NAME}_train.csv')\n",
        "val_df = pd.read_csv(f'data/processed/{SERIES_NAME}_val.csv')\n",
        "test_df = pd.read_csv(f'data/processed/{SERIES_NAME}_test.csv')\n",
        "\n",
        "# Determine value column\n",
        "# First, check if SERIES_NAME is directly a column\n",
        "if SERIES_NAME in train_df.columns:\n",
        "    value_col = SERIES_NAME\n",
        "elif 'value' in train_df.columns: # Fallback to a generic 'value' column\n",
        "    value_col = 'value'\n",
        "elif f'{SERIES_NAME.replace(\"onshore\", \"power\").replace(\"offshore\", \"power\")}' in train_df.columns: # Specific check for wind_power\n",
        "    value_col = f'{SERIES_NAME.replace(\"onshore\", \"power\").replace(\"offshore\", \"power\")}'\n",
        "else:\n",
        "    # If neither SERIES_NAME nor 'value' is found, try the original list of options\n",
        "    potential_cols = [c for c in train_df.columns if c in ['solar', 'price', 'wind_offshore', 'wind_onshore', 'consumption', 'wind_power']]\n",
        "    if potential_cols:\n",
        "        value_col = potential_cols[0]\n",
        "    else:\n",
        "        # If no suitable column is found after all attempts, raise an error\n",
        "        print(f\"ERROR: Could not find a suitable value column for SERIES_NAME: '{SERIES_NAME}'.\")\n",
        "        print(f\"       Expected columns (or generic 'value'): ['{SERIES_NAME}', 'value', 'solar', 'price', 'wind_offshore', 'wind_onshore', 'consumption', 'wind_power'].\")\n",
        "        print(f\"       Actual columns in train_df: {train_df.columns.tolist()}\")\n",
        "        raise ValueError(\"No valid value column found in the DataFrame.\")\n",
        "\n",
        "feature_cols = [c for c in train_df.columns if c not in ['timestamp', value_col]]\n",
        "\n",
        "print(f\"ðŸ“‚ Data loaded for: {SERIES_NAME.upper()}\")\n",
        "print(f\"   Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}\")\n",
        "print(f\"   Value column: {value_col}\")\n",
        "print(f\"   Features: {len(feature_cols)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2eec36f1",
      "metadata": {
        "id": "2eec36f1"
      },
      "source": [
        "## ðŸ”§ Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a0a03164",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0a03164",
        "outputId": "decf9e45-8213-4b5c-deae-f1b4bb4da267"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Data prepared:\n",
            "   X_train_seq: (21673, 24, 27)\n",
            "   y_test_seq: (2184,)\n"
          ]
        }
      ],
      "source": [
        "def create_sequences(data, target, seq_length):\n",
        "    \"\"\"Create sequences for RNN models\"\"\"\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i + seq_length])\n",
        "        y.append(target[i + seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Scale data\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "X_train = scaler_X.fit_transform(train_df[feature_cols])\n",
        "y_train = scaler_y.fit_transform(train_df[[value_col]])\n",
        "\n",
        "X_val = scaler_X.transform(val_df[feature_cols])\n",
        "y_val = scaler_y.transform(val_df[[value_col]])\n",
        "\n",
        "X_test = scaler_X.transform(test_df[feature_cols])\n",
        "y_test_orig = test_df[value_col].values\n",
        "\n",
        "# Create sequences\n",
        "seq_length = 24\n",
        "X_train_seq, y_train_seq = create_sequences(X_train, y_train.flatten(), seq_length)\n",
        "X_val_seq, y_val_seq = create_sequences(X_val, y_val.flatten(), seq_length)\n",
        "X_test_seq, _ = create_sequences(X_test, np.zeros(len(X_test)), seq_length)\n",
        "y_test_seq = y_test_orig[seq_length:]\n",
        "\n",
        "print(f\"âœ… Data prepared:\")\n",
        "print(f\"   X_train_seq: {X_train_seq.shape}\")\n",
        "print(f\"   y_test_seq: {y_test_seq.shape}\")\n",
        "\n",
        "# Storage for results\n",
        "all_results = []"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b30aa764",
      "metadata": {
        "id": "b30aa764"
      },
      "source": [
        "---\n",
        "# ðŸ”µ BASIC MODELS\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38f42898",
      "metadata": {
        "id": "38f42898"
      },
      "source": [
        "## ðŸ§ª Model 1: LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "3168efd2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3168efd2",
        "outputId": "6dfcbe1f-41b6-41f5-8e12-e46302182dfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ðŸ§ª MODEL 1: LSTM\n",
            "================================================================================\n",
            "\n",
            "ðŸ“Š LSTM RESULTS:\n",
            "   RÂ² = 0.9548\n",
            "   RMSE = 397.74\n",
            "   MAE = 290.85\n",
            "   Time = 22.7s\n"
          ]
        }
      ],
      "source": [
        "if RUN_BASIC:\n",
        "    print(\"=\"*80)\n",
        "    print(\"ðŸ§ª MODEL 1: LSTM\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Build\n",
        "    model_lstm = keras.Sequential([\n",
        "        layers.LSTM(64, activation='relu', return_sequences=False,\n",
        "                   input_shape=(seq_length, len(feature_cols))),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    model_lstm.compile(optimizer=keras.optimizers.Adam(0.001), loss='mse')\n",
        "\n",
        "    # Train\n",
        "    start = time.time()\n",
        "    history = model_lstm.fit(\n",
        "        X_train_seq, y_train_seq,\n",
        "        validation_data=(X_val_seq, y_val_seq),\n",
        "        epochs=100, batch_size=64,\n",
        "        callbacks=[\n",
        "            EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
        "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
        "        ],\n",
        "        verbose=0\n",
        "    )\n",
        "    train_time = time.time() - start\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred_scaled = model_lstm.predict(X_test_seq, verbose=0)\n",
        "    y_pred = scaler_y.inverse_transform(y_pred_scaled).flatten()\n",
        "\n",
        "    r2 = r2_score(y_test_seq, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test_seq, y_pred))\n",
        "    mae = mean_absolute_error(y_test_seq, y_pred)\n",
        "\n",
        "    print(f\"\\nðŸ“Š LSTM RESULTS:\")\n",
        "    print(f\"   RÂ² = {r2:.4f}\")\n",
        "    print(f\"   RMSE = {rmse:.2f}\")\n",
        "    print(f\"   MAE = {mae:.2f}\")\n",
        "    print(f\"   Time = {train_time:.1f}s\")\n",
        "\n",
        "    all_results.append({\n",
        "        'Model': 'LSTM',\n",
        "        'RÂ²': r2,\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'Time (s)': train_time\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07b77a30",
      "metadata": {
        "id": "07b77a30"
      },
      "source": [
        "## ðŸ§ª Model 2: GRU (â­ NEU!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d7f85ea4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7f85ea4",
        "outputId": "851a9455-ba91-4080-ab44-192e787c10b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ðŸ§ª MODEL 2: GRU (Gated Recurrent Unit)\n",
            "================================================================================\n",
            "\n",
            "ðŸ“Š GRU RESULTS:\n",
            "   RÂ² = 0.9532\n",
            "   RMSE = 405.06\n",
            "   MAE = 312.30\n",
            "   Time = 23.1s\n",
            "\n",
            "ðŸ’¡ GRU vs LSTM: -1.9% faster!\n"
          ]
        }
      ],
      "source": [
        "if RUN_BASIC:\n",
        "    print(\"=\"*80)\n",
        "    print(\"ðŸ§ª MODEL 2: GRU (Gated Recurrent Unit)\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Build GRU (schneller als LSTM!)\n",
        "    model_gru = keras.Sequential([\n",
        "        layers.GRU(64, activation='relu', return_sequences=False,\n",
        "                  input_shape=(seq_length, len(feature_cols))),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    model_gru.compile(optimizer=keras.optimizers.Adam(0.001), loss='mse')\n",
        "\n",
        "    # Train\n",
        "    start = time.time()\n",
        "    history = model_gru.fit(\n",
        "        X_train_seq, y_train_seq,\n",
        "        validation_data=(X_val_seq, y_val_seq),\n",
        "        epochs=100, batch_size=64,\n",
        "        callbacks=[\n",
        "            EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
        "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
        "        ],\n",
        "        verbose=0\n",
        "    )\n",
        "    train_time_gru = time.time() - start\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred_gru_scaled = model_gru.predict(X_test_seq, verbose=0)\n",
        "    y_pred_gru = scaler_y.inverse_transform(y_pred_gru_scaled).flatten()\n",
        "\n",
        "    r2_gru = r2_score(y_test_seq, y_pred_gru)\n",
        "    rmse_gru = np.sqrt(mean_squared_error(y_test_seq, y_pred_gru))\n",
        "    mae_gru = mean_absolute_error(y_test_seq, y_pred_gru)\n",
        "\n",
        "    print(f\"\\nðŸ“Š GRU RESULTS:\")\n",
        "    print(f\"   RÂ² = {r2_gru:.4f}\")\n",
        "    print(f\"   RMSE = {rmse_gru:.2f}\")\n",
        "    print(f\"   MAE = {mae_gru:.2f}\")\n",
        "    print(f\"   Time = {train_time_gru:.1f}s\")\n",
        "    print(f\"\\nðŸ’¡ GRU vs LSTM: {((train_time - train_time_gru) / train_time * 100):.1f}% faster!\")\n",
        "\n",
        "    all_results.append({\n",
        "        'Model': 'GRU',\n",
        "        'RÂ²': r2_gru,\n",
        "        'RMSE': rmse_gru,\n",
        "        'MAE': mae_gru,\n",
        "        'Time (s)': train_time_gru\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53a5145e",
      "metadata": {
        "id": "53a5145e"
      },
      "source": [
        "## ðŸ§ª Model 3: Bi-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b3823262",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3823262",
        "outputId": "489551bf-b72c-4bb5-9798-f53a6ceb3505"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ðŸ§ª MODEL 3: Bi-LSTM (Bidirectional)\n",
            "================================================================================\n",
            "\n",
            "ðŸ“Š BI-LSTM RESULTS:\n",
            "   RÂ² = 0.9522\n",
            "   RMSE = 409.37\n",
            "   MAE = 311.78\n",
            "   Time = 60.8s\n"
          ]
        }
      ],
      "source": [
        "if RUN_BASIC:\n",
        "    print(\"=\"*80)\n",
        "    print(\"ðŸ§ª MODEL 3: Bi-LSTM (Bidirectional)\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Build\n",
        "    model_bilstm = keras.Sequential([\n",
        "        layers.Bidirectional(layers.LSTM(64, activation='relu', return_sequences=True),\n",
        "                           input_shape=(seq_length, len(feature_cols))),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Bidirectional(layers.LSTM(32, activation='relu')),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(16, activation='relu'),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    model_bilstm.compile(optimizer=keras.optimizers.Adam(0.001), loss='mse')\n",
        "\n",
        "    # Train\n",
        "    start = time.time()\n",
        "    history = model_bilstm.fit(\n",
        "        X_train_seq, y_train_seq,\n",
        "        validation_data=(X_val_seq, y_val_seq),\n",
        "        epochs=100, batch_size=64,\n",
        "        callbacks=[\n",
        "            EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
        "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
        "        ],\n",
        "        verbose=0\n",
        "    )\n",
        "    train_time_bilstm = time.time() - start\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred_bilstm_scaled = model_bilstm.predict(X_test_seq, verbose=0)\n",
        "    y_pred_bilstm = scaler_y.inverse_transform(y_pred_bilstm_scaled).flatten()\n",
        "\n",
        "    r2_bilstm = r2_score(y_test_seq, y_pred_bilstm)\n",
        "    rmse_bilstm = np.sqrt(mean_squared_error(y_test_seq, y_pred_bilstm))\n",
        "    mae_bilstm = mean_absolute_error(y_test_seq, y_pred_bilstm)\n",
        "\n",
        "    print(f\"\\nðŸ“Š BI-LSTM RESULTS:\")\n",
        "    print(f\"   RÂ² = {r2_bilstm:.4f}\")\n",
        "    print(f\"   RMSE = {rmse_bilstm:.2f}\")\n",
        "    print(f\"   MAE = {mae_bilstm:.2f}\")\n",
        "    print(f\"   Time = {train_time_bilstm:.1f}s\")\n",
        "\n",
        "    all_results.append({\n",
        "        'Model': 'Bi-LSTM',\n",
        "        'RÂ²': r2_bilstm,\n",
        "        'RMSE': rmse_bilstm,\n",
        "        'MAE': mae_bilstm,\n",
        "        'Time (s)': train_time_bilstm\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e81bdb7d",
      "metadata": {
        "id": "e81bdb7d"
      },
      "source": [
        "---\n",
        "# ðŸŸ¢ GENERATIVE MODELS\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76712816",
      "metadata": {
        "id": "76712816"
      },
      "source": [
        "## ðŸ§ª Model 4: Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d23f548f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d23f548f",
        "outputId": "c6a9e573-af84-42fc-cf71-32b84c8d3c10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ðŸ§ª MODEL 4: Autoencoder\n",
            "================================================================================\n",
            "\n",
            "ðŸ“Š AUTOENCODER RESULTS:\n",
            "   RÂ² = 0.8782\n",
            "   RMSE = 653.26\n",
            "   MAE = 501.30\n",
            "   Time = 187.2s\n"
          ]
        }
      ],
      "source": [
        "if RUN_GENERATIVE:\n",
        "    print(\"=\"*80)\n",
        "    print(\"ðŸ§ª MODEL 4: Autoencoder\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Build Autoencoder\n",
        "    encoding_dim = 32\n",
        "    input_ae = layers.Input(shape=(seq_length, len(feature_cols)))\n",
        "    encoded = layers.LSTM(64, activation='relu', return_sequences=True)(input_ae)\n",
        "    encoded = layers.LSTM(encoding_dim, activation='relu')(encoded)\n",
        "\n",
        "    decoded = layers.RepeatVector(seq_length)(encoded)\n",
        "    decoded = layers.LSTM(64, activation='relu', return_sequences=True)(decoded)\n",
        "    decoded = layers.TimeDistributed(layers.Dense(len(feature_cols)))(decoded)\n",
        "\n",
        "    autoencoder = keras.Model(input_ae, decoded)\n",
        "    autoencoder.compile(optimizer='adam', loss='mse')\n",
        "    encoder = keras.Model(input_ae, encoded)\n",
        "\n",
        "    # Train Autoencoder\n",
        "    start = time.time()\n",
        "    autoencoder.fit(\n",
        "        X_train_seq, X_train_seq,\n",
        "        validation_data=(X_val_seq, X_val_seq),\n",
        "        epochs=50, batch_size=64,\n",
        "        callbacks=[EarlyStopping(patience=10, restore_best_weights=True)],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Train Forecast Head\n",
        "    encoded_train = encoder.predict(X_train_seq, verbose=0)\n",
        "    encoded_val = encoder.predict(X_val_seq, verbose=0)\n",
        "    encoded_test = encoder.predict(X_test_seq, verbose=0)\n",
        "\n",
        "    forecast_head = keras.Sequential([\n",
        "        layers.Dense(16, activation='relu', input_shape=(encoding_dim,)),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "    forecast_head.compile(optimizer='adam', loss='mse')\n",
        "    forecast_head.fit(\n",
        "        encoded_train, y_train_seq,\n",
        "        validation_data=(encoded_val, y_val_seq),\n",
        "        epochs=50, batch_size=64,\n",
        "        callbacks=[EarlyStopping(patience=10, restore_best_weights=True)],\n",
        "        verbose=0\n",
        "    )\n",
        "    train_time_ae = time.time() - start\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred_ae_scaled = forecast_head.predict(encoded_test, verbose=0)\n",
        "    y_pred_ae = scaler_y.inverse_transform(y_pred_ae_scaled).flatten()\n",
        "\n",
        "    r2_ae = r2_score(y_test_seq, y_pred_ae)\n",
        "    rmse_ae = np.sqrt(mean_squared_error(y_test_seq, y_pred_ae))\n",
        "    mae_ae = mean_absolute_error(y_test_seq, y_pred_ae)\n",
        "\n",
        "    print(f\"\\nðŸ“Š AUTOENCODER RESULTS:\")\n",
        "    print(f\"   RÂ² = {r2_ae:.4f}\")\n",
        "    print(f\"   RMSE = {rmse_ae:.2f}\")\n",
        "    print(f\"   MAE = {mae_ae:.2f}\")\n",
        "    print(f\"   Time = {train_time_ae:.1f}s\")\n",
        "\n",
        "    all_results.append({\n",
        "        'Model': 'Autoencoder',\n",
        "        'RÂ²': r2_ae,\n",
        "        'RMSE': rmse_ae,\n",
        "        'MAE': mae_ae,\n",
        "        'Time (s)': train_time_ae\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64d2ce7a",
      "metadata": {
        "id": "64d2ce7a"
      },
      "source": [
        "## ðŸ§ª Model 5: VAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "712a98ce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "712a98ce",
        "outputId": "b1525432-cbcd-4006-b78c-9f426f8d9537"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ðŸ§ª MODEL 5: VAE (Variational Autoencoder)\n",
            "================================================================================\n",
            "\n",
            "ðŸ“Š VAE RESULTS:\n",
            "   RÂ² = 0.8578\n",
            "   RMSE = 705.88\n",
            "   MAE = 550.90\n",
            "   Time = 195.8s\n"
          ]
        }
      ],
      "source": [
        "if RUN_GENERATIVE:\n",
        "    print(\"=\"*80)\n",
        "    print(\"ðŸ§ª MODEL 5: VAE (Variational Autoencoder)\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Build VAE parts as separate functional models\n",
        "    latent_dim = 32\n",
        "\n",
        "    # Encoder Model\n",
        "    input_vae_enc = layers.Input(shape=(seq_length, len(feature_cols)))\n",
        "    x_enc = layers.LSTM(64, activation='relu', return_sequences=True)(input_vae_enc)\n",
        "    x_enc = layers.LSTM(64, activation='relu')(x_enc)\n",
        "\n",
        "    z_mean = layers.Dense(latent_dim)(x_enc)\n",
        "    z_log_var = layers.Dense(latent_dim)(x_enc)\n",
        "\n",
        "    def sampling(args):\n",
        "        z_mean, z_log_var = args\n",
        "        epsilon = tf.random.normal(shape=tf.shape(z_mean))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "    z = layers.Lambda(sampling)([z_mean, z_log_var])\n",
        "\n",
        "    encoder_model = keras.Model(input_vae_enc, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "\n",
        "    # Decoder Model\n",
        "    decoder_input = layers.Input(shape=(latent_dim,))\n",
        "    x_decoded = layers.RepeatVector(seq_length)(decoder_input)\n",
        "    x_decoded = layers.LSTM(64, activation='relu', return_sequences=True)(x_decoded)\n",
        "    x_decoded = layers.TimeDistributed(layers.Dense(len(feature_cols)))(x_decoded)\n",
        "    decoder_model = keras.Model(decoder_input, x_decoded, name=\"decoder\")\n",
        "\n",
        "    # Custom VAE Model subclass\n",
        "    class CustomVAE(keras.Model):\n",
        "        def __init__(self, encoder, decoder, **kwargs):\n",
        "            super().__init__(**kwargs)\n",
        "            self.encoder = encoder\n",
        "            self.decoder = decoder\n",
        "            self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "            self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
        "            self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "        def call(self, inputs):\n",
        "            _, _, z = self.encoder(inputs)\n",
        "            reconstruction = self.decoder(z)\n",
        "            return reconstruction\n",
        "\n",
        "        def train_step(self, data):\n",
        "            # Unpack the data. We use X_train_seq as both input and target for reconstruction.\n",
        "            if isinstance(data, tuple):\n",
        "                inputs, _ = data\n",
        "            else:\n",
        "                inputs = data\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                z_mean, z_log_var, z = self.encoder(inputs)\n",
        "                reconstruction = self.decoder(z)\n",
        "\n",
        "                # Calculate reconstruction loss\n",
        "                reconstruction_loss = keras.ops.mean(keras.ops.sum(keras.losses.mse(inputs, reconstruction), axis=-1))\n",
        "\n",
        "                # Calculate KL divergence loss\n",
        "                kl_loss = -0.5 * keras.ops.mean(1 + z_log_var - keras.ops.square(z_mean) - keras.ops.exp(z_log_var))\n",
        "\n",
        "                total_loss = reconstruction_loss + kl_loss\n",
        "\n",
        "            grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "            self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "\n",
        "            self.total_loss_tracker.update_state(total_loss)\n",
        "            self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "            self.kl_loss_tracker.update_state(kl_loss)\n",
        "\n",
        "            return {\n",
        "                \"loss\": self.total_loss_tracker.result(),\n",
        "                \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "                \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "            }\n",
        "\n",
        "        @property\n",
        "        def metrics(self):\n",
        "            return [\n",
        "                self.total_loss_tracker,\n",
        "                self.reconstruction_loss_tracker,\n",
        "                self.kl_loss_tracker,\n",
        "            ]\n",
        "\n",
        "    # Instantiate the custom VAE model\n",
        "    vae = CustomVAE(encoder_model, decoder_model)\n",
        "    # Provide a dummy loss function to satisfy the compile() check.\n",
        "    # The actual loss computation is handled within the custom train_step.\n",
        "    vae.compile(optimizer='adam', loss=keras.losses.MeanSquaredError())\n",
        "\n",
        "    # The encoder used for generating latent space representations for the forecast head\n",
        "    prediction_encoder_vae = keras.Model(input_vae_enc, z_mean, name=\"prediction_encoder_vae\")\n",
        "\n",
        "    # Train VAE\n",
        "    start = time.time()\n",
        "    vae.fit(\n",
        "        X_train_seq, X_train_seq, # VAE trains on its inputs\n",
        "        validation_data=(X_val_seq, X_val_seq),\n",
        "        epochs=50, batch_size=64,\n",
        "        callbacks=[EarlyStopping(patience=10, restore_best_weights=True)],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Train Forecast Head\n",
        "    encoded_vae_train = prediction_encoder_vae.predict(X_train_seq, verbose=0)\n",
        "    encoded_vae_val = prediction_encoder_vae.predict(X_val_seq, verbose=0)\n",
        "    encoded_vae_test = prediction_encoder_vae.predict(X_test_seq, verbose=0)\n",
        "\n",
        "    forecast_head_vae = keras.Sequential([\n",
        "        layers.Dense(16, activation='relu', input_shape=(latent_dim,)),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "    forecast_head_vae.compile(optimizer='adam', loss='mse')\n",
        "    forecast_head_vae.fit(\n",
        "        encoded_vae_train, y_train_seq,\n",
        "        validation_data=(encoded_vae_val, y_val_seq),\n",
        "        epochs=50, batch_size=64,\n",
        "        callbacks=[EarlyStopping(patience=10, restore_best_weights=True)],\n",
        "        verbose=0\n",
        "    )\n",
        "    train_time_vae = time.time() - start\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred_vae_scaled = forecast_head_vae.predict(encoded_vae_test, verbose=0)\n",
        "    y_pred_vae = scaler_y.inverse_transform(y_pred_vae_scaled).flatten()\n",
        "\n",
        "    r2_vae = r2_score(y_test_seq, y_pred_vae)\n",
        "    rmse_vae = np.sqrt(mean_squared_error(y_test_seq, y_pred_vae))\n",
        "    mae_vae = mean_absolute_error(y_test_seq, y_pred_vae)\n",
        "\n",
        "    print(f\"\\nðŸ“Š VAE RESULTS:\")\n",
        "    print(f\"   RÂ² = {r2_vae:.4f}\")\n",
        "    print(f\"   RMSE = {rmse_vae:.2f}\")\n",
        "    print(f\"   MAE = {mae_vae:.2f}\")\n",
        "    print(f\"   Time = {train_time_vae:.1f}s\")\n",
        "\n",
        "    all_results.append({\n",
        "        'Model': 'VAE',\n",
        "        'RÂ²': r2_vae,\n",
        "        'RMSE': rmse_vae,\n",
        "        'MAE': mae_vae,\n",
        "        'Time (s)': train_time_vae\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f1fa817",
      "metadata": {
        "id": "1f1fa817"
      },
      "source": [
        "---\n",
        "# ðŸ”´ ADVANCED MODELS (Darts Framework)\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "06f5ec5b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06f5ec5b",
        "outputId": "84fb4286-97bd-4f0b-a80b-b11d185af61a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Darts TimeSeries prepared\n"
          ]
        }
      ],
      "source": [
        "# Prepare Darts TimeSeries (needed for N-BEATS, N-HiTS, DeepAR, TFT)\n",
        "if RUN_ADVANCED or RUN_PROBABILISTIC or RUN_TFT:\n",
        "    from darts import TimeSeries\n",
        "    from darts.dataprocessing.transformers import Scaler as DartsScaler\n",
        "\n",
        "    ts_train = TimeSeries.from_values(train_df[value_col].values)\n",
        "    ts_val = TimeSeries.from_values(val_df[value_col].values)\n",
        "    ts_test = TimeSeries.from_values(test_df[value_col].values)\n",
        "\n",
        "    scaler_darts = DartsScaler()\n",
        "    ts_train_scaled = scaler_darts.fit_transform(ts_train)\n",
        "    ts_val_scaled = scaler_darts.transform(ts_val)\n",
        "\n",
        "    print(\"âœ… Darts TimeSeries prepared\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d97b09f2",
      "metadata": {
        "id": "d97b09f2"
      },
      "source": [
        "## ðŸ§ª Model 6: N-BEATS (â­ NEU!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "49be663f",
      "metadata": {
        "id": "49be663f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b829cf1-045e-4d48-bbaa-338c5a8b242a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:darts.models:The StatsForecast module could not be imported. To enable support for the AutoARIMA, AutoETS and Croston models, please consider installing it.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ðŸ§ª MODEL 6: N-BEATS (Neural Basis Expansion)\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“Š N-BEATS RESULTS:\n",
            "   RÂ² = -4.6288\n",
            "   RMSE = 4449.91\n",
            "   MAE = 4025.21\n",
            "   Time = 1960.6s\n"
          ]
        }
      ],
      "source": [
        "if RUN_ADVANCED:\n",
        "    from darts.models import NBEATSModel\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"ðŸ§ª MODEL 6: N-BEATS (Neural Basis Expansion)\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    model_nbeats = NBEATSModel(\n",
        "        input_chunk_length=24,\n",
        "        output_chunk_length=1,\n",
        "        n_epochs=100,\n",
        "        batch_size=64,\n",
        "        pl_trainer_kwargs={\n",
        "            \"accelerator\": \"gpu\",\n",
        "            \"devices\": 1,\n",
        "            \"enable_progress_bar\": False\n",
        "        },\n",
        "        force_reset=True,\n",
        "        save_checkpoints=False\n",
        "    )\n",
        "\n",
        "    start = time.time()\n",
        "    model_nbeats.fit(series=ts_train_scaled, val_series=ts_val_scaled, verbose=False)\n",
        "    train_time_nbeats = time.time() - start\n",
        "\n",
        "    # Predict\n",
        "    n_pred = len(ts_test)\n",
        "    pred_nbeats_scaled = model_nbeats.predict(n=n_pred, series=ts_train_scaled)\n",
        "    pred_nbeats = scaler_darts.inverse_transform(pred_nbeats_scaled)\n",
        "\n",
        "    y_pred_nbeats = pred_nbeats.values().flatten()\n",
        "    y_test_nbeats = ts_test.values().flatten()\n",
        "    min_len = min(len(y_pred_nbeats), len(y_test_nbeats))\n",
        "    y_pred_nbeats = y_pred_nbeats[:min_len]\n",
        "    y_test_nbeats = y_test_nbeats[:min_len]\n",
        "\n",
        "    r2_nbeats = r2_score(y_test_nbeats, y_pred_nbeats)\n",
        "    rmse_nbeats = np.sqrt(mean_squared_error(y_test_nbeats, y_pred_nbeats))\n",
        "    mae_nbeats = mean_absolute_error(y_test_nbeats, y_pred_nbeats)\n",
        "\n",
        "    print(f\"\\nðŸ“Š N-BEATS RESULTS:\")\n",
        "    print(f\"   RÂ² = {r2_nbeats:.4f}\")\n",
        "    print(f\"   RMSE = {rmse_nbeats:.2f}\")\n",
        "    print(f\"   MAE = {mae_nbeats:.2f}\")\n",
        "    print(f\"   Time = {train_time_nbeats:.1f}s\")\n",
        "\n",
        "    all_results.append({\n",
        "        'Model': 'N-BEATS',\n",
        "        'RÂ²': r2_nbeats,\n",
        "        'RMSE': rmse_nbeats,\n",
        "        'MAE': mae_nbeats,\n",
        "        'Time (s)': train_time_nbeats\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6ac4199",
      "metadata": {
        "id": "a6ac4199"
      },
      "source": [
        "## ðŸ§ª Model 7: N-HiTS (â­ NEU!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "f27c5475",
      "metadata": {
        "id": "f27c5475",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "930d81d7-1220-43fd-a0d4-8ed30ab0fd4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ðŸ§ª MODEL 7: N-HiTS (Hierarchical Interpolation)\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“Š N-HiTS RESULTS:\n",
            "   RÂ² = -1020362908632544335979385413523074054721873770507200578521543108568222094789179351122159208850598667152323357247732863162432485555072804285410359907129289700071309282501492132709475073834017183592611840.0000\n",
            "   RMSE = 59912894252966492245340209921589062559646374018397585260939069088464186146647402062028191637713190912000.00\n",
            "   MAE = 5512239490579539786223312351368323695143320159634953432993759504505395117323920727764990542383260106752.00\n",
            "   Time = 259.7s\n"
          ]
        }
      ],
      "source": [
        "if RUN_ADVANCED:\n",
        "    from darts.models import NHiTSModel\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"ðŸ§ª MODEL 7: N-HiTS (Hierarchical Interpolation)\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    model_nhits = NHiTSModel(\n",
        "        input_chunk_length=24,\n",
        "        output_chunk_length=1,\n",
        "        n_epochs=100,\n",
        "        batch_size=64,\n",
        "        pl_trainer_kwargs={\n",
        "            \"accelerator\": \"gpu\",\n",
        "            \"devices\": 1,\n",
        "            \"enable_progress_bar\": False\n",
        "        },\n",
        "        force_reset=True,\n",
        "        save_checkpoints=False\n",
        "    )\n",
        "\n",
        "    start = time.time()\n",
        "    model_nhits.fit(series=ts_train_scaled, val_series=ts_val_scaled, verbose=False)\n",
        "    train_time_nhits = time.time() - start\n",
        "\n",
        "    # Predict\n",
        "    pred_nhits_scaled = model_nhits.predict(n=n_pred, series=ts_train_scaled)\n",
        "    pred_nhits = scaler_darts.inverse_transform(pred_nhits_scaled)\n",
        "\n",
        "    y_pred_nhits = pred_nhits.values().flatten()\n",
        "    y_test_nhits = ts_test.values().flatten()\n",
        "    min_len = min(len(y_pred_nhits), len(y_test_nhits))\n",
        "    y_pred_nhits = y_pred_nhits[:min_len]\n",
        "    y_test_nhits = y_test_nhits[:min_len]\n",
        "\n",
        "    r2_nhits = r2_score(y_test_nhits, y_pred_nhits)\n",
        "    rmse_nhits = np.sqrt(mean_squared_error(y_test_nhits, y_pred_nhits))\n",
        "    mae_nhits = mean_absolute_error(y_test_nhits, y_pred_nhits)\n",
        "\n",
        "    print(f\"\\nðŸ“Š N-HiTS RESULTS:\")\n",
        "    print(f\"   RÂ² = {r2_nhits:.4f}\")\n",
        "    print(f\"   RMSE = {rmse_nhits:.2f}\")\n",
        "    print(f\"   MAE = {mae_nhits:.2f}\")\n",
        "    print(f\"   Time = {train_time_nhits:.1f}s\")\n",
        "\n",
        "    all_results.append({\n",
        "        'Model': 'N-HiTS',\n",
        "        'RÂ²': r2_nhits,\n",
        "        'RMSE': rmse_nhits,\n",
        "        'MAE': mae_nhits,\n",
        "        'Time (s)': train_time_nhits\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fea7afb3",
      "metadata": {
        "id": "fea7afb3"
      },
      "source": [
        "## ðŸ§ª Model 8: DeepAR (â­ NEU! - Probabilistisch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "106816d8",
      "metadata": {
        "id": "106816d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1301c016-04e3-4a77-c85f-37f6ead57fea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ðŸ§ª MODEL 8: DeepAR (Probabilistic Forecasting)\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“Š DEEPAR RESULTS:\n",
            "   RÂ² = -1.0304\n",
            "   RMSE = 2672.60\n",
            "   MAE = 2167.69\n",
            "   Time = 284.8s\n",
            "\n",
            "ðŸ’¡ DeepAR liefert probabilistische Forecasts (Konfidenzintervalle mÃ¶glich!)\n"
          ]
        }
      ],
      "source": [
        "if RUN_PROBABILISTIC:\n",
        "    from darts.models import RNNModel\n",
        "    from darts.utils.likelihood_models import GaussianLikelihood # Import GaussianLikelihood\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"ðŸ§ª MODEL 8: DeepAR (Probabilistic Forecasting)\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # DeepAR via Darts' RNNModel with probabilistic output\n",
        "    model_deepar = RNNModel(\n",
        "        model='LSTM',\n",
        "        input_chunk_length=24,\n",
        "        training_length=48,\n",
        "        n_epochs=100,\n",
        "        batch_size=64,\n",
        "        hidden_dim=64,\n",
        "        n_rnn_layers=2,\n",
        "        dropout=0.2,\n",
        "        likelihood=GaussianLikelihood(),  # Pass an instance of GaussianLikelihood\n",
        "        pl_trainer_kwargs={\n",
        "            \"accelerator\": \"gpu\",\n",
        "            \"devices\": 1,\n",
        "            \"enable_progress_bar\": False\n",
        "        },\n",
        "        force_reset=True,\n",
        "        save_checkpoints=False\n",
        "    )\n",
        "\n",
        "    start = time.time()\n",
        "    model_deepar.fit(series=ts_train_scaled, val_series=ts_val_scaled, verbose=False)\n",
        "    train_time_deepar = time.time() - start\n",
        "\n",
        "    # Predict (median)\n",
        "    pred_deepar_scaled = model_deepar.predict(n=n_pred, series=ts_train_scaled, num_samples=100)\n",
        "    pred_deepar = scaler_darts.inverse_transform(pred_deepar_scaled)\n",
        "\n",
        "    y_pred_deepar = pred_deepar.values().flatten()\n",
        "    y_test_deepar = ts_test.values().flatten()\n",
        "    min_len = min(len(y_pred_deepar), len(y_test_deepar))\n",
        "    y_pred_deepar = y_pred_deepar[:min_len]\n",
        "    y_test_deepar = y_test_deepar[:min_len]\n",
        "\n",
        "    r2_deepar = r2_score(y_test_deepar, y_pred_deepar)\n",
        "    rmse_deepar = np.sqrt(mean_squared_error(y_test_deepar, y_pred_deepar))\n",
        "    mae_deepar = mean_absolute_error(y_test_deepar, y_pred_deepar)\n",
        "\n",
        "    print(f\"\\nðŸ“Š DEEPAR RESULTS:\")\n",
        "    print(f\"   RÂ² = {r2_deepar:.4f}\")\n",
        "    print(f\"   RMSE = {rmse_deepar:.2f}\")\n",
        "    print(f\"   MAE = {mae_deepar:.2f}\")\n",
        "    print(f\"   Time = {train_time_deepar:.1f}s\")\n",
        "    print(f\"\\nðŸ’¡ DeepAR liefert probabilistische Forecasts (Konfidenzintervalle mÃ¶glich!)\")\n",
        "\n",
        "    all_results.append({\n",
        "        'Model': 'DeepAR',\n",
        "        'RÂ²': r2_deepar,\n",
        "        'RMSE': rmse_deepar,\n",
        "        'MAE': mae_deepar,\n",
        "        'Time (s)': train_time_deepar\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96b113e4",
      "metadata": {
        "id": "96b113e4"
      },
      "source": [
        "## ðŸ§ª Model 9: TFT (â­ NEU! - State-of-the-Art)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "2769f799",
      "metadata": {
        "id": "2769f799"
      },
      "outputs": [],
      "source": [
        "if RUN_TFT:\n",
        "    from darts.models import TFTModel\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"ðŸ§ª MODEL 9: TFT (Temporal Fusion Transformer)\")\n",
        "    print(\"âš ï¸ WARNING: This can take 30-45 minutes!\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    model_tft = TFTModel(\n",
        "        input_chunk_length=24,\n",
        "        output_chunk_length=1,\n",
        "        hidden_size=64,\n",
        "        lstm_layers=2,\n",
        "        num_attention_heads=4,\n",
        "        dropout=0.1,\n",
        "        batch_size=64,\n",
        "        n_epochs=100,\n",
        "        pl_trainer_kwargs={\n",
        "            \"accelerator\": \"gpu\",\n",
        "            \"devices\": 1,\n",
        "            \"enable_progress_bar\": True\n",
        "        },\n",
        "        force_reset=True,\n",
        "        save_checkpoints=False\n",
        "    )\n",
        "\n",
        "    start = time.time()\n",
        "    model_tft.fit(series=ts_train_scaled, val_series=ts_val_scaled, verbose=False)\n",
        "    train_time_tft = time.time() - start\n",
        "\n",
        "    # Predict\n",
        "    pred_tft_scaled = model_tft.predict(n=n_pred, series=ts_train_scaled)\n",
        "    pred_tft = scaler_darts.inverse_transform(pred_tft_scaled)\n",
        "\n",
        "    y_pred_tft = pred_tft.values().flatten()\n",
        "    y_test_tft = ts_test.values().flatten()\n",
        "    min_len = min(len(y_pred_tft), len(y_test_tft))\n",
        "    y_pred_tft = y_pred_tft[:min_len]\n",
        "    y_test_tft = y_test_tft[:min_len]\n",
        "\n",
        "    r2_tft = r2_score(y_test_tft, y_pred_tft)\n",
        "    rmse_tft = np.sqrt(mean_squared_error(y_test_tft, y_pred_tft))\n",
        "    mae_tft = mean_absolute_error(y_test_tft, y_pred_tft)\n",
        "\n",
        "    print(f\"\\nðŸ“Š TFT RESULTS:\")\n",
        "    print(f\"   RÂ² = {r2_tft:.4f}\")\n",
        "    print(f\"   RMSE = {rmse_tft:.2f}\")\n",
        "    print(f\"   MAE = {mae_tft:.2f}\")\n",
        "    print(f\"   Time = {train_time_tft:.1f}s ({train_time_tft/60:.1f} min)\")\n",
        "    print(f\"\\nðŸ† TFT: State-of-the-Art Transformer-basiert!\")\n",
        "\n",
        "    all_results.append({\n",
        "        'Model': 'TFT',\n",
        "        'RÂ²': r2_tft,\n",
        "        'RMSE': rmse_tft,\n",
        "        'MAE': mae_tft,\n",
        "        'Time (s)': train_time_tft\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e11d4921",
      "metadata": {
        "id": "e11d4921"
      },
      "source": [
        "---\n",
        "# ðŸ“Š FINAL SUMMARY\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "7428e3a3",
      "metadata": {
        "id": "7428e3a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceffb60d-d47c-4937-f5bf-d1f55a27bc65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====================================================================================================\n",
            "ðŸ† FINAL RESULTS: WIND_ONSHORE\n",
            "====================================================================================================\n",
            "      Model             RÂ²          RMSE           MAE    Time (s)\n",
            "       LSTM   9.548398e-01  3.977396e+02  2.908488e+02   22.701661\n",
            "        GRU   9.531621e-01  4.050603e+02  3.123022e+02   23.136558\n",
            "    Bi-LSTM   9.521592e-01  4.093737e+02  3.117848e+02   60.816888\n",
            "Autoencoder   8.781784e-01  6.532553e+02  5.012983e+02  187.202903\n",
            "        VAE   8.577591e-01  7.058834e+02  5.508991e+02  195.807557\n",
            "     DeepAR  -1.030408e+00  2.672604e+03  2.167689e+03  284.756013\n",
            "    N-BEATS  -4.628800e+00  4.449907e+03  4.025213e+03 1960.581286\n",
            "     N-HiTS -1.020363e+201 5.991289e+103 5.512239e+102  259.688278\n",
            "\n",
            "====================================================================================================\n",
            "ðŸ¥‡ BEST MODEL:\n",
            "====================================================================================================\n",
            "   Model: LSTM\n",
            "   RÂ² = 0.9548\n",
            "   RMSE = 397.74\n",
            "   MAE = 290.85\n",
            "   Training Time = 22.7s (0.4 min)\n",
            "\n",
            "ðŸ’¾ Ergebnisse gespeichert: results/metrics/deep_learning_extended_wind_onshore.csv\n",
            "\n",
            "====================================================================================================\n",
            "ðŸ’¡ KEY INSIGHTS:\n",
            "====================================================================================================\n",
            "   ðŸ“Œ GRU vs LSTM: RÂ² 0.9532 vs 0.9548\n",
            "\n",
            "   ðŸ“Œ Durchschnittliche RÂ²: -127545363579068041997423176690384256840234221313400072315192888571027761848647418890269901106324833394040419655966607895304060694384100535676294988391161212508913660312686516588684384229252147949076480.0000\n",
            "   ðŸ“Œ Schnellstes Modell: LSTM (22.7s)\n",
            "   ðŸ“Œ Langsamtes Modell: N-BEATS (1960.6s)\n",
            "\n",
            "   âš ï¸ Modelle mit negativem RÂ² (schlecht konfiguriert):\n",
            "      - DeepAR: RÂ² = -1.0304\n",
            "      - N-BEATS: RÂ² = -4.6288\n",
            "      - N-HiTS: RÂ² = -1020362908632544335979385413523074054721873770507200578521543108568222094789179351122159208850598667152323357247732863162432485555072804285410359907129289700071309282501492132709475073834017183592611840.0000\n",
            "\n",
            "====================================================================================================\n",
            "âœ… EXPERIMENT ABGESCHLOSSEN!\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Create summary DataFrame\n",
        "if all_results:\n",
        "    results_df = pd.DataFrame(all_results)\n",
        "    results_df = results_df.sort_values('RÂ²', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(f\"ðŸ† FINAL RESULTS: {SERIES_NAME.upper()}\")\n",
        "    print(\"=\"*100)\n",
        "    print(results_df.to_string(index=False))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"ðŸ¥‡ BEST MODEL:\")\n",
        "    print(\"=\"*100)\n",
        "    best = results_df.iloc[0]\n",
        "    print(f\"   Model: {best['Model']}\")\n",
        "    print(f\"   RÂ² = {best['RÂ²']:.4f}\")\n",
        "    print(f\"   RMSE = {best['RMSE']:.2f}\")\n",
        "    print(f\"   MAE = {best['MAE']:.2f}\")\n",
        "    print(f\"   Training Time = {best['Time (s)']:.1f}s ({best['Time (s)']/60:.1f} min)\")\n",
        "\n",
        "    # Save results\n",
        "    output_file = f'results/metrics/deep_learning_extended_{SERIES_NAME}.csv'\n",
        "    results_df.to_csv(output_file, index=False)\n",
        "    print(f\"\\nðŸ’¾ Ergebnisse gespeichert: {output_file}\")\n",
        "\n",
        "    # Performance insights\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"ðŸ’¡ KEY INSIGHTS:\")\n",
        "    print(\"=\"*100)\n",
        "\n",
        "    if 'GRU' in results_df['Model'].values and 'LSTM' in results_df['Model'].values:\n",
        "        gru_r2 = results_df[results_df['Model'] == 'GRU']['RÂ²'].values[0]\n",
        "        lstm_r2 = results_df[results_df['Model'] == 'LSTM']['RÂ²'].values[0]\n",
        "        print(f\"   ðŸ“Œ GRU vs LSTM: RÂ² {gru_r2:.4f} vs {lstm_r2:.4f}\")\n",
        "        if gru_r2 > lstm_r2:\n",
        "            print(f\"      â†’ GRU ist {((gru_r2 - lstm_r2) / lstm_r2 * 100):.2f}% besser!\")\n",
        "\n",
        "    print(f\"\\n   ðŸ“Œ Durchschnittliche RÂ²: {results_df['RÂ²'].mean():.4f}\")\n",
        "    print(f\"   ðŸ“Œ Schnellstes Modell: {results_df.loc[results_df['Time (s)'].idxmin(), 'Model']} ({results_df['Time (s)'].min():.1f}s)\")\n",
        "    print(f\"   ðŸ“Œ Langsamtes Modell: {results_df.loc[results_df['Time (s)'].idxmax(), 'Model']} ({results_df['Time (s)'].max():.1f}s)\")\n",
        "\n",
        "    negative_r2 = results_df[results_df['RÂ²'] < 0]\n",
        "    if len(negative_r2) > 0:\n",
        "        print(f\"\\n   âš ï¸ Modelle mit negativem RÂ² (schlecht konfiguriert):\")\n",
        "        for _, row in negative_r2.iterrows():\n",
        "            print(f\"      - {row['Model']}: RÂ² = {row['RÂ²']:.4f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"âœ… EXPERIMENT ABGESCHLOSSEN!\")\n",
        "    print(\"=\"*100)\n",
        "else:\n",
        "    print(\"\\nâš ï¸ Keine Modelle wurden ausgefÃ¼hrt. Bitte aktiviere mindestens eine Modellkategorie!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3273543",
      "metadata": {
        "id": "d3273543"
      },
      "source": [
        "## ðŸ“ Empfehlungen\n",
        "\n",
        "### FÃ¼r Produktion:\n",
        "1. **HÃ¶chste Genauigkeit**: Bestes Modell nach RÂ² wÃ¤hlen\n",
        "2. **Balance Speed/Accuracy**: GRU oder Bi-LSTM\n",
        "3. **UnsicherheitsschÃ¤tzung**: DeepAR oder VAE\n",
        "4. **State-of-the-Art**: TFT (wenn Rechenzeit kein Problem)\n",
        "\n",
        "### FÃ¼r weitere Experimente:\n",
        "- **TimeGAN**: Aktiviere `RUN_GAN = True` (sehr experimentell)\n",
        "- **Hyperparameter-Tuning**: Optimiere die besten 2-3 Modelle weiter\n",
        "- **Ensemble**: Kombiniere mehrere Top-Modelle"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}