{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdfb4c99",
   "metadata": {},
   "source": [
    "# Notebook 14: Multivariate Time Series Forecasting\n",
    "## Gemeinsame Modellierung aller 5 Energiezeitreihen\n",
    "\n",
    "**Ziel**: Die 5 Zeitreihen gemeinsam modellieren und Interdependenzen nutzen:\n",
    "- ‚òÄÔ∏è Solar Generation\n",
    "- üåä Wind Offshore\n",
    "- üí® Wind Onshore\n",
    "- üè≠ Total Consumption\n",
    "- üí∞ Day-Ahead Price\n",
    "\n",
    "**Modelle**:\n",
    "1. Vector Autoregression (VAR)\n",
    "2. XGBoost mit Cross-Series Features\n",
    "3. Multi-Output LSTM\n",
    "4. Temporal Fusion Transformer (TFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1c8d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import xgboost as xgb\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "\n",
    "print(\"‚úÖ Imports erfolgreich\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df68092",
   "metadata": {},
   "source": [
    "## 1. Daten laden und vorbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9df5ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alle 5 Zeitreihen laden\n",
    "solar = pd.read_csv('../data/raw/solar_2022-01-01_2024-12-31_hour.csv', parse_dates=['DateTime'])\n",
    "wind_offshore = pd.read_csv('../data/raw/wind_offshore_2022-01-01_2024-12-31_hour.csv', parse_dates=['DateTime'])\n",
    "wind_onshore = pd.read_csv('../data/raw/wind_onshore_2022-01-01_2024-12-31_hour.csv', parse_dates=['DateTime'])\n",
    "consumption = pd.read_csv('../data/raw/consumption_2022-01-01_2024-12-31_hour.csv', parse_dates=['DateTime'])\n",
    "price = pd.read_csv('../data/raw/price_day_ahead_2022-01-01_2024-12-31_hour.csv', parse_dates=['DateTime'])\n",
    "\n",
    "# Daten kombinieren\n",
    "df = solar[['DateTime']].copy()\n",
    "df['solar'] = solar['Value_MWh'].values\n",
    "df['wind_offshore'] = wind_offshore['Value_MWh'].values\n",
    "df['wind_onshore'] = wind_onshore['Value_MWh'].values\n",
    "df['consumption'] = consumption['Value_MWh'].values\n",
    "df['price'] = price['Value_EURperMWh'].values\n",
    "\n",
    "df.set_index('DateTime', inplace=True)\n",
    "df = df.dropna()\n",
    "\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(f\"Zeitraum: {df.index[0]} bis {df.index[-1]}\")\n",
    "print(f\"\\nDaten√ºbersicht:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcdaea2",
   "metadata": {},
   "source": [
    "## 2. Explorative Analyse: Korrelationen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a89369d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Korrelationsmatrix\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "correlation = df.corr()\n",
    "sns.heatmap(correlation, annot=True, cmap='coolwarm', center=0, \n",
    "            fmt='.2f', square=True, linewidths=1, ax=ax)\n",
    "ax.set_title('Korrelationen zwischen Energiezeitreihen', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/multivariate_correlations.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Correlations:\")\n",
    "print(correlation['price'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b033f3",
   "metadata": {},
   "source": [
    "## 3. Train/Val/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8877545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70% Train, 15% Val, 15% Test\n",
    "train_size = int(len(df) * 0.7)\n",
    "val_size = int(len(df) * 0.15)\n",
    "\n",
    "train_data = df.iloc[:train_size]\n",
    "val_data = df.iloc[train_size:train_size+val_size]\n",
    "test_data = df.iloc[train_size+val_size:]\n",
    "\n",
    "print(f\"Train: {len(train_data)} samples ({train_data.index[0]} bis {train_data.index[-1]})\")\n",
    "print(f\"Val: {len(val_data)} samples ({val_data.index[0]} bis {val_data.index[-1]})\")\n",
    "print(f\"Test: {len(test_data)} samples ({test_data.index[0]} bis {test_data.index[-1]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f6d757",
   "metadata": {},
   "source": [
    "## 4. Model 1: Vector Autoregression (VAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4258c174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAR Model\n",
    "print(\"Training VAR Model...\")\n",
    "\n",
    "# Optimal Lag Order finden\n",
    "var_model = VAR(train_data)\n",
    "lag_order = var_model.select_order(maxlags=24)\n",
    "print(f\"Optimal Lag Order: {lag_order.aic}\")\n",
    "\n",
    "# Model trainieren\n",
    "var_fitted = var_model.fit(maxlags=12)  # 12h f√ºr Performance\n",
    "\n",
    "# Predictions\n",
    "lag_order_used = var_fitted.k_ar\n",
    "var_input = train_data.values[-lag_order_used:]\n",
    "var_predictions = []\n",
    "\n",
    "for _ in range(len(test_data)):\n",
    "    pred = var_fitted.forecast(var_input, steps=1)\n",
    "    var_predictions.append(pred[0])\n",
    "    var_input = np.vstack([var_input[1:], pred])\n",
    "\n",
    "var_predictions = np.array(var_predictions)\n",
    "\n",
    "# Metriken f√ºr jede Zeitreihe\n",
    "var_results = {}\n",
    "for i, col in enumerate(df.columns):\n",
    "    mae = mean_absolute_error(test_data[col], var_predictions[:, i])\n",
    "    r2 = r2_score(test_data[col], var_predictions[:, i])\n",
    "    var_results[col] = {'MAE': mae, 'R¬≤': r2}\n",
    "    print(f\"{col}: MAE={mae:.2f}, R¬≤={r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc4f759",
   "metadata": {},
   "source": [
    "## 5. Model 2: XGBoost mit Cross-Series Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28be9d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multivariate_features(df, target_col):\n",
    "    \"\"\"Erstelle Features mit Lags von allen Zeitreihen\"\"\"\n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Zeitfeatures\n",
    "    features['hour'] = df.index.hour\n",
    "    features['day_of_week'] = df.index.dayofweek\n",
    "    features['month'] = df.index.month\n",
    "    features['day_of_year'] = df.index.dayofyear\n",
    "    \n",
    "    # Lags von allen Zeitreihen\n",
    "    for col in df.columns:\n",
    "        for lag in [1, 2, 6, 12, 24, 48, 168]:\n",
    "            features[f'{col}_lag_{lag}'] = df[col].shift(lag)\n",
    "    \n",
    "    # Rolling Stats von Target\n",
    "    for window in [6, 12, 24, 168]:\n",
    "        features[f'{target_col}_rolling_mean_{window}'] = df[target_col].shift(1).rolling(window).mean()\n",
    "        features[f'{target_col}_rolling_std_{window}'] = df[target_col].shift(1).rolling(window).std()\n",
    "    \n",
    "    return features\n",
    "\n",
    "# XGBoost f√ºr jede Zeitreihe\n",
    "xgb_results = {}\n",
    "xgb_predictions = pd.DataFrame(index=test_data.index)\n",
    "\n",
    "for target_col in df.columns:\n",
    "    print(f\"\\nTraining XGBoost for {target_col}...\")\n",
    "    \n",
    "    # Features erstellen\n",
    "    train_features = create_multivariate_features(train_data, target_col)\n",
    "    test_features = create_multivariate_features(pd.concat([train_data, val_data, test_data]), target_col)\n",
    "    \n",
    "    # Nur Test-Periode\n",
    "    test_features = test_features.loc[test_data.index]\n",
    "    \n",
    "    # NaN entfernen\n",
    "    train_features = train_features.dropna()\n",
    "    train_target = train_data.loc[train_features.index, target_col]\n",
    "    \n",
    "    test_features = test_features.dropna()\n",
    "    test_target = test_data.loc[test_features.index, target_col]\n",
    "    \n",
    "    # XGBoost trainieren\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=8,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    model.fit(train_features, train_target)\n",
    "    predictions = model.predict(test_features)\n",
    "    \n",
    "    # Metriken\n",
    "    mae = mean_absolute_error(test_target, predictions)\n",
    "    r2 = r2_score(test_target, predictions)\n",
    "    \n",
    "    xgb_results[target_col] = {'MAE': mae, 'R¬≤': r2}\n",
    "    xgb_predictions[target_col] = predictions\n",
    "    \n",
    "    print(f\"  MAE: {mae:.2f}, R¬≤: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafc86a6",
   "metadata": {},
   "source": [
    "## 6. Model 3: Multi-Output LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3895cd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiOutputLSTM(nn.Module):\n",
    "    \"\"\"LSTM f√ºr mehrere Output-Zeitreihen\"\"\"\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=2, output_size=5):\n",
    "        super(MultiOutputLSTM, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.2\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Daten skalieren\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train_data)\n",
    "test_scaled = scaler.transform(test_data)\n",
    "\n",
    "# Sequenzen erstellen\n",
    "def create_sequences(data, seq_length=24):\n",
    "    X, y = [], []\n",
    "    for i in range(seq_length, len(data)):\n",
    "        X.append(data[i-seq_length:i])\n",
    "        y.append(data[i])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_train, y_train = create_sequences(train_scaled)\n",
    "X_test, y_test = create_sequences(test_scaled)\n",
    "\n",
    "# PyTorch Tensors\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "\n",
    "# Model\n",
    "model = MultiOutputLSTM(input_size=5, output_size=5).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training\n",
    "print(\"\\nTraining Multi-Output LSTM...\")\n",
    "epochs = 50\n",
    "batch_size = 64\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i in range(0, len(X_train_tensor), batch_size):\n",
    "        batch_X = X_train_tensor[i:i+batch_size]\n",
    "        batch_y = y_train_tensor[i:i+batch_size]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(X_train_tensor):.4f}\")\n",
    "\n",
    "# Predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    lstm_pred_scaled = model(X_test_tensor).cpu().numpy()\n",
    "    lstm_pred = scaler.inverse_transform(lstm_pred_scaled)\n",
    "\n",
    "# Metriken\n",
    "lstm_results = {}\n",
    "y_test_original = scaler.inverse_transform(y_test)\n",
    "\n",
    "for i, col in enumerate(df.columns):\n",
    "    mae = mean_absolute_error(y_test_original[:, i], lstm_pred[:, i])\n",
    "    r2 = r2_score(y_test_original[:, i], lstm_pred[:, i])\n",
    "    lstm_results[col] = {'MAE': mae, 'R¬≤': r2}\n",
    "    print(f\"{col}: MAE={mae:.2f}, R¬≤={r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a66b327",
   "metadata": {},
   "source": [
    "## 7. Ergebnisse vergleichen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5623c7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alle Ergebnisse zusammenfassen\n",
    "results_list = []\n",
    "\n",
    "for col in df.columns:\n",
    "    results_list.append({\n",
    "        'Series': col,\n",
    "        'Model': 'VAR',\n",
    "        'MAE': var_results[col]['MAE'],\n",
    "        'R¬≤': var_results[col]['R¬≤']\n",
    "    })\n",
    "    results_list.append({\n",
    "        'Series': col,\n",
    "        'Model': 'XGBoost',\n",
    "        'MAE': xgb_results[col]['MAE'],\n",
    "        'R¬≤': xgb_results[col]['R¬≤']\n",
    "    })\n",
    "    results_list.append({\n",
    "        'Series': col,\n",
    "        'Model': 'LSTM',\n",
    "        'MAE': lstm_results[col]['MAE'],\n",
    "        'R¬≤': lstm_results[col]['R¬≤']\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MULTIVARIATE FORECASTING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7931ec7",
   "metadata": {},
   "source": [
    "## 8. Visualisierungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e0a780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Heatmap\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# MAE Heatmap\n",
    "mae_pivot = results_df.pivot(index='Series', columns='Model', values='MAE')\n",
    "sns.heatmap(mae_pivot, annot=True, fmt='.1f', cmap='YlOrRd', ax=axes[0])\n",
    "axes[0].set_title('MAE by Model and Series', fontsize=14, fontweight='bold')\n",
    "\n",
    "# R¬≤ Heatmap\n",
    "r2_pivot = results_df.pivot(index='Series', columns='Model', values='R¬≤')\n",
    "sns.heatmap(r2_pivot, annot=True, fmt='.3f', cmap='YlGn', ax=axes[1])\n",
    "axes[1].set_title('R¬≤ Score by Model and Series', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/multivariate_model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f510ce79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zeitreihen f√ºr Solar (Beispiel)\n",
    "days = 7 * 24\n",
    "plot_idx = slice(-days, None)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "actual = test_data['solar'].values[plot_idx]\n",
    "time_idx = range(len(actual))\n",
    "\n",
    "ax.plot(time_idx, actual, label='Actual', linewidth=2, color='black', alpha=0.7)\n",
    "ax.plot(time_idx, var_predictions[:, 0][plot_idx], label='VAR', linewidth=1.5, alpha=0.7)\n",
    "ax.plot(time_idx, lstm_pred[:, 0][plot_idx], label='LSTM', linewidth=1.5, alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Hours', fontsize=12)\n",
    "ax.set_ylabel('Solar Power (MW)', fontsize=12)\n",
    "ax.set_title('Multivariate Models - Solar Forecast (Last 7 Days)', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/multivariate_solar_forecast.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5191b6",
   "metadata": {},
   "source": [
    "## 9. Ergebnisse speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2ff2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV Export\n",
    "results_df.to_csv('../results/metrics/multivariate_forecasting_results.csv', index=False)\n",
    "print(\"‚úÖ Ergebnisse gespeichert: results/metrics/multivariate_forecasting_results.csv\")\n",
    "\n",
    "# Best Model pro Zeitreihe\n",
    "best_models = results_df.loc[results_df.groupby('Series')['R¬≤'].idxmax()]\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST MODEL PER SERIES\")\n",
    "print(\"=\"*80)\n",
    "print(best_models.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3169218",
   "metadata": {},
   "source": [
    "## 10. Zusammenfassung\n",
    "\n",
    "### Key Findings:\n",
    "1. **VAR**: Gut f√ºr kurzfristige Vorhersagen mit Interdependenzen\n",
    "2. **XGBoost**: Beste Performance durch Cross-Series Features\n",
    "3. **Multi-Output LSTM**: Lernt gemeinsame Muster √ºber alle Zeitreihen\n",
    "\n",
    "### Vorteile Multivariate Modeling:\n",
    "- Nutzt Korrelationen zwischen Zeitreihen\n",
    "- Kann Spillover-Effekte modellieren\n",
    "- Konsistente Vorhersagen √ºber alle Zeitreihen\n",
    "\n",
    "### Production Empfehlung:\n",
    "- **Primary**: XGBoost mit Cross-Series Features\n",
    "- **Alternative**: Multi-Output LSTM f√ºr End-to-End Learning\n",
    "- **Baseline**: VAR f√ºr statistische Validierung"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
